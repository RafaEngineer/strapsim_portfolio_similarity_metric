{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8INIk6EVVGo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`README.md`**\n",
        "\n",
        "# STRAPSim: A Portfolio Similarity Metric for ETF Alignment and Portfolio Trades\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2509.24151-b31b1b.svg)](https://arxiv.org/abs/2509.24151)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/strapsim_portfolio_similarity_metric)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Quantitative%20Finance-00529B)](https://github.com/chirindaopensource/strapsim_portfolio_similarity_metric)\n",
        "[![Primary Data](https://img.shields.io/badge/Data-Corporate%20Bond%20ETF%20Holdings-lightgrey)](https://github.com/chirindaopensource/strapsim_portfolio_similarity_metric)\n",
        "[![Ground Truth](https://img.shields.io/badge/Ground%20Truth-Monthly%20Total%20Returns-lightgrey)](https://github.com/chirindaopensource/strapsim_portfolio_similarity_metric)\n",
        "[![Core Algorithm](https://img.shields.io/badge/Algorithm-STRAPSim-orange)](https://github.com/chirindaopensource/strapsim_portfolio_similarity_metric)\n",
        "[![Similarity Method](https://img.shields.io/badge/Similarity-Random%20Forest%20Proximity-red)](https://github.com/chirindaopensource/strapsim_portfolio_similarity_metric)\n",
        "[![Evaluation Metric](https://img.shields.io/badge/Evaluation-Spearman%20Rank%20Correlation-yellow)](https://github.com/chirindaopensource/strapsim_portfolio_similarity_metric)\n",
        "[![Baselines](https://img.shields.io/badge/Baselines-Jaccard%20%7C%20BERTScore-blueviolet)](https://github.com/chirindaopensource/strapsim_portfolio_similarity_metric)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=flat&logo=scikit-learn&logoColor=white)](https://scikit-learn.org/)\n",
        "[![Numba](https://img.shields.io/badge/Numba-00A3E0.svg?style=flat&logo=Numba&logoColor=white)](https://numba.pydata.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "--\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/strapsim_portfolio_similarity_metric`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"STRAPSim: A Portfolio Similarity Metric for ETF Alignment and Portfolio Trades\"** by:\n",
        "\n",
        "*   Mingshu Li\n",
        "*   Dhruv Desai\n",
        "*   Jerinsh Jeyapaulraj\n",
        "*   Philip Sommer\n",
        "*   Riya Jain\n",
        "*   Peter Chu\n",
        "*   Dhagash Mehta\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's novel portfolio similarity metric, STRAPSim. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from rigorous data validation and feature engineering, through the training of a supervised similarity model and the implementation of the core STRAPSim algorithm, to the final statistical evaluation and a comprehensive suite of robustness checks.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callables](#key-callables)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"STRAPSim: A Portfolio Similarity Metric for ETF Alignment and Portfolio Trades.\" The core of this repository is the iPython Notebook `strapsim_portfolio_similarity_metric_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation to the final generation of the summary evaluation table (Table 4) and a full suite of sensitivity analyses.\n",
        "\n",
        "The paper introduces **STRAPSim (Semantic, Two-level, Residual-Aware Portfolio Similarity)**, a novel method for measuring the similarity between structured asset baskets like ETFs. It addresses the key limitations of traditional metrics by incorporating learned, constituent-level similarity and a dynamic, weight-aware matching process. This codebase operationalizes the paper's advanced approach, allowing users to:\n",
        "-   Rigorously validate and cleanse institutional-grade fixed-income portfolio data.\n",
        "-   Train a Random Forest model to learn a \"semantic\" similarity metric between individual bonds based on their financial characteristics.\n",
        "-   Generate a pairwise proximity matrix for all bonds in the universe.\n",
        "-   Compute the STRAPSim score between any two portfolios using the novel greedy, residual-aware matching algorithm.\n",
        "-   Benchmark STRAPSim's performance against standard metrics (Jaccard, Weighted Jaccard) and an adapted BERTScore.\n",
        "-   Evaluate all metrics against a market-based ground truth (return correlation) using Spearman rank correlation.\n",
        "-   Systematically test the stability of the findings across a wide array of robustness checks.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in machine learning, algorithm design, and quantitative finance.\n",
        "\n",
        "**1. Supervised Similarity Learning:**\n",
        "The foundation of the method is a constituent-level similarity score, $S_{ij}$, between any two bonds, $i$ and $j$. Instead of relying on simple distance metrics, the paper learns this similarity in a supervised fashion. A Random Forest model is trained to predict key financial characteristics of bonds (OAS and Yield) from their features (issuer, maturity, rating, etc.). The \"proximity\" between two bonds is then defined as the fraction of trees in the forest where they fall into the same terminal leaf node. This creates a nuanced, non-linear similarity measure that is tailored to the financial properties of the assets.\n",
        "$$\n",
        "\\text{Proximity}(x_i, x_j) = \\frac{1}{T} \\sum_{t=1}^{T} I[\\text{Leaf}_t(x_i) = \\text{Leaf}_t(x_j)]\n",
        "$$\n",
        "\n",
        "**2. The STRAPSim Algorithm:**\n",
        "STRAPSim is a greedy, bipartite matching algorithm that computes the similarity between two portfolios, X and Y, defined by their constituents and weights. It iteratively matches the pair of constituents $(x_i, y_j)$ with the highest remaining similarity score $S_{ij}$. The key innovation is its **residual-aware** dynamic: after each match, the weights of the involved constituents are decremented by the amount of weight transferred (the minimum of the two). This prevents a single, high-weight constituent from being \"re-used\" in multiple matches and ensures the algorithm correctly models the one-to-one nature of portfolio alignment.\n",
        "$$\n",
        "\\text{STRAPSim}(x, y) = \\sum_{i,j=\\text{argsort}(S)} S_{ij} \\min(w_x^{(t)}(i), w_y^{(t)}(j))\n",
        "$$\n",
        "where weights $w^{(t)}$ are updated at each step $t$.\n",
        "\n",
        "**3. Evaluation via Rank Correlation:**\n",
        "The performance of STRAPSim and the baseline metrics is evaluated by comparing their ability to rank portfolio pairs in a way that aligns with a market-based measure of similarity. The paper uses the historical monthly return correlation between ETFs as this \"ground truth.\" The primary evaluation metric is the **Spearman Rank Correlation Coefficient ($\\rho_s$)**, which measures the monotonic relationship between the ranking produced by a similarity metric and the ranking produced by the return correlations. A higher $\\rho_s$ indicates a better alignment with market behavior.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`strapsim_portfolio_similarity_metric_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Phase Architecture:** The entire pipeline is broken down into 28 distinct, modular tasks, each with its own orchestrator function, covering validation, cleansing, feature engineering, modeling, computation, evaluation, and robustness testing.\n",
        "-   **Configuration-Driven Design:** All methodological and computational parameters are managed in an external `config.yaml` file, allowing for easy customization and scenario testing without code changes.\n",
        "-   **Advanced Similarity Learning:** A complete pipeline for training a multi-output Random Forest Regressor, including cross-validated hyperparameter tuning, to serve as the engine for the similarity metric.\n",
        "-   **Optimized Algorithm Implementation:** A high-performance, memory-efficient implementation of the core STRAPSim algorithm and all baseline metrics (Jaccard, Weighted Jaccard, adapted BERTScore with residuals).\n",
        "-   **Rigorous Evaluation Framework:** A systematic process for transforming similarity and correlation matrices into ranks and computing the Spearman rank correlation and p-values for every method and every reference ETF.\n",
        "-   **Comprehensive Robustness Suite:** A powerful, extensible set of orchestrators for running a full suite of sensitivity analyses on model hyperparameters, data partitioning, and metric-specific parameters.\n",
        "-   **Automated Reporting:** Programmatic generation of the final summary table (replicating Table 4 from the paper) and detailed logs for all analysis and robustness check results.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Validation & Cleansing (Tasks 1-8):** Ingests and rigorously validates all raw data (holdings, features, returns) and the `config.yaml` file, performs a deep data quality audit, and standardizes all data into an analysis-ready format.\n",
        "2.  **Feature Engineering (Tasks 9-11):** Prepares the bond feature data for machine learning, including one-hot encoding of categorical variables and max-scaling of numerical variables, and partitions the data into training and testing sets.\n",
        "3.  **Model Training & Proximity Generation (Tasks 12-14):** Performs a grid search to find optimal Random Forest hyperparameters, trains the final model, validates its performance against the paper's benchmarks, and uses it to generate the crucial N x N bond proximity matrix.\n",
        "4.  **Similarity Computation (Tasks 15-21):** Prepares the portfolio data structures and then computes the full N x N similarity matrices for STRAPSim and all baseline methods, as well as the ground-truth return correlation matrix.\n",
        "5.  **Evaluation & Reporting (Tasks 22-24):** Prepares the ranked data, computes the Spearman rank correlation for all methods, and aggregates the results into the final summary table.\n",
        "6.  **Robustness Analysis (Tasks 25-28):** Orchestrates the entire suite of sensitivity checks.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `strapsim_portfolio_similarity_metric_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callables\n",
        "\n",
        "The project is designed around two primary user-facing interface functions:\n",
        "\n",
        "1.  **`run_strapsim_pipeline` (or its variant `run_strapsim_pipeline_for_robustness`):** This function executes the core research pipeline from end-to-end, producing the main findings of the study and all the necessary data artifacts (e.g., the trained model, the proximity matrix) required for deeper analysis.\n",
        "\n",
        "2.  **`run_full_study`:** This is the top-level orchestrator. It first calls the main pipeline function to generate the core results and artifacts, and then passes these artifacts to the robustness analysis suite (`run_robustness_analysis_suite`) to perform a complete validation of the study's conclusions. A single call to this function reproduces the entire project.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `scikit-learn`, `pyyaml`, `tqdm`, `numba`, `jsonschema`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/strapsim_portfolio_similarity_metric.git\n",
        "    cd strapsim_portfolio_similarity_metric\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install -r requirements.txt\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires three `pandas.DataFrame`s and a `config.yaml` file. The schemas for these DataFrames are rigorously defined and validated by the pipeline.\n",
        "-   **ETF Holdings DataFrame:** Contains portfolio composition data, including `etf_id`, `cusip`, and `weight`.\n",
        "-   **Bond Features DataFrame:** The security master file, containing financial characteristics for every bond, indexed by `cusip`.\n",
        "-   **Monthly Returns DataFrame:** A time-series DataFrame with a `date` column and one column for each ETF's monthly total returns.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `strapsim_portfolio_similarity_metric_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to call the top-level orchestrator:\n",
        "\n",
        "```python\n",
        "# This single call runs the entire project, including the main analysis\n",
        "# and all robustness checks.\n",
        "full_results = run_full_study(\n",
        "    etf_holdings_df=my_holdings_data,\n",
        "    bond_features_df=my_features_data,\n",
        "    monthly_returns_df=my_returns_data,\n",
        "    config=my_config_dict\n",
        ")\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `run_full_study` function returns a comprehensive nested dictionary containing all results and artifacts:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"main_analysis_artifacts\": {\n",
        "        \"config\": {...},\n",
        "        \"trained_model\": <RandomForestRegressor object>,\n",
        "        \"proximity_matrix_df\": <DataFrame>,\n",
        "        \"similarity_matrices\": {\n",
        "            \"STRAPSim\": <DataFrame>,\n",
        "            \"Jaccard\": <DataFrame>,\n",
        "            ...\n",
        "        },\n",
        "        \"final_summary_table\": <DataFrame>,\n",
        "        ...\n",
        "    },\n",
        "    \"robustness_analysis_results\": {\n",
        "        \"hyperparameter_sensitivity\": <DataFrame>,\n",
        "        \"data_split_sensitivity\": <DataFrame>,\n",
        "        \"metric_component_sensitivity\": <DataFrame>\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "strapsim_portfolio_similarity_metric/\n",
        "│\n",
        "├── strapsim_portfolio_similarity_metric_draft.ipynb   # Main implementation notebook\n",
        "├── config.yaml                                        # Master configuration file\n",
        "├── requirements.txt                                   # Python package dependencies\n",
        "├── LICENSE                                            # MIT license file\n",
        "└── README.md                                          # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can easily modify all methodological parameters, such as data paths, feature definitions, model hyperparameters, and evaluation settings, without altering the core Python code.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Alternative Similarity Models:** Implementing other supervised models (e.g., Gradient Boosting, Neural Networks) to generate the constituent-level proximity matrix.\n",
        "-   **Optimal Transport Baselines:** Adding Wasserstein distance (Optimal Transport) as a more advanced baseline for comparing weighted distributions.\n",
        "-   **Performance Optimization:** For extremely large universes, the STRAPSim algorithm could be further accelerated using more advanced data structures or a compiled language extension.\n",
        "-   **Application Modules:** Building specific application modules on top of the STRAPSim score, such as a portfolio recommendation engine or a tool for optimizing portfolio trades.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{li2025strapsim,\n",
        "  title={{STRAPSim: A Portfolio Similarity Metric for ETF Alignment and Portfolio Trades}},\n",
        "  author={Li, Mingshu and Desai, Dhruv and Jeyapaulraj, Jerinsh and Sommer, Philip and Jain, Riya and Chu, Peter and Mehta, Dhagash},\n",
        "  journal={arXiv preprint arXiv:2509.24151},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Professional-Grade Implementation of the STRAPSim Framework.\n",
        "GitHub repository: https://github.com/chirindaopensource/strapsim_portfolio_similarity_metric\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Mingshu Li, Dhruv Desai, Jerinsh Jeyapaulraj, Philip Sommer, Riya Jain, Peter Chu, and Dhagash Mehta** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, SciPy, Scikit-learn, Numba, and Jupyter**, whose work makes complex computational analysis accessible and robust.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `strapsim_portfolio_similarity_metric_draft.ipynb` and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "y4b4Mp81bxmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*STRAPSim: A Portfolio Similarity Metric for ETF Alignment and Portfolio Trades*\"\n",
        "\n",
        "Authors: Mingshu Li, Dhruv Desai, Jerinsh Jeyapaulraj, Philip Sommer, Riya Jain, Peter Chu, Dhagash Mehta\n",
        "\n",
        "E-Journal Submission Date: 29 September 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2509.24151\n",
        "\n",
        "Abstract:\n",
        "\n",
        "Accurately measuring portfolio similarity is critical for a wide range of financial applications, including Exchange-traded Fund (ETF) recommendation, portfolio trading, and risk alignment. Existing similarity measures often rely on exact asset overlap or static distance metrics, which fail to capture similarities among the constituents (e.g., securities within the portfolio) as well as nuanced relationships between partially overlapping portfolios with heterogeneous weights. We introduce STRAPSim (Semantic, Two-level, Residual-Aware Portfolio Similarity), a novel method that computes portfolio similarity by matching constituents based on semantic similarity, weighting them according to their portfolio share, and aggregating results via residual-aware greedy alignment. We benchmark our approach against Jaccard, weighted Jaccard, as well as BERTScore-inspired variants across public classification, regression, and recommendation tasks, as well as on corporate bond ETF datasets. Empirical results show that our method consistently outperforms baselines in predictive accuracy and ranking alignment, achieving the highest Spearman correlation with return-based similarity. By leveraging constituent-aware matching and dynamic reweighting, portfolio similarity offers a scalable, interpretable framework for comparing structured asset baskets, demonstrating its utility in ETF benchmarking, portfolio construction, and systematic execution.\n"
      ],
      "metadata": {
        "id": "kH1IJcVoVYdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### Summary of STRAPSim: A Portfolio Similarity Metric\n",
        "\n",
        "#### The Core Problem and Its Financial Context\n",
        "\n",
        "The paper begins by identifying a critical deficiency in existing methods for measuring the similarity between financial portfolios, such as Exchange-Traded Funds (ETFs) or custom trading baskets. The authors argue that current approaches are inadequate for several reasons:\n",
        "\n",
        "*   **Set-Based Metrics (e.g., Jaccard Index):** These metrics only account for the exact overlap of assets (constituents). They fail to recognize that two different, but highly similar, corporate bonds (e.g., from the same issuer with close maturities) contribute to portfolio similarity. They also ignore the *weights* of these assets, treating a 5% holding identically to a 0.1% holding.\n",
        "*   **High-Level Metrics (e.g., Return Correlation, Sector Exposure):** These methods operate on aggregate portfolio characteristics. While useful, they can be noisy, backward-looking (especially return correlation), and often miss the granular, constituent-level drivers of similarity. For illiquid assets like many corporate bonds, reliable return data can be sparse, making correlation an unstable measure.\n",
        "\n",
        "This problem is particularly acute in the growing field of **portfolio trading**, where an entire basket of securities is executed as a single transaction. The ability to accurately match a custom, illiquid basket to a liquid benchmark ETF is crucial for efficient pricing, hedging, and risk management.\n",
        "\n",
        "#### Introduction of the Proposed Metric: STRAPSim\n",
        "\n",
        "To address these shortcomings, the authors propose a new metric named **STRAPSim (Semantic, Two-level, Residual-Aware Portfolio Similarity)**. The methodology is designed to be both granular and flexible. The core algorithm can be broken down as follows:\n",
        "\n",
        "1.  **Constituent-Level Semantic Similarity:** First, a pairwise similarity score, `S_ij`, is computed for every asset `i` in Portfolio X and every asset `j` in Portfolio Y. This is a \"pluggable\" component; the authors use a random forest proximity score based on bond characteristics (issuer, maturity, rating, etc.) for their main experiment. This step moves beyond exact matches to capture the *semantic* closeness of non-identical assets.\n",
        "\n",
        "2.  **Greedy Matching:** The algorithm iteratively finds the pair of unmatched (or partially matched) assets `(i, j)` with the highest similarity score `S_ij`.\n",
        "\n",
        "3.  **Weight-Aware Aggregation:** For the matched pair, the contribution to the total similarity score is calculated as `S_ij * min(w_x(i), w_y(j))`, where `w_x(i)` and `w_y(j)` are the current available weights of the assets in their respective portfolios. This ensures that the match is limited by the smaller of the two holdings.\n",
        "\n",
        "4.  **Residual-Aware Weight Update:** This is the most critical innovation. After a match, the algorithm updates the weights of both assets by subtracting the matched amount: `min(w_x(i), w_y(j))`. An asset whose weight is reduced to zero is considered fully \"consumed\" and is removed from future matching rounds. This prevents a single, highly versatile asset in one portfolio from being repeatedly matched to multiple assets in the other, thereby avoiding overcounting of similarity.\n",
        "\n",
        "The final STRAPSim score is the sum of all these weighted similarity contributions.\n",
        "\n",
        "#### Theoretical Distinctions and Benchmarks\n",
        "\n",
        "The paper astutely positions STRAPSim against other methodologies:\n",
        "\n",
        "*   **Jaccard & Weighted Jaccard:** STRAPSim is superior as it incorporates semantic similarity for non-identical assets and handles partial overlaps in weights more dynamically.\n",
        "*   **BERTScore-inspired Metric:** This is a key conceptual benchmark. BERTScore, from Natural Language Processing, also uses greedy matching based on semantic similarity. However, it is **not residual-aware**. It would match each asset in Portfolio X to its best counterpart in Y independently, without \"using up\" the weight of the matched asset. STRAPSim's residual-aware dynamic prevents this, providing a more accurate, one-to-one mapping analogous to a flow or transport problem.\n",
        "*   **Optimal Transport (OT):** While conceptually similar to OT in that it matches distributions, STRAPSim uses a computationally efficient and interpretable greedy algorithm rather than solving a global linear program, making it more scalable for this application.\n",
        "\n",
        "#### Empirical Validation and Experimental Design\n",
        "\n",
        "The authors validate their metric through a rigorous, two-part empirical study:\n",
        "\n",
        "1.  **Public \"Toy\" Datasets:** They first demonstrate the general applicability of STRAPSim on standard machine learning datasets (e.g., Iris, Breast Cancer) for classification and regression tasks. By treating data instances as \"portfolios\" and features as \"constituents,\" they use the STRAPSim score within a k-Nearest Neighbors (k-NN) framework.\n",
        "2.  **Corporate Bond ETF Dataset:** This is the core financial experiment. They compare 20 corporate bond ETFs. The crucial step here is defining a \"ground truth\" for economic similarity. They use **historical monthly return correlation** as this benchmark, based on the sound financial principle that portfolios with similar economic exposures should exhibit highly correlated returns.\n",
        "\n",
        "The primary evaluation metric is the **Spearman rank correlation** between the similarity scores produced by each metric (STRAPSim, Jaccard, etc.) and the benchmark return correlations. A higher Spearman correlation indicates that the metric is better at ranking portfolio pairs in a way that aligns with their real-world market behavior.\n",
        "\n",
        "#### Key Results and Findings\n",
        "\n",
        "The empirical results are compelling and consistent across all tests:\n",
        "\n",
        "*   On the public datasets, STRAPSim consistently outperforms the baselines in both classification accuracy and regression error, demonstrating its robustness as a general-purpose weighted set similarity metric.\n",
        "*   In the crucial corporate bond ETF experiment, **STRAPSim achieves the highest Spearman correlation (0.6783) with return-based similarity**. This result was also the most statistically significant (lowest p-value).\n",
        "*   Visualizations (heatmaps) confirm that the similarity matrix generated by STRAPSim more closely resembles the structure of the return correlation matrix compared to the blockier, less nuanced matrices from Jaccard-based methods.\n",
        "\n",
        "#### Conclusion and Implications\n",
        "\n",
        "The paper concludes that STRAPSim provides a superior framework for measuring portfolio similarity due to its unique combination of three features:\n",
        "1.  **Constituent-aware semantic matching.**\n",
        "2.  **Weight-sensitive aggregation.**\n",
        "3.  **Residual-aware dynamics.**\n",
        "\n",
        "This makes it a highly practical tool for financial applications. For institutional finance, it can directly enhance portfolio trading workflows by enabling more accurate and automated matching of custom baskets to liquid ETFs. This improves pricing, hedging, and overall execution efficiency. The metric's interpretability—the ability to inspect which constituents were matched and which remain as residual—is a significant advantage in high-stakes financial decision-making."
      ],
      "metadata": {
        "id": "XtlFSrEyaoIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "yjvhH0zPqLJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "#\n",
        "#  STRAPSim: A Portfolio Similarity Metric for ETF Alignment and Portfolio Trades\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"STRAPSim: A Portfolio Similarity Metric\n",
        "#  for ETF Alignment and Portfolio Trades\" by Li et al. (2025). It delivers a\n",
        "#  computationally efficient and interpretable system for quantifying the\n",
        "#  economic and functional substitutability between complex, structured asset\n",
        "#  baskets, such as Exchange-Traded Funds (ETFs) and bespoke bond portfolios.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Supervised learning of constituent-level similarity via Random Forest proximities.\n",
        "#  • The STRAPSim Algorithm: A novel, greedy bipartite matching approach that is:\n",
        "#      - Semantic: Matches constituents based on learned similarity, not just identity.\n",
        "#      - Weight-Aware: Accounts for the economic significance of each holding.\n",
        "#      - Residual-Aware: Dynamically updates weights to prevent over-counting.\n",
        "#  • Comprehensive benchmarking against standard (Jaccard, Weighted Jaccard) and\n",
        "#    advanced (adapted BERTScore) similarity metrics.\n",
        "#  • Rigorous evaluation framework using Spearman rank correlation against a\n",
        "#    market-based ground truth (historical return correlation).\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • End-to-end, modular pipeline from data validation to final results aggregation.\n",
        "#  • Robust data cleansing and feature engineering for institutional fixed-income data.\n",
        "#  • Cross-validated hyperparameter optimization for the Random Forest model.\n",
        "#  • Memory-efficient and computationally optimized algorithm implementations.\n",
        "#  • A full suite of robustness checks, including sensitivity analyses for\n",
        "#    hyperparameters, data partitioning, and metric-specific parameters.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Li, M., Desai, D., Jeyapaulraj, J., Sommer, P., Jain, R., Chu, P., & Mehta, D.\n",
        "#  (2025). STRAPSim: A Portfolio Similarity Metric for ETF Alignment and\n",
        "#  Portfolio Trades. arXiv preprint arXiv:2509.24151.\n",
        "#  https://arxiv.org/abs/2509.24151\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "# Standard Library Imports\n",
        "import copy\n",
        "import itertools\n",
        "import logging\n",
        "import math\n",
        "import time\n",
        "from typing import Any, Dict, List, Set, Tuple, Union\n",
        "\n",
        "# Third-Party Library Imports\n",
        "# Core data manipulation and numerical computation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Machine learning and statistical analysis\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Performance and utility\n",
        "import jsonschema\n",
        "from numba import njit\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define custom type hints for complex data structures to improve readability\n",
        "Portfolio = Dict[str, Union[List[str], np.ndarray]]\n",
        "PortfolioData = Dict[str, Portfolio]\n",
        "BERTScoreFullResult = Tuple[float, float, float, float, float, float]\n",
        "STRAPSimResult = Tuple[float, float, float]\n",
        "\n",
        "# Configure logging for the entire application\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")"
      ],
      "metadata": {
        "id": "ERNYizuMqU__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "_HMJGU3kqWI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "\n",
        "## **Discussion of Inputs, Processes and Outputs of Key Callables**\n",
        "### **Input Validation and Data Quality Assurance (Tasks 1-5)**\n",
        "\n",
        "#### **`validate_configuration` (Task 1 Orchestrator)**\n",
        "*   **Inputs:** The raw `config` dictionary.\n",
        "*   **Processes:** Orchestrates a three-part validation. It first calls `_validate_config_schema` to check the entire nested structure and data types against a rigorous `jsonschema`. It then calls the `_validate_config_dates` to verify the exact string values, format, and business-day logic of all date parameters. Finally, it calls the `_validate_config_values` to exhaustively check every other parameter's value against the study's hardcoded specifications.\n",
        "*   **Outputs:** A tuple `(bool, List[str])` indicating pass/fail status and a list of detailed error messages.\n",
        "*   **Transformation:** This function transforms a potentially invalid configuration dictionary into a boolean validation result.\n",
        "*   **Role in Research:** This callable serves as the foundational gatekeeper for the entire study. It ensures that the experiment is run with the exact parameters specified by the authors, which is the first and most critical step in achieving a faithful replication. It enforces the methodological constants of the experiment.\n",
        "\n",
        "#### **`validate_etf_holdings_dataframe` (Task 2 Orchestrator)**\n",
        "*   **Inputs:** The raw `etf_holdings_df` and the validated `config` dictionary.\n",
        "*   **Processes:** Orchestrates a three-part validation. It checks the DataFrame's structure (column names, ETF count). It verifies the data type of each column. Finally, it calls the `_validate_holdings_quality` to perform deep content checks, including identifier lengths, numerical ranges, per-ETF weight summation to 1.0, and the crucial CUSIP check digit validation.\n",
        "*   **Outputs:** A tuple `(bool, List[str])` indicating pass/fail status and a list of errors.\n",
        "*   **Transformation:** Transforms a raw holdings DataFrame into a boolean validation result.\n",
        "*   **Role in Research:** This function ensures the integrity of the portfolio composition data. The per-ETF weight summation check is particularly critical, as the entire study is predicated on portfolios being valid, weighted distributions of assets.\n",
        "\n",
        "#### **`validate_bond_features_dataframe` (Task 3 Orchestrator)**\n",
        "*   **Inputs:** The raw `bond_features_df` and the validated `config` dictionary.\n",
        "*   **Processes:** Orchestrates a three-part validation. It checks the structure (column names, unique bond count, CUSIP primary key integrity). It performs a rigorous dtype check, paying special attention to the `bond_rating_composite` to ensure it is an *ordered* categorical type, which is essential for the Random Forest model to potentially leverage this ordinal information. Finally, it validates all numerical ranges and enforces the \"no missing values\" constraint from the paper.\n",
        "*   **Outputs:** A tuple `(bool, List[str])` indicating pass/fail status and a list of errors.\n",
        "*   **Transformation:** Transforms a raw features DataFrame into a boolean validation result.\n",
        "*   **Role in Research:** This function ensures the quality of the raw data that will be used to train the core machine learning model. The integrity of these features is paramount, as they are the basis for learning the constituent-level similarity metric.\n",
        "\n",
        "#### **`validate_monthly_returns_dataframe` (Task 4 Orchestrator)**\n",
        "*   **Inputs:** The raw `monthly_returns_df`, the validated `etf_holdings_df`, and the `config` dictionary.\n",
        "*   **Processes:** Orchestrates a three-part validation. It checks the structure (ETF column count and names against holdings data, number of observations). It validates the temporal integrity of the 'date' column (chronological, unique, month-end business day). Finally, it calls the `_validate_returns_quality` to check for missing values, decimal format, and the required 4-decimal-place precision.\n",
        "*   **Outputs:** A tuple `(bool, List[str])` indicating pass/fail status and a list of errors.\n",
        "*   **Transformation:** Transforms a raw returns DataFrame into a boolean validation result.\n",
        "*   **Role in Research:** This function ensures the integrity of the \"ground truth\" data. As the paper states, \"we use historical monthly return correlations as a calibration benchmark.\" Flaws in this data would invalidate the entire evaluation of STRAPSim's performance.\n",
        "\n",
        "#### **`validate_cross_dataset_consistency` (Task 5 Orchestrator)**\n",
        "*   **Inputs:** The three raw DataFrames (`etf_holdings_df`, `bond_features_df`, `monthly_returns_df`) and the `config` dictionary.\n",
        "*   **Processes:** Orchestrates three referential integrity checks. It verifies that every CUSIP in the holdings data exists in the features data. It ensures a perfect one-to-one match between the ETF identifiers in the holdings and returns data. Finally, it confirms that the valuation date of the holdings aligns perfectly with the end date of the return series.\n",
        "*   **Outputs:** A tuple `(bool, List[str])` indicating pass/fail status and a list of errors.\n",
        "*   **Transformation:** Transforms three separate DataFrames into a single boolean validation of their mutual consistency.\n",
        "*   **Role in Research:** This is the final gatekeeper of the validation phase. It ensures that the three disparate sources of data can be safely joined and analyzed as a single, coherent universe, preventing errors from misaligned identifiers or dates.\n",
        "\n",
        "--\n",
        "\n",
        "### **Data Cleansing (Tasks 6-8)**\n",
        "\n",
        "#### **`cleanse_etf_holdings_dataframe` (Task 6 Orchestrator)**\n",
        "*   **Inputs:** The validated raw `etf_holdings_df` and `config`.\n",
        "*   **Processes:** Orchestrates a multi-step cleansing pipeline. It standardizes all identifier strings (whitespace, casing). It then aggregates any duplicate holdings for a given CUSIP within a single ETF, summing their weights. Critically, it then re-normalizes the weights for every ETF to ensure they sum exactly to 1.0 after the aggregation.\n",
        "*   **Outputs:** A cleansed `pandas.DataFrame` with standardized identifiers, unique holdings per ETF, and perfectly normalized weights.\n",
        "*   **Transformation:** Transforms a validated but potentially messy DataFrame into a canonical, analytically pure representation.\n",
        "*   **Role in Research:** This function prepares the portfolio data for the similarity algorithms, ensuring that each portfolio is a unique, valid probability distribution over its constituents.\n",
        "\n",
        "#### **`cleanse_bond_features_dataframe` (Task 7 Orchestrator)**\n",
        "*   **Inputs:** The validated raw `bond_features_df` and `config`.\n",
        "*   **Processes:** Standardizes all categorical text fields. It then applies specific, rule-based corrections to numerical data (e.g., setting minimum `age_days` to 1, imputing invalid `coupon_frequency` with the mode). Finally, it performs a hard validation to ensure there are no missing values in the target variable columns, raising an error if any are found, as per the paper's claim: \"As the dataset contained no missing values, no imputation was required.\"\n",
        "*   **Outputs:** A cleansed `pandas.DataFrame` with standardized features and validated targets.\n",
        "*   **Transformation:** Transforms a raw features DataFrame into a clean, canonical format ready for feature engineering.\n",
        "*   **Role in Research:** This function prepares the predictor and target variables for the machine learning model, ensuring consistency and adherence to logical business rules.\n",
        "\n",
        "#### **`cleanse_monthly_returns_dataframe` (Task 8 Orchestrator)**\n",
        "*   **Inputs:** The validated raw `monthly_returns_df` and `config`.\n",
        "*   **Processes:** Standardizes the time-series structure by setting a clean, sorted `DatetimeIndex`. It enforces the \"no missing values\" policy by raising an error if any are found. Finally, it formats the numerical return values by rounding them to the specified 4-decimal-place precision.\n",
        "*   **Outputs:** A cleansed `pandas.DataFrame` with a `DatetimeIndex` and consistently formatted return values.\n",
        "*   **Transformation:** Transforms a raw returns DataFrame into a clean, properly indexed time-series object.\n",
        "*   **Role in Research:** This function prepares the ground-truth data for the final correlation calculation, ensuring it is in the optimal format for econometric analysis.\n",
        "\n",
        "--\n",
        "\n",
        "### **Feature Engineering and Model Training (Tasks 9-14)**\n",
        "\n",
        "#### **`prepare_categorical_features` (Task 9 Orchestrator)**\n",
        "*   **Inputs:** The cleansed `bond_features_df` and `config`.\n",
        "*   **Processes:** Identifies the categorical columns to be encoded based on the configuration. It analyzes and reports their cardinality. It then applies one-hot encoding using `pandas.get_dummies` to transform these columns into a wide matrix of binary numerical features.\n",
        "*   **Outputs:** A `pandas.DataFrame` containing only the new one-hot encoded features, indexed by CUSIP.\n",
        "*   **Transformation:** Transforms a set of categorical columns into a high-dimensional numerical representation.\n",
        "*   **Role in Research:** This function implements the feature engineering step for categorical variables described in Section 3.2: \"Categorical features... were transformed using one-hot encoding.\"\n",
        "\n",
        "#### **`prepare_numerical_features` (Task 10 Orchestrator)**\n",
        "*   **Inputs:** The cleansed `bond_features_df`, `config`, and an optional `scaling_params` Series.\n",
        "*   **Processes:** Operates in two modes. In \"training\" mode (`scaling_params` is `None`), it identifies the numerical columns, computes their maximum values, and stores these as the scaling parameters. It then scales the features by dividing each by its corresponding maximum. In \"inference\" mode, it uses the provided `scaling_params` to scale the data.\n",
        "*   **Outputs:** A tuple containing the scaled numerical `pandas.DataFrame` and the `pandas.Series` of scaling parameters used.\n",
        "*   **Transformation:** Transforms numerical features from their original scale to a [0, 1] range.\n",
        "*   **Role in Research:** This function implements the feature scaling mentioned in Section 4.1: \"feature values are first scaled by their maximum to ensure comparability across dimensions.\" The two-mode design is a best practice to prevent data leakage from the test set into the training process.\n",
        "\n",
        "#### **`construct_and_split_data` (Task 11 Orchestrator)**\n",
        "*   **Inputs:** The cleansed `bond_features_df`, the `categorical_features_encoded` DataFrame, the `numerical_features_scaled` DataFrame, and `config`.\n",
        "*   **Processes:** Merges the categorical and numerical feature sets into a single complete feature matrix, `X`. It then extracts the target variables (`oas_bp`, `yield_to_maturity`) and ensures their row order is perfectly aligned with `X`. Finally, it uses `sklearn.model_selection.train_test_split` to partition both `X` and `Y` into training and testing sets according to the configuration (90/10 split, fixed random state).\n",
        "*   **Outputs:** A tuple of four DataFrames: `(X_train, X_test, y_train, y_test)`.\n",
        "*   **Transformation:** Transforms the complete, aligned datasets into the standard partitioned format required for supervised machine learning.\n",
        "*   **Role in Research:** This function implements the data partitioning described in Section 3.2: \"The data were randomly shuffled and split into 90% for training and 10% for testing.\"\n",
        "\n",
        "#### **`optimize_random_forest_hyperparameters` (Task 12 Orchestrator)**\n",
        "*   **Inputs:** `X_train`, `y_train`, and `config`.\n",
        "*   **Processes:** Implements a grid search with 5-fold cross-validation to find the optimal `n_estimators` and `max_depth` for the `RandomForestRegressor`. It systematically trains and evaluates models for each combination of hyperparameters in the search space, using Root Mean Squared Error (RMSE) as the evaluation metric.\n",
        "*   **Outputs:** A dictionary containing the best-performing hyperparameter values.\n",
        "*   **Transformation:** Transforms a training dataset and a parameter search space into a single, optimal set of parameters.\n",
        "*   **Role in Research:** This function implements the hyperparameter tuning process described in Section 3.2: \"We applied 5-fold cross-validation on the training set to perform hyperparameter tuning, specifically optimizing the number of trees and maximum depth of the model.\"\n",
        "\n",
        "#### **`train_final_model_and_evaluate` (Task 13 Orchestrator)**\n",
        "*   **Inputs:** The four data splits (`X_train`, `y_train`, `X_test`, `y_test`), the `optimal_params` dictionary, and `config`.\n",
        "*   **Processes:** Instantiates a new `RandomForestRegressor` with the optimal hyperparameters and trains it on the *entire* training dataset. It then generates predictions on both the train and test sets and calculates the final RMSE and MAPE, comparing them against the benchmark values reported in the paper to validate the model's fidelity.\n",
        "*   **Outputs:** The final, trained `sklearn.ensemble.RandomForestRegressor` object.\n",
        "*   **Transformation:** Transforms the training data and optimal parameters into a trained, predictive model.\n",
        "*   **Role in Research:** This function produces the final model that will be used to generate the constituent-level similarity scores. The evaluation step confirms that our model has a similar performance profile to the one in the paper, as reported in Section 3.2: \"On the training data, the model achieved an RMSE of 0.21... On the testing data, the RMSE... were 0.51...\"\n",
        "\n",
        "#### **`generate_proximity_matrix` (Task 14 Orchestrator)**\n",
        "*   **Inputs:** The `final_model` object and the complete, unsplit feature matrix `X_complete`.\n",
        "*   **Processes:** This function operationalizes the concept of Random Forest proximity. It uses the trained model's `.apply()` method to find the terminal leaf node for every bond in every tree. It then systematically counts, for every pair of bonds, how many times they landed in the same leaf node. This count is divided by the total number of trees to get a proximity score between 0 and 1.\n",
        "*   **Outputs:** A square, symmetric `pandas.DataFrame` (the proximity matrix `S`), where `S[i, j]` is the proximity score between bond `i` and bond `j`.\n",
        "*   **Transformation:** Transforms a trained model and a feature matrix into a pairwise similarity matrix.\n",
        "*   **Role in Research:** This function is the direct implementation of the constituent-level similarity method described in Section 2.1.1 and Equation (2):\n",
        "    $$\n",
        "    \\text{Proximity}(x_i, x_j) = \\frac{1}{T} \\sum_{t=1}^{T} I[\\text{Leaf}_t(x_i) = \\text{Leaf}_t(x_j)]\n",
        "    $$\n",
        "    This matrix is the fundamental input for the STRAPSim and adapted BERTScore algorithms.\n",
        "\n",
        "--\n",
        "\n",
        "### **Similarity Computation and Evaluation (Tasks 15-24)**\n",
        "\n",
        "#### **`extract_portfolio_level_data` (Task 15 Orchestrator)**\n",
        "*   **Inputs:** The cleansed `etf_holdings_df` and the `proximity_matrix_df`.\n",
        "*   **Processes:** Transforms the flat holdings DataFrame into a structured, nested dictionary. For each ETF, it creates an entry containing its constituent CUSIPs, their weights, and, critically, their integer indices corresponding to the rows/columns of the proximity matrix.\n",
        "*   **Outputs:** A `PortfolioData` dictionary, optimized for use in the similarity algorithms.\n",
        "*   **Transformation:** Transforms a DataFrame into a structured dictionary with pre-computed indices for efficient lookups.\n",
        "*   **Role in Research:** This is a crucial data preparation step that structures the portfolio data in a way that makes the subsequent, computationally intensive similarity calculations much more efficient.\n",
        "\n",
        "#### **`compute_strapsim_between_pair` (Task 16 Core Algorithm)**\n",
        "*   **Inputs:** Two portfolio data dictionaries, the `proximity_matrix` as a NumPy array, and `config`.\n",
        "*   **Processes:** This is the core implementation of the STRAPSim algorithm. It performs a greedy matching process by iterating through all pairs of constituents (pre-sorted by their proximity score). In each step, it transfers the minimum available weight between the best-matched pair, updates a cumulative similarity score, and decrements the residual weights of the involved constituents.\n",
        "*   **Outputs:** A tuple containing the final STRAPSim score and the final residual weights for both portfolios.\n",
        "*   **Transformation:** Transforms two weighted sets and a similarity matrix into a single scalar similarity score.\n",
        "*   **Role in Research:** This is the direct and complete implementation of the novel algorithm proposed in the paper, as defined by Equations (1a) and (1b):\n",
        "    $$\n",
        "    \\text{STRAPSim}(x, y) = \\sum_{i,j=\\text{argsort}(S)} S_{ij} \\min(w_x^{(t)}(i), w_y^{(t)}(j))\n",
        "    $$\n",
        "    $$\n",
        "    \\begin{aligned}\n",
        "    w_x^{(t+1)}(i) &= w_x^{(t)}(i) - \\min(w_x^{(t)}(i), w_y^{(t)}(j)) \\\\\n",
        "    w_y^{(t+1)}(j) &= w_y^{(t)}(j) - \\min(w_x^{(t)}(i), w_y^{(t)}(j))\n",
        "    \\end{aligned}\n",
        "    $$\n",
        "\n",
        "#### **`compute_jaccard_matrix` (Task 18 Orchestrator)**\n",
        "*   **Inputs:** The `PortfolioData` dictionary and `config`.\n",
        "*   **Processes:** For each unique pair of portfolios, it converts their constituent lists to sets and computes the ratio of the size of their intersection to the size of their union.\n",
        "*   **Outputs:** A square `pandas.DataFrame` of pairwise Jaccard similarity scores.\n",
        "*   **Transformation:** Transforms the portfolio data into a matrix of set-based similarity scores.\n",
        "*   **Role in Research:** Implements the first baseline metric, the Jaccard Index, as defined in Equation (3):\n",
        "    $$\n",
        "    J(X, Y) = \\frac{|X \\cap Y|}{|X \\cup Y|}\n",
        "    $$\n",
        "\n",
        "#### **`compute_weighted_jaccard_matrix` (Task 19 Orchestrator)**\n",
        "*   **Inputs:** The `PortfolioData` dictionary and `config`.\n",
        "*   **Processes:** For each unique pair of portfolios, it computes the weighted intersection (sum of minimum weights for common assets) and the weighted union (sum of maximum weights for all assets) and finds their ratio.\n",
        "*   **Outputs:** A square `pandas.DataFrame` of pairwise Weighted Jaccard scores.\n",
        "*   **Transformation:** Transforms the portfolio data into a matrix of weight-aware, set-based similarity scores.\n",
        "*   **Role in Research:** Implements the second baseline metric, the Weighted Jaccard Index, as defined in Equation (4):\n",
        "    $$\n",
        "    J_w(X, Y) = \\frac{\\sum_{k \\in X \\cap Y} \\min(w_x(k), w_y(k))}{\\sum_{k \\in X \\cup Y} \\max(w_x(k), w_y(k))}\n",
        "    $$\n",
        "\n",
        "#### **`compute_bertscore_matrix` (Task 20 Orchestrator, Remediated)**\n",
        "*   **Inputs:** The `PortfolioData` dictionary, `proximity_matrix`, `config`, and a `weight_scheme`.\n",
        "*   **Processes:** For each unique pair of portfolios, it calls the remediated `_compute_bertscore_for_pair` helper. This helper calculates the Recall (weighted average of best matches from X to Y), Precision (weighted average of best matches from Y to X), and their harmonic mean (F1 score). It also calculates the corresponding residuals.\n",
        "*   **Outputs:** A tuple of six `pandas.DataFrame`s for Recall, Precision, F1, and their respective residuals.\n",
        "*   **Transformation:** Transforms the portfolio data and proximity matrix into six matrices of semantic similarity scores and residuals.\n",
        "*   **Role in Research:** Implements the third baseline, the adapted BERTScore, as defined in Equations (6a, 6b, 6c) and the residual Equations (7a, 7b, 7c).\n",
        "\n",
        "#### **`compute_return_correlation_matrix` (Task 21 Orchestrator)**\n",
        "*   **Inputs:** The cleansed `monthly_returns_df` and `config`.\n",
        "*   **Processes:** Uses the highly optimized `.corr()` method to compute the entire pairwise Pearson correlation matrix for all ETF return series.\n",
        "*   **Outputs:** A square `pandas.DataFrame` of pairwise return correlations.\n",
        "*   **Transformation:** Transforms a time-series DataFrame into a correlation matrix.\n",
        "*   **Role in Research:** This function computes the \"calibration benchmark\" or \"ground truth\" matrix, as described in Section 4.3. This matrix represents the market-based measure of economic similarity that the model-based metrics aim to approximate.\n",
        "\n",
        "#### **`prepare_data_for_spearman_analysis` (Task 22 Orchestrator)**\n",
        "*   **Inputs:** A dictionary of all computed similarity matrices and the `correlation_matrix`.\n",
        "*   **Processes:** Transforms all the wide-format (N x N) matrices into a single, long-format \"tidy\" DataFrame. It then performs a grouped ranking operation, calculating the ranks of the similarity scores and correlation values independently for each reference ETF.\n",
        "*   **Outputs:** A single, tidy `pandas.DataFrame` containing the raw scores and the calculated ranks, ready for statistical testing.\n",
        "*   **Transformation:** Transforms multiple wide matrices into a single, long, ranked DataFrame.\n",
        "*   **Role in Research:** This is the final data preparation step before the evaluation. It structures the data in the precise format required to perform the main statistical test of the paper: comparing the *rankings* produced by each method.\n",
        "\n",
        "#### **`compute_spearman_rank_correlation` (Task 23 Orchestrator)**\n",
        "*   **Inputs:** The tidy `analysis_df` from the previous step and `config`.\n",
        "*   **Processes:** Groups the data by `['reference_etf', 'method']` and applies the `scipy.stats.spearmanr` test to the rank columns of each group. This computes the Spearman correlation coefficient ($\\rho_s$) and its p-value for every method from the perspective of every ETF.\n",
        "*   **Outputs:** A `pandas.DataFrame` summarizing the results (correlation and p-value) for each of the 80 individual tests.\n",
        "*   **Transformation:** Transforms the ranked data into a set of statistical test results.\n",
        "*   **Role in Research:** This function performs the primary statistical evaluation of the paper, as described in Section 4.3: \"we compute the Spearman rank correlation between similarity scores and return correlations across all ETF pairs.\"\n",
        "\n",
        "#### **`aggregate_spearman_results` (Task 24 Orchestrator)**\n",
        "*   **Inputs:** The `spearman_results_df` from the previous step and `config`.\n",
        "*   **Processes:** Groups the detailed test results by `method` and calculates the final summary statistics: average correlation, average p-value, and the percentage of tests that were statistically significant at the 5% and 10% levels.\n",
        "*   **Outputs:** A final, formatted `pandas.DataFrame` that is a direct replication of Table 4 from the paper.\n",
        "*   **Transformation:** Transforms the detailed test results into the final, aggregated summary table.\n",
        "*   **Role in Research:** This function produces the final, conclusive result of the study, allowing for a direct comparison of STRAPSim's performance against the baselines, as presented in Table 4: \"Statistics of Spearman's Rank Correlation.\"\n",
        "\n",
        "--\n",
        "\n",
        "### **Robustness Analysis (Tasks 25-28)**\n",
        "\n",
        "#### **`run_full_study` (Top-Level Orchestrator)**\n",
        "*   **Inputs:** The three raw DataFrames and the `config`.\n",
        "*   **Processes:** This is the highest-level orchestrator. It first calls `run_strapsim_pipeline_for_robustness` to execute the main analysis once and generate a comprehensive dictionary of all necessary intermediate data artifacts. It then passes this entire dictionary of artifacts to `run_robustness_analysis_suite`.\n",
        "*   **Outputs:** A nested dictionary containing both the main pipeline artifacts and the summary results from all robustness checks.\n",
        "*   **Transformation:** Transforms the raw data into the complete, final set of results for the entire study, including all sensitivity analyses.\n",
        "*   **Role in Research:** This function provides a single, convenient entry point to run the entire replication from start to finish, embodying the principles of reproducibility and ease of use.\n",
        "\n",
        "#### **`run_robustness_analysis_suite` (Robustness Master Orchestrator)**\n",
        "*   **Inputs:** The `pipeline_artifacts` dictionary.\n",
        "*   **Processes:** This function acts as a sub-orchestrator for the robustness phase. It unpacks the necessary artifacts and calls the three main sensitivity analysis functions in sequence: `run_hyperparameter_sensitivity_analysis`, `run_data_split_sensitivity_analysis`, and the remediated `run_metric_component_sensitivity_analysis`.\n",
        "*   **Outputs:** A dictionary where each key is the name of a sensitivity analysis and each value is the corresponding summary DataFrame.\n",
        "*   **Transformation:** Transforms the set of main pipeline artifacts into a set of summary tables for each robustness check.\n",
        "*   **Role in Research:** This function encapsulates the entire suite of validation checks performed to test the stability and reliability of the main findings, as described in Tasks 26, 27, and 28.\n",
        "\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **Usage Example**\n",
        "\n",
        "### **Example Usage: Executing the End-to-End STRAPSim Study**\n",
        "\n",
        "This script serves as a practical guide for executing the complete research pipeline developed to replicate the \"STRAPSim\" paper. It follows a logical, three-step process:\n",
        "\n",
        "1.  **Data and Configuration Loading:** It begins by loading the three required raw datasets (ETF holdings, bond features, and monthly returns) into `pandas` DataFrames. It also loads the master configuration from the `config.yaml` file. This separation of data, configuration, and code is a critical best practice.\n",
        "2.  **Main Pipeline Execution:** It then calls the primary orchestrator, `run_full_study`. This single function call triggers the entire end-to-end process: data validation, cleansing, feature engineering, model training, similarity computation, and final evaluation.\n",
        "3.  **Robustness Analysis Execution:** The `run_full_study` function internally passes the artifacts from the main pipeline to the `run_robustness_analysis_suite`, which then conducts all the specified sensitivity tests.\n",
        "4.  **Results Inspection:** Finally, the script demonstrates how to access and inspect the key outputs from the returned results dictionary, such as the final summary table that replicates Table 4 from the paper.\n",
        "\n",
        "This example assumes that all the modular functions developed in Tasks 1 through 28 are organized into appropriate modules (e.g., `validation.py`, `pipeline.py`, `robustness.py`) and are importable.\n",
        "\n",
        "\n",
        "```python\n",
        "#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "#\n",
        "#  Example Execution Script for the STRAPSim Research Pipeline\n",
        "#\n",
        "#  This script demonstrates the end-to-end usage of the STRAPSim analytical\n",
        "#  framework. It simulates a real-world workflow where a researcher loads the\n",
        "#  raw data and a configuration file, and then executes the entire study with a\n",
        "#  single command.\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import yaml\n",
        "import logging\n",
        "from typing import Any, Dict\n",
        "\n",
        "# Assume all the orchestrator functions are located in a module named 'strapsim_pipeline'\n",
        "# from strapsim_pipeline import run_full_study\n",
        "\n",
        "# --- Helper function to create realistic dummy data for the example ---\n",
        "# In a real scenario, this data would be loaded from CSV files, a database,\n",
        "# or a data provider API (e.g., Bloomberg, Refinitiv).\n",
        "\n",
        "def create_dummy_data() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Creates realistic, but simplified, dummy data for demonstration.\"\"\"\n",
        "    # Holdings Data\n",
        "    holdings_data = {\n",
        "        'etf_id': ['ETF_A', 'ETF_A', 'ETF_B', 'ETF_B'],\n",
        "        'etf_name': ['ETF A', 'ETF A', 'ETF B', 'ETF B'],\n",
        "        'cusip': ['CUSIP001', 'CUSIP002', 'CUSIP001', 'CUSIP003'],\n",
        "        'isin': ['ISIN001', 'ISIN002', 'ISIN001', 'ISIN003'],\n",
        "        'sedol': ['SEDOL01', 'SEDOL02', 'SEDOL01', 'SEDOL03'],\n",
        "        'weight': [0.6, 0.4, 0.5, 0.5],\n",
        "        'market_value_usd': [600000, 400000, 500000, 500000],\n",
        "        'shares_held': [600, 400, 500, 500],\n",
        "        'as_of_date': pd.to_datetime('2024-03-31')\n",
        "    }\n",
        "    etf_holdings_df = pd.DataFrame(holdings_data)\n",
        "\n",
        "    # Features Data (simplified for brevity)\n",
        "    features_data = {\n",
        "        'cusip': ['CUSIP001', 'CUSIP002', 'CUSIP003'],\n",
        "        'issuer': ['Issuer X', 'Issuer Y', 'Issuer X'],\n",
        "        'industry': ['Tech', 'Finance', 'Tech'],\n",
        "        'days_to_maturity': [1825, 3650, 2190],\n",
        "        'age_days': [730, 365, 1095],\n",
        "        'bond_rating_composite': ['AA', 'A', 'AA-'],\n",
        "        'country_of_risk': ['USA', 'USA', 'CAN'],\n",
        "        'coupon_rate': [2.5, 3.0, 2.75],\n",
        "        'coupon_frequency': [2, 2, 2],\n",
        "        'amount_issued_usd': [1e9, 5e8, 7.5e8],\n",
        "        'rule_144a_flag': [False, False, True],\n",
        "        'oas_bp': [50.0, 75.0, 60.0],\n",
        "        'yield_to_maturity': [4.5, 5.0, 4.8]\n",
        "    }\n",
        "    bond_features_df = pd.DataFrame(features_data)\n",
        "    \n",
        "    # This is a simplified example. The real data would have 6870 bonds.\n",
        "    # The rating column would need to be an ordered categorical.\n",
        "    rating_order = [\n",
        "        'AAA', 'AA+', 'AA', 'AA-', 'A+', 'A', 'A-', 'BBB+', 'BBB', 'BBB-',\n",
        "        'BB+', 'BB', 'BB-', 'B+', 'B', 'B-', 'CCC+', 'CCC', 'CCC-', 'CC', 'C', 'D'\n",
        "    ]\n",
        "    bond_features_df['bond_rating_composite'] = pd.Categorical(\n",
        "        bond_features_df['bond_rating_composite'], categories=rating_order, ordered=True\n",
        "    )\n",
        "\n",
        "\n",
        "    # Returns Data\n",
        "    dates = pd.to_datetime(['2024-02-29', '2024-03-31']) # Simplified time series\n",
        "    returns_data = {\n",
        "        'date': dates,\n",
        "        'ETF_A': [0.005, -0.002],\n",
        "        'ETF_B': [0.006, -0.001]\n",
        "    }\n",
        "    monthly_returns_df = pd.DataFrame(returns_data)\n",
        "    \n",
        "    # In the real run, the data would have 20 ETFs and 26 months of returns.\n",
        "    \n",
        "    return etf_holdings_df, bond_features_df, monthly_returns_df\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ==========================================================================\n",
        "    # Step 1: Load Data and Configuration\n",
        "    # ==========================================================================\n",
        "    \n",
        "    # --- Load Configuration from YAML file ---\n",
        "    # In a real application, the YAML file is the single source of truth for all parameters.\n",
        "    # This decouples the code from the experimental setup.\n",
        "    try:\n",
        "        # Open and read the YAML configuration file.\n",
        "        with open('config.yaml', 'r') as f:\n",
        "            # Use the safe_load method to parse the YAML into a Python dictionary.\n",
        "            config: Dict[str, Any] = yaml.safe_load(f)\n",
        "        logging.info(\"Successfully loaded configuration from 'config.yaml'.\")\n",
        "    except FileNotFoundError:\n",
        "        logging.error(\"FATAL: 'config.yaml' not found. The pipeline cannot run without its configuration.\")\n",
        "        exit()\n",
        "    except yaml.YAMLError as e:\n",
        "        logging.error(f\"FATAL: Error parsing 'config.yaml': {e}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Load Raw DataFrames ---\n",
        "    # This section simulates loading the three required raw data files.\n",
        "    # In a production environment, these would be read from CSVs, Parquet files,\n",
        "    # or a database connection.\n",
        "    logging.info(\"Loading raw data files...\")\n",
        "    \n",
        "    # For this example, we use a dummy data generator.\n",
        "    # Replace this with your actual data loading logic, e.g.:\n",
        "    # etf_holdings_df = pd.read_csv('data/etf_holdings.csv', parse_dates=['as_of_date'])\n",
        "    # bond_features_df = pd.read_csv('data/bond_features.csv')\n",
        "    # monthly_returns_df = pd.read_csv('data/monthly_returns.csv', parse_dates=['date'])\n",
        "    \n",
        "    # NOTE: The dummy data is extremely simplified and will not produce the same\n",
        "    # numerical results as the paper. Its purpose is to demonstrate the API\n",
        "    # and the data flow of the pipeline. To replicate the paper, you must use\n",
        "    # the full datasets with 20 ETFs and 6870 bonds.\n",
        "    etf_holdings_df, bond_features_df, monthly_returns_df = create_dummy_data()\n",
        "    \n",
        "    logging.info(\"Raw data loaded successfully.\")\n",
        "\n",
        "    # ==========================================================================\n",
        "    # Step 2: Execute the Full Research Study\n",
        "    # ==========================================================================\n",
        "    \n",
        "    # This is the main entry point to the entire analytical framework.\n",
        "    # The `run_full_study` function orchestrates both the core analysis and\n",
        "    # the subsequent suite of robustness checks.\n",
        "    \n",
        "    # It takes the raw data and the configuration as input.\n",
        "    # It returns a comprehensive dictionary containing all results and artifacts.\n",
        "    try:\n",
        "        # Execute the entire study.\n",
        "        full_study_results = run_full_study(\n",
        "            etf_holdings_df=etf_holdings_df,\n",
        "            bond_features_df=bond_features_df,\n",
        "            monthly_returns_df=monthly_returns_df,\n",
        "            config=config\n",
        "        )\n",
        "        logging.info(\"Full study, including main analysis and robustness checks, completed successfully.\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        # If any part of the pipeline fails, the error will be caught and logged.\n",
        "        logging.error(f\"The research pipeline failed with a critical error: {e}\", exc_info=True)\n",
        "        exit()\n",
        "\n",
        "    # ==========================================================================\n",
        "    # Step 3: Inspect the Results\n",
        "    # ==========================================================================\n",
        "    \n",
        "    # The returned object is a nested dictionary containing all artifacts.\n",
        "    # We can now inspect the key outputs.\n",
        "    \n",
        "    logging.info(\"\\n\" + \"=\"*80)\n",
        "    logging.info(\"Inspecting Key Results from the Study\")\n",
        "    logging.info(\"=\"*80)\n",
        "\n",
        "    # --- Main Analysis Results ---\n",
        "    # The main pipeline's final summary table (replication of Table 4).\n",
        "    # Note: This is not produced by the artifact-generating pipeline, but is\n",
        "    # implicitly calculated within the robustness checks. We can extract it\n",
        "    # from the first robustness run for inspection.\n",
        "    main_summary_table = full_study_results['main_analysis_artifacts'].get('final_summary_table')\n",
        "    if main_summary_table is not None:\n",
        "        print(\"\\n--- Main Analysis Summary (Table 4 Replication) ---\")\n",
        "        print(main_summary_table.to_string())\n",
        "\n",
        "    # The STRAPSim similarity matrix.\n",
        "    strapsim_matrix = full_study_results['main_analysis_artifacts']['similarity_matrices']['STRAPSim']\n",
        "    print(\"\\n--- STRAPSim Similarity Matrix (Head) ---\")\n",
        "    print(strapsim_matrix.head())\n",
        "\n",
        "    # --- Robustness Analysis Results ---\n",
        "    # The summary table from the hyperparameter sensitivity analysis.\n",
        "    hyperparam_sensitivity_results = full_study_results['robustness_analysis_results']['hyperparameter_sensitivity']\n",
        "    print(\"\\n--- Hyperparameter Sensitivity Analysis Summary ---\")\n",
        "    print(hyperparam_sensitivity_results.to_string())\n",
        "    \n",
        "    # The summary table from the data split sensitivity analysis.\n",
        "    split_sensitivity_results = full_study_results['robustness_analysis_results']['data_split_sensitivity']\n",
        "    print(\"\\n--- Data Split Sensitivity Analysis Summary ---\")\n",
        "    print(split_sensitivity_results.to_string())\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "VlKQTd0gOn7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Core Pipeline"
      ],
      "metadata": {
        "id": "dJejaStoW2ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Configuration Parameter Validation\n",
        "\n",
        "def _create_validation_schema() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Creates the comprehensive jsonschema validation schema for the configuration.\n",
        "\n",
        "    This function defines the precise, exhaustive structure, data types, and\n",
        "    constraints for the entire configuration object, ensuring strict adherence\n",
        "    to the research paper's specifications. This remediated version provides a\n",
        "    complete definition for all nested objects, including the `baseline_methods`\n",
        "    section, leaving no part of the configuration unspecified.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary representing the complete and rigorous\n",
        "                        jsonschema for the configuration dictionary.\n",
        "    \"\"\"\n",
        "    # Define the jsonschema for the configuration dictionary.\n",
        "    # This schema is exhaustive, enforcing the exact structure, types, and\n",
        "    # required keys for all top-level and nested objects.\n",
        "    # 'additionalProperties': False is used globally to ensure no unspecified\n",
        "    # keys are permitted anywhere in the configuration.\n",
        "    schema = {\n",
        "        \"type\": \"object\",\n",
        "        \"additionalProperties\": False,\n",
        "        \"required\": [\n",
        "            'data_setup', 'feature_engineering', 'random_forest_config',\n",
        "            'strapsim_algorithm', 'baseline_methods', 'evaluation_config',\n",
        "            'expected_results', 'computational_config'\n",
        "        ],\n",
        "        \"properties\": {\n",
        "            \"data_setup\": {\n",
        "                \"type\": \"object\",\n",
        "                \"additionalProperties\": False,\n",
        "                \"required\": [\n",
        "                    'valuation_date', 'return_series_start_date',\n",
        "                    'return_series_end_date', 'return_frequency',\n",
        "                    'total_corporate_bond_etfs', 'total_unique_bonds',\n",
        "                    'average_bonds_per_etf', 'return_type',\n",
        "                    'currency_normalization'\n",
        "                ],\n",
        "                \"properties\": {\n",
        "                    \"valuation_date\": {\"type\": \"string\", \"format\": \"date\"},\n",
        "                    \"return_series_start_date\": {\"type\": \"string\", \"format\": \"date\"},\n",
        "                    \"return_series_end_date\": {\"type\": \"string\", \"format\": \"date\"},\n",
        "                    \"return_frequency\": {\"type\": \"string\", \"enum\": [\"M\"]},\n",
        "                    \"total_corporate_bond_etfs\": {\"type\": \"integer\", \"minimum\": 1},\n",
        "                    \"total_unique_bonds\": {\"type\": \"integer\", \"minimum\": 1},\n",
        "                    \"average_bonds_per_etf\": {\"type\": \"integer\", \"minimum\": 1},\n",
        "                    \"return_type\": {\"type\": \"string\"},\n",
        "                    \"currency_normalization\": {\"type\": \"string\"}\n",
        "                }\n",
        "            },\n",
        "            \"feature_engineering\": {\n",
        "                \"type\": \"object\",\n",
        "                \"additionalProperties\": False,\n",
        "                \"required\": [\n",
        "                    'categorical_features_to_one_hot_encode',\n",
        "                    'numerical_features', 'target_variables',\n",
        "                    'training_set_fraction', 'testing_set_fraction',\n",
        "                    'random_state_for_shuffle', 'shuffle_before_split',\n",
        "                    'missing_value_handling', 'validate_no_missing_values'\n",
        "                ],\n",
        "                \"properties\": {\n",
        "                    \"categorical_features_to_one_hot_encode\": {\n",
        "                        \"type\": \"array\", \"items\": {\"type\": \"string\"}\n",
        "                    },\n",
        "                    \"numerical_features\": {\n",
        "                        \"type\": \"array\", \"items\": {\"type\": \"string\"}\n",
        "                    },\n",
        "                    \"target_variables\": {\n",
        "                        \"type\": \"array\", \"items\": {\"type\": \"string\"}\n",
        "                    },\n",
        "                    \"training_set_fraction\": {\"type\": \"number\", \"minimum\": 0.0, \"maximum\": 1.0},\n",
        "                    \"testing_set_fraction\": {\"type\": \"number\", \"minimum\": 0.0, \"maximum\": 1.0},\n",
        "                    \"random_state_for_shuffle\": {\"type\": \"integer\"},\n",
        "                    \"shuffle_before_split\": {\"type\": \"boolean\"},\n",
        "                    \"missing_value_handling\": {\"type\": \"string\"},\n",
        "                    \"validate_no_missing_values\": {\"type\": \"boolean\"}\n",
        "                }\n",
        "            },\n",
        "            \"random_forest_config\": {\n",
        "                \"type\": \"object\",\n",
        "                \"additionalProperties\": False,\n",
        "                \"required\": [\n",
        "                    'model_type', 'cross_validation_folds',\n",
        "                    'cv_scoring_metric', 'hyperparameters_to_tune',\n",
        "                    'tuning_method', 'tuning_objective',\n",
        "                    'expected_training_rmse', 'expected_training_mape',\n",
        "                    'expected_testing_rmse', 'expected_testing_mape',\n",
        "                    'random_state', 'n_jobs', 'multi_output_strategy'\n",
        "                ],\n",
        "                \"properties\": {\n",
        "                    \"model_type\": {\"type\": \"string\"},\n",
        "                    \"cross_validation_folds\": {\"type\": \"integer\", \"minimum\": 2},\n",
        "                    \"cv_scoring_metric\": {\"type\": \"string\"},\n",
        "                    \"hyperparameters_to_tune\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"additionalProperties\": False,\n",
        "                        \"required\": [\"n_estimators\", \"max_depth\"],\n",
        "                        \"properties\": {\n",
        "                            \"n_estimators\": {\n",
        "                                \"type\": \"array\", \"items\": {\"type\": \"integer\"}\n",
        "                            },\n",
        "                            \"max_depth\": {\n",
        "                                \"type\": \"array\",\n",
        "                                \"items\": {\"type\": [\"integer\", \"null\"]}\n",
        "                            }\n",
        "                        }\n",
        "                    },\n",
        "                    \"tuning_method\": {\"type\": \"string\"},\n",
        "                    \"tuning_objective\": {\"type\": \"string\"},\n",
        "                    \"expected_training_rmse\": {\"type\": \"number\"},\n",
        "                    \"expected_training_mape\": {\"type\": \"number\"},\n",
        "                    \"expected_testing_rmse\": {\"type\": \"number\"},\n",
        "                    \"expected_testing_mape\": {\"type\": \"number\"},\n",
        "                    \"random_state\": {\"type\": \"integer\"},\n",
        "                    \"n_jobs\": {\"type\": \"integer\"},\n",
        "                    \"multi_output_strategy\": {\"type\": \"string\"}\n",
        "                }\n",
        "            },\n",
        "            \"strapsim_algorithm\": {\n",
        "                \"type\": \"object\",\n",
        "                \"additionalProperties\": False,\n",
        "                \"required\": [\n",
        "                    'convergence_threshold', 'max_iterations',\n",
        "                    'similarity_threshold', 'weight_precision_tolerance',\n",
        "                    'similarity_function_type', 'proximity_normalization',\n",
        "                    'residual_tracking', 'weight_conservation_validation',\n",
        "                    'greedy_selection_strategy', 'return_similarity_score',\n",
        "                    'return_residuals', 'return_matching_details',\n",
        "                    'precision_digits'\n",
        "                ],\n",
        "                \"properties\": {\n",
        "                    \"convergence_threshold\": {\"type\": \"number\"},\n",
        "                    \"max_iterations\": {\"type\": \"integer\"},\n",
        "                    \"similarity_threshold\": {\"type\": \"number\"},\n",
        "                    \"weight_precision_tolerance\": {\"type\": \"number\"},\n",
        "                    \"similarity_function_type\": {\"type\": \"string\"},\n",
        "                    \"proximity_normalization\": {\"type\": \"boolean\"},\n",
        "                    \"residual_tracking\": {\"type\": \"boolean\"},\n",
        "                    \"weight_conservation_validation\": {\"type\": \"boolean\"},\n",
        "                    \"greedy_selection_strategy\": {\"type\": \"string\"},\n",
        "                    \"return_similarity_score\": {\"type\": \"boolean\"},\n",
        "                    \"return_residuals\": {\"type\": \"boolean\"},\n",
        "                    \"return_matching_details\": {\"type\": \"boolean\"},\n",
        "                    \"precision_digits\": {\"type\": \"integer\"}\n",
        "                }\n",
        "            },\n",
        "            \"baseline_methods\": {\n",
        "                \"type\": \"object\",\n",
        "                \"additionalProperties\": False,\n",
        "                \"required\": [\n",
        "                    'jaccard_config', 'weighted_jaccard_config',\n",
        "                    'bertscore_config'\n",
        "                ],\n",
        "                \"properties\": {\n",
        "                    \"jaccard_config\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"additionalProperties\": False,\n",
        "                        \"required\": [\"exact_match_required\", \"identifier_field\", \"case_sensitive_matching\", \"compute_residual\"],\n",
        "                        \"properties\": {\n",
        "                            \"exact_match_required\": {\"type\": \"boolean\"},\n",
        "                            \"identifier_field\": {\"type\": \"string\"},\n",
        "                            \"case_sensitive_matching\": {\"type\": \"boolean\"},\n",
        "                            \"compute_residual\": {\"type\": \"boolean\"}\n",
        "                        }\n",
        "                    },\n",
        "                    \"weighted_jaccard_config\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"additionalProperties\": False,\n",
        "                        \"required\": [\"weight_field\", \"min_weight_intersection\", \"max_weight_union\", \"weight_normalization_check\", \"exact_match_required\", \"compute_residual\"],\n",
        "                        \"properties\": {\n",
        "                            \"weight_field\": {\"type\": \"string\"},\n",
        "                            \"min_weight_intersection\": {\"type\": \"boolean\"},\n",
        "                            \"max_weight_union\": {\"type\": \"boolean\"},\n",
        "                            \"weight_normalization_check\": {\"type\": \"boolean\"},\n",
        "                            \"exact_match_required\": {\"type\": \"boolean\"},\n",
        "                            \"compute_residual\": {\"type\": \"boolean\"}\n",
        "                        }\n",
        "                    },\n",
        "                    \"bertscore_config\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"additionalProperties\": False,\n",
        "                        \"required\": [\"compute_recall\", \"compute_precision\", \"compute_f1\", \"similarity_function\", \"weight_scheme\", \"bidirectional_matching\", \"compute_residuals\", \"residual_aggregation\"],\n",
        "                        \"properties\": {\n",
        "                            \"compute_recall\": {\"type\": \"boolean\"},\n",
        "                            \"compute_precision\": {\"type\": \"boolean\"},\n",
        "                            \"compute_f1\": {\"type\": \"boolean\"},\n",
        "                            \"similarity_function\": {\"type\": \"string\"},\n",
        "                            \"weight_scheme\": {\"type\": \"string\"},\n",
        "                            \"bidirectional_matching\": {\"type\": \"boolean\"},\n",
        "                            \"compute_residuals\": {\"type\": \"boolean\"},\n",
        "                            \"residual_aggregation\": {\"type\": \"string\"}\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"evaluation_config\": {\n",
        "                \"type\": \"object\",\n",
        "                \"additionalProperties\": False,\n",
        "                \"required\": [\n",
        "                    'ground_truth_metric', 'return_correlation_method',\n",
        "                    'primary_evaluation_metric', 'correlation_test_type',\n",
        "                    'significance_levels', 'multiple_testing_correction',\n",
        "                    'compute_average_correlation', 'compute_average_pvalue',\n",
        "                    'compute_significance_percentage',\n",
        "                    'exclude_self_comparison', 'ranking_method',\n",
        "                    'handle_ranking_ties', 'minimum_etf_pairs_for_analysis'\n",
        "                ],\n",
        "                \"properties\": {\n",
        "                    \"ground_truth_metric\": {\"type\": \"string\"},\n",
        "                    \"return_correlation_method\": {\"type\": \"string\"},\n",
        "                    \"primary_evaluation_metric\": {\"type\": \"string\"},\n",
        "                    \"correlation_test_type\": {\"type\": \"string\"},\n",
        "                    \"significance_levels\": {\n",
        "                        \"type\": \"array\", \"items\": {\"type\": \"number\"}\n",
        "                    },\n",
        "                    \"multiple_testing_correction\": {\"type\": [\"string\", \"null\"]},\n",
        "                    \"compute_average_correlation\": {\"type\": \"boolean\"},\n",
        "                    \"compute_average_pvalue\": {\"type\": \"boolean\"},\n",
        "                    \"compute_significance_percentage\": {\"type\": \"boolean\"},\n",
        "                    \"exclude_self_comparison\": {\"type\": \"boolean\"},\n",
        "                    \"ranking_method\": {\"type\": \"string\"},\n",
        "                    \"handle_ranking_ties\": {\"type\": \"string\"},\n",
        "                    \"minimum_etf_pairs_for_analysis\": {\"type\": \"integer\"}\n",
        "                }\n",
        "            },\n",
        "            \"expected_results\": {\n",
        "                \"type\": \"object\",\n",
        "                \"additionalProperties\": False,\n",
        "                \"required\": [\n",
        "                    'strapsim_expected_correlation',\n",
        "                    'strapsim_expected_pvalue',\n",
        "                    'strapsim_expected_significance_5pct',\n",
        "                    'strapsim_expected_significance_10pct',\n",
        "                    'jaccard_expected_correlation',\n",
        "                    'weighted_jaccard_expected_correlation',\n",
        "                    'bertscore_expected_correlation',\n",
        "                    'correlation_tolerance', 'pvalue_tolerance',\n",
        "                    'significance_percentage_tolerance'\n",
        "                ],\n",
        "                \"properties\": {\n",
        "                    \"strapsim_expected_correlation\": {\"type\": \"number\"},\n",
        "                    \"strapsim_expected_pvalue\": {\"type\": \"number\"},\n",
        "                    \"strapsim_expected_significance_5pct\": {\"type\": \"integer\"},\n",
        "                    \"strapsim_expected_significance_10pct\": {\"type\": \"integer\"},\n",
        "                    \"jaccard_expected_correlation\": {\"type\": \"number\"},\n",
        "                    \"weighted_jaccard_expected_correlation\": {\"type\": \"number\"},\n",
        "                    \"bertscore_expected_correlation\": {\"type\": \"number\"},\n",
        "                    \"correlation_tolerance\": {\"type\": \"number\"},\n",
        "                    \"pvalue_tolerance\": {\"type\": \"number\"},\n",
        "                    \"significance_percentage_tolerance\": {\"type\": \"integer\"}\n",
        "                }\n",
        "            },\n",
        "            \"computational_config\": {\n",
        "                \"type\": \"object\",\n",
        "                \"additionalProperties\": False,\n",
        "                \"required\": [\n",
        "                    'n_jobs', 'parallel_backend', 'float_precision',\n",
        "                    'similarity_score_precision', 'weight_precision',\n",
        "                    'correlation_precision', 'global_random_seed',\n",
        "                    'ensure_reproducible_results',\n",
        "                    'cache_similarity_matrices',\n",
        "                    'garbage_collection_enabled', 'efficient_data_types'\n",
        "                ],\n",
        "                \"properties\": {\n",
        "                    \"n_jobs\": {\"type\": \"integer\"},\n",
        "                    \"parallel_backend\": {\"type\": \"string\"},\n",
        "                    \"float_precision\": {\"type\": \"string\"},\n",
        "                    \"similarity_score_precision\": {\"type\": \"integer\"},\n",
        "                    \"weight_precision\": {\"type\": \"integer\"},\n",
        "                    \"correlation_precision\": {\"type\": \"integer\"},\n",
        "                    \"global_random_seed\": {\"type\": \"integer\"},\n",
        "                    \"ensure_reproducible_results\": {\"type\": \"boolean\"},\n",
        "                    \"cache_similarity_matrices\": {\"type\": \"boolean\"},\n",
        "                    \"garbage_collection_enabled\": {\"type\": \"boolean\"},\n",
        "                    \"efficient_data_types\": {\"type\": \"boolean\"}\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return schema\n",
        "\n",
        "\n",
        "def _validate_config_schema(\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the structure and types of the config dict against a schema.\n",
        "\n",
        "    This function serves as the first line of defense, ensuring the provided\n",
        "    configuration dictionary has the correct nested structure, keys, and data\n",
        "    types before proceeding to more specific value-based validation.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The configuration dictionary to validate.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages. An empty list indicates success.\n",
        "    \"\"\"\n",
        "    # Initialize a list to store validation error messages.\n",
        "    errors = []\n",
        "    try:\n",
        "        # Create the validation schema.\n",
        "        schema = _create_validation_schema()\n",
        "        # Validate the provided config dictionary against the schema.\n",
        "        # This will raise a ValidationError if the config is invalid.\n",
        "        jsonschema.validate(instance=config, schema=schema)\n",
        "    except jsonschema.ValidationError as e:\n",
        "        # If validation fails, format a detailed error message.\n",
        "        # The message includes the invalid value and the path to it.\n",
        "        errors.append(\n",
        "            f\"Schema validation failed at '{'.'.join(map(str, e.path))}': {e.message}\"\n",
        "        )\n",
        "    except jsonschema.SchemaError as e:\n",
        "        # If the schema itself is invalid, this indicates a programming error.\n",
        "        errors.append(f\"Internal schema definition error: {e.message}\")\n",
        "\n",
        "    # Return the list of accumulated errors.\n",
        "    return errors\n",
        "\n",
        "\n",
        "def _validate_config_dates(\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates date-related parameters for value, format, logic, and business rules.\n",
        "\n",
        "    This remediated function performs a complete and rigorous validation of all\n",
        "    temporal parameters within the configuration. It ensures:\n",
        "    1.  **Exact Value Compliance**: The date strings match the hardcoded values\n",
        "        required for a precise replication of the study.\n",
        "    2.  **Format Integrity**: The date strings are correctly formatted as 'YYYY-MM-DD'.\n",
        "    3.  **Chronological Logic**: The return series start date precedes the end date.\n",
        "    4.  **Business Day Convention**: All dates fall on the last business day of\n",
        "        their respective months, a critical requirement for financial time-series.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The configuration dictionary to validate.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of all date-related validation error messages. An\n",
        "                   empty list signifies that all temporal parameters are valid.\n",
        "    \"\"\"\n",
        "    # Initialize a list to store validation error messages.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Define the expected hardcoded date values for the study.\n",
        "    expected_dates: Dict[str, str] = {\n",
        "        'valuation_date': '2024-03-31',\n",
        "        'return_series_start_date': '2022-02-28',\n",
        "        'return_series_end_date': '2024-03-31'\n",
        "    }\n",
        "\n",
        "    # --- Step 1: Exact Value Validation ---\n",
        "    # First, check if the raw string values match the study's specification.\n",
        "    for key, expected_val in expected_dates.items():\n",
        "        actual_val = config['data_setup'].get(key)\n",
        "        if actual_val != expected_val:\n",
        "            errors.append(\n",
        "                f\"Parameter 'data_setup.{key}' has incorrect value. Expected \"\n",
        "                f\"'{expected_val}', but found '{actual_val}'.\"\n",
        "            )\n",
        "\n",
        "    # If there are already errors in the fundamental values, return early.\n",
        "    if errors:\n",
        "        return errors\n",
        "\n",
        "    # --- Step 2: Format and Logic Validation ---\n",
        "    # Define the expected date format for robust parsing.\n",
        "    date_format = \"%Y-%m-%d\"\n",
        "    # Define paths to the date parameters for iteration.\n",
        "    date_keys: List[str] = list(expected_dates.keys())\n",
        "\n",
        "    # Dictionary to store parsed date objects for subsequent logical checks.\n",
        "    parsed_dates: Dict[str, Union[pd.Timestamp, None]] = {key: None for key in date_keys}\n",
        "\n",
        "    # Loop through each date key to parse and validate the format.\n",
        "    for key in date_keys:\n",
        "        try:\n",
        "            # Access the date string from the nested dictionary.\n",
        "            date_str = config['data_setup'][key]\n",
        "            # Attempt to parse the date string into a pandas Timestamp object.\n",
        "            # This will raise a ValueError if the format is incorrect.\n",
        "            parsed_dates[key] = pd.to_datetime(date_str, format=date_format)\n",
        "        except ValueError:\n",
        "            # This error should ideally not be reached if the schema is correct,\n",
        "            # but it serves as a robust fallback.\n",
        "            errors.append(\n",
        "                f\"Parameter 'data_setup.{key}' ('{date_str}') is not a valid \"\n",
        "                f\"date in 'YYYY-MM-DD' format.\"\n",
        "            )\n",
        "\n",
        "    # If any date failed to parse, return the errors found so far.\n",
        "    if any(d is None for d in parsed_dates.values()):\n",
        "        return errors\n",
        "\n",
        "    # --- Step 3: Chronological and Business Day Convention Validation ---\n",
        "    # Proceed with logical checks now that all dates are parsed successfully.\n",
        "\n",
        "    # Extract the parsed Timestamp objects for convenience.\n",
        "    start_date = parsed_dates['return_series_start_date']\n",
        "    end_date = parsed_dates['return_series_end_date']\n",
        "\n",
        "    # Check 3a: Chronological order of the return series.\n",
        "    if start_date >= end_date:\n",
        "        errors.append(\n",
        "            \"'return_series_start_date' must be strictly before 'return_series_end_date'.\"\n",
        "        )\n",
        "\n",
        "    # Check 3b: Validate that each date is a month-end business day.\n",
        "    for key, date_obj in parsed_dates.items():\n",
        "        # is_on_offset checks if the date is on the specified frequency anchor.\n",
        "        # MonthEnd(0) represents the last business day of the month.\n",
        "        if not pd.tseries.offsets.MonthEnd(0).is_on_offset(date_obj):\n",
        "            errors.append(\n",
        "                f\"Parameter 'data_setup.{key}' with value '{date_obj.date()}' is not a \"\n",
        "                \"month-end business day.\"\n",
        "            )\n",
        "\n",
        "    # Return the list of all accumulated errors.\n",
        "    return errors\n",
        "\n",
        "\n",
        "def _validate_config_values(\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Performs an exhaustive validation of specific parameter values in the config.\n",
        "\n",
        "    This remediated function is a complete and rigorous implementation that\n",
        "    checks every single value-based constraint specified in the instructions for\n",
        "    Task 1, Step 3. It ensures that the configuration is not only structurally\n",
        "    correct but also semantically identical to the one required for a precise\n",
        "    replication of the study. It validates simple values, list contents, and\n",
        "    numerical relationships across all sections of the configuration.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The configuration dictionary to validate.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of all value-related validation error messages. An\n",
        "                   empty list signifies that all values are correct.\n",
        "    \"\"\"\n",
        "    # Initialize a list to store validation error messages.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Helper function for generating consistent error messages.\n",
        "    def check_value(path: str, actual: Any, expected: Any):\n",
        "        if actual != expected:\n",
        "            errors.append(f\"Parameter '{path}' has incorrect value. Expected '{expected}', but found '{actual}'.\")\n",
        "\n",
        "    def check_list_content(path: str, actual: List[Any], expected: List[Any]):\n",
        "        # Use set comparison for order-agnostic check of list contents.\n",
        "        if set(actual) != set(expected):\n",
        "            errors.append(f\"Parameter '{path}' has incorrect list contents. Expected elements: {set(expected)}, but found {set(actual)}.\")\n",
        "\n",
        "    # --- Data Setup Validations ---\n",
        "    check_value('data_setup.total_corporate_bond_etfs', config['data_setup']['total_corporate_bond_etfs'], 20)\n",
        "    check_value('data_setup.total_unique_bonds', config['data_setup']['total_unique_bonds'], 6870)\n",
        "    check_value('data_setup.average_bonds_per_etf', config['data_setup']['average_bonds_per_etf'], 511)\n",
        "    check_value('data_setup.return_type', config['data_setup']['return_type'], 'total_return')\n",
        "    check_value('data_setup.currency_normalization', config['data_setup']['currency_normalization'], 'USD')\n",
        "\n",
        "    # --- Feature Engineering Validations ---\n",
        "    check_list_content(\n",
        "        'feature_engineering.categorical_features_to_one_hot_encode',\n",
        "        config['feature_engineering']['categorical_features_to_one_hot_encode'],\n",
        "        ['issuer', 'industry', 'bond_rating', 'market', 'currency', 'country', 'flag_144a']\n",
        "    )\n",
        "    check_list_content(\n",
        "        'feature_engineering.numerical_features',\n",
        "        config['feature_engineering']['numerical_features'],\n",
        "        ['days_to_maturity', 'age', 'coupon', 'amount_issued', 'coupon_frequency']\n",
        "    )\n",
        "    check_list_content(\n",
        "        'feature_engineering.target_variables',\n",
        "        config['feature_engineering']['target_variables'],\n",
        "        ['oas', 'yield']\n",
        "    )\n",
        "    check_value('feature_engineering.training_set_fraction', config['feature_engineering']['training_set_fraction'], 0.90)\n",
        "    check_value('feature_engineering.testing_set_fraction', config['feature_engineering']['testing_set_fraction'], 0.10)\n",
        "    # Use math.isclose for robust floating-point comparison.\n",
        "    train_frac = config['feature_engineering']['training_set_fraction']\n",
        "    test_frac = config['feature_engineering']['testing_set_fraction']\n",
        "    if not math.isclose(train_frac + test_frac, 1.0, abs_tol=1e-9):\n",
        "        errors.append(\"Sum of 'training_set_fraction' and 'testing_set_fraction' must be 1.0.\")\n",
        "    check_value('feature_engineering.random_state_for_shuffle', config['feature_engineering']['random_state_for_shuffle'], 42)\n",
        "    check_value('feature_engineering.shuffle_before_split', config['feature_engineering']['shuffle_before_split'], True)\n",
        "    check_value('feature_engineering.missing_value_handling', config['feature_engineering']['missing_value_handling'], 'none_required')\n",
        "    check_value('feature_engineering.validate_no_missing_values', config['feature_engineering']['validate_no_missing_values'], True)\n",
        "\n",
        "    # --- Random Forest Config Validations ---\n",
        "    rf_config = config['random_forest_config']\n",
        "    check_value('random_forest_config.model_type', rf_config['model_type'], 'RandomForestRegressor')\n",
        "    check_value('random_forest_config.cross_validation_folds', rf_config['cross_validation_folds'], 5)\n",
        "    check_value('random_forest_config.cv_scoring_metric', rf_config['cv_scoring_metric'], 'neg_root_mean_squared_error')\n",
        "    check_list_content(\n",
        "        'random_forest_config.hyperparameters_to_tune.n_estimators',\n",
        "        rf_config['hyperparameters_to_tune']['n_estimators'],\n",
        "        [50, 100, 200, 500]\n",
        "    )\n",
        "    # Special handling for list containing None.\n",
        "    expected_depths: Set[Union[int, None]] = {5, 10, 15, 20, None}\n",
        "    actual_depths: Set[Union[int, None]] = set(rf_config['hyperparameters_to_tune']['max_depth'])\n",
        "    if actual_depths != expected_depths:\n",
        "        errors.append(f\"Parameter 'random_forest_config.hyperparameters_to_tune.max_depth' has incorrect list contents. Expected elements: {expected_depths}, but found {actual_depths}.\")\n",
        "    check_value('random_forest_config.tuning_method', rf_config['tuning_method'], 'grid_search')\n",
        "    check_value('random_forest_config.tuning_objective', rf_config['tuning_objective'], 'minimize_rmse')\n",
        "    check_value('random_forest_config.random_state', rf_config['random_state'], 42)\n",
        "    check_value('random_forest_config.n_jobs', rf_config['n_jobs'], -1)\n",
        "    check_value('random_forest_config.multi_output_strategy', rf_config['multi_output_strategy'], 'direct')\n",
        "\n",
        "    # --- STRAPSim Algorithm Validations ---\n",
        "    strapsim_config = config['strapsim_algorithm']\n",
        "    check_value('strapsim_algorithm.convergence_threshold', strapsim_config['convergence_threshold'], 1e-6)\n",
        "    check_value('strapsim_algorithm.max_iterations', strapsim_config['max_iterations'], 10000)\n",
        "    check_value('strapsim_algorithm.similarity_threshold', strapsim_config['similarity_threshold'], 0.0)\n",
        "    check_value('strapsim_algorithm.weight_precision_tolerance', strapsim_config['weight_precision_tolerance'], 1e-8)\n",
        "    check_value('strapsim_algorithm.similarity_function_type', strapsim_config['similarity_function_type'], 'random_forest_proximity')\n",
        "    check_value('strapsim_algorithm.residual_tracking', strapsim_config['residual_tracking'], True)\n",
        "    check_value('strapsim_algorithm.weight_conservation_validation', strapsim_config['weight_conservation_validation'], True)\n",
        "    check_value('strapsim_algorithm.greedy_selection_strategy', strapsim_config['greedy_selection_strategy'], 'max_similarity')\n",
        "\n",
        "    # --- Baseline Methods Validations ---\n",
        "    # This confirms the sub-dictionaries contain the required boolean/string parameters.\n",
        "    # The schema validation already confirms their existence.\n",
        "    jaccard_conf = config['baseline_methods']['jaccard_config']\n",
        "    check_value('baseline_methods.jaccard_config.exact_match_required', jaccard_conf.get('exact_match_required'), True)\n",
        "\n",
        "    wj_conf = config['baseline_methods']['weighted_jaccard_config']\n",
        "    check_value('baseline_methods.weighted_jaccard_config.min_weight_intersection', wj_conf.get('min_weight_intersection'), True)\n",
        "\n",
        "    bert_conf = config['baseline_methods']['bertscore_config']\n",
        "    check_value('baseline_methods.bertscore_config.compute_f1', bert_conf.get('compute_f1'), True)\n",
        "\n",
        "    # Return the list of all accumulated errors.\n",
        "    return errors\n",
        "\n",
        "\n",
        "def validate_configuration(\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete validation of the research configuration.\n",
        "\n",
        "    This function serves as the main entry point for configuration validation.\n",
        "    It executes a series of checks in a specific order:\n",
        "    1.  Structural and Type Validation: Ensures the configuration dictionary\n",
        "        has the correct keys, nested structure, and data types.\n",
        "    2.  Date Validation: Verifies the format, logic, and business day rules\n",
        "        for all date-related parameters.\n",
        "    3.  Value Validation: Checks specific numerical and list-based constraints\n",
        "        that are critical for the study's methodology.\n",
        "\n",
        "    The function aggregates all errors from these steps into a single,\n",
        "    comprehensive report, providing a clear pass/fail status.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The complete configuration dictionary for the\n",
        "                                 STRAPSim research pipeline.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[bool, List[str]]: A tuple containing:\n",
        "        - bool: True if the configuration is valid, False otherwise.\n",
        "        - List[str]: A list of detailed error messages. If the list is\n",
        "                     empty, the configuration is valid.\n",
        "    \"\"\"\n",
        "    # Input validation for the function itself.\n",
        "    if not isinstance(config, dict):\n",
        "        return False, [\"Configuration must be a dictionary.\"]\n",
        "\n",
        "    # --- Step 1: Validate the overall structure and data types using a schema ---\n",
        "    # This is the most robust way to check the nested structure.\n",
        "    schema_errors = _validate_config_schema(config)\n",
        "    # If the fundamental structure is wrong, we stop here as subsequent checks\n",
        "    # might fail with KeyErrors.\n",
        "    if schema_errors:\n",
        "        return False, schema_errors\n",
        "\n",
        "    # --- Step 2: Validate date formats, logic, and business rules ---\n",
        "    # This requires custom logic that jsonschema cannot handle.\n",
        "    date_errors = _validate_config_dates(config)\n",
        "\n",
        "    # --- Step 3: Validate specific numerical values and list contents ---\n",
        "    # This also requires custom logic for constraints like sums and set equality.\n",
        "    value_errors = _validate_config_values(config)\n",
        "\n",
        "    # --- Aggregation of all errors ---\n",
        "    # Combine errors from all validation steps into a single list.\n",
        "    all_errors = schema_errors + date_errors + value_errors\n",
        "\n",
        "    # --- Final Verdict ---\n",
        "    # The configuration is valid if and only if the list of errors is empty.\n",
        "    is_valid = not all_errors\n",
        "\n",
        "    # Return the final validation status and the list of all found errors.\n",
        "    return is_valid, all_errors\n"
      ],
      "metadata": {
        "id": "7Vwi-wkeqaES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: ETF Holdings DataFrame Validation\n",
        "\n",
        "def _validate_holdings_structure(\n",
        "    etf_holdings_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the structural integrity of the ETF holdings DataFrame.\n",
        "\n",
        "    This function performs the initial, most fundamental checks on the holdings\n",
        "    DataFrame to ensure it conforms to the expected schema before any deeper\n",
        "    analysis is attempted. It verifies:\n",
        "    1.  The DataFrame is not empty.\n",
        "    2.  It contains the exact set of required columns, with no extras.\n",
        "    3.  The number of unique ETFs matches the study's configuration.\n",
        "\n",
        "    Args:\n",
        "        etf_holdings_df (pd.DataFrame): The DataFrame containing ETF holdings data.\n",
        "        config (Dict[str, Any]): The validated research configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages describing structural violations.\n",
        "                   An empty list signifies that the structure is valid.\n",
        "    \"\"\"\n",
        "    # Initialize a list to store validation error messages.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Check 1: Verify the DataFrame is not empty.\n",
        "    # A non-empty DataFrame is a prerequisite for all other checks.\n",
        "    if etf_holdings_df.empty:\n",
        "        # If it's empty, add a critical error and return immediately.\n",
        "        errors.append(\"ETF holdings DataFrame is empty.\")\n",
        "        return errors\n",
        "\n",
        "    # Check 2: Verify the DataFrame contains the exact required columns.\n",
        "    # Define the expected set of columns as per the specification.\n",
        "    expected_columns: Set[str] = {\n",
        "        'etf_id', 'etf_name', 'cusip', 'isin', 'sedol', 'weight',\n",
        "        'market_value_usd', 'shares_held', 'as_of_date'\n",
        "    }\n",
        "    # Get the actual set of columns from the input DataFrame.\n",
        "    actual_columns: Set[str] = set(etf_holdings_df.columns)\n",
        "\n",
        "    # Compare the actual and expected sets.\n",
        "    if actual_columns != expected_columns:\n",
        "        # If they don't match, identify missing and unexpected columns.\n",
        "        missing_cols = expected_columns - actual_columns\n",
        "        extra_cols = actual_columns - expected_columns\n",
        "        # Format a detailed error message.\n",
        "        error_msg = \"ETF holdings DataFrame column mismatch.\"\n",
        "        if missing_cols:\n",
        "            error_msg += f\" Missing columns: {sorted(list(missing_cols))}.\"\n",
        "        if extra_cols:\n",
        "            error_msg += f\" Extra columns: {sorted(list(extra_cols))}.\"\n",
        "        errors.append(error_msg)\n",
        "\n",
        "    # Check 3: Verify the number of unique ETFs matches the configuration.\n",
        "    # Access the expected number from the config dictionary.\n",
        "    expected_etf_count = config['data_setup']['total_corporate_bond_etfs']\n",
        "    # Calculate the actual number of unique ETFs in the 'etf_id' column.\n",
        "    actual_etf_count = etf_holdings_df['etf_id'].nunique()\n",
        "\n",
        "    # Compare the actual count to the expected count.\n",
        "    if actual_etf_count != expected_etf_count:\n",
        "        # If they differ, format a precise error message.\n",
        "        errors.append(\n",
        "            f\"Expected {expected_etf_count} unique ETFs based on config, but \"\n",
        "            f\"found {actual_etf_count} in 'etf_id' column.\"\n",
        "        )\n",
        "\n",
        "    # Return the list of all accumulated structural errors.\n",
        "    return errors\n",
        "\n",
        "\n",
        "def _validate_holdings_dtypes(\n",
        "    etf_holdings_df: pd.DataFrame\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the data types of each column in the ETF holdings DataFrame.\n",
        "\n",
        "    Ensuring correct data types is crucial for preventing runtime errors and\n",
        "    ensuring the correctness of numerical and logical operations downstream.\n",
        "    This function checks each column against its specified, expected data type.\n",
        "\n",
        "    Args:\n",
        "        etf_holdings_df (pd.DataFrame): The DataFrame containing ETF holdings data.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages for any column with an incorrect\n",
        "                   data type. An empty list signifies all dtypes are correct.\n",
        "    \"\"\"\n",
        "    # Initialize a list to store validation error messages.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Define the expected data type for each column.\n",
        "    # Using string representations allows for flexibility (e.g., 'int64').\n",
        "    expected_dtypes: Dict[str, str] = {\n",
        "        'etf_id': 'object',\n",
        "        'etf_name': 'object',\n",
        "        'cusip': 'object',\n",
        "        'isin': 'object',\n",
        "        'sedol': 'object',\n",
        "        'weight': 'float64',\n",
        "        'market_value_usd': 'float64',\n",
        "        'shares_held': 'int64',\n",
        "        'as_of_date': 'datetime64[ns]'\n",
        "    }\n",
        "\n",
        "    # Iterate through the expected dtypes dictionary to check each column.\n",
        "    for col, expected_dtype in expected_dtypes.items():\n",
        "        # Check if the column exists in the DataFrame to avoid KeyErrors.\n",
        "        if col in etf_holdings_df.columns:\n",
        "            # Get the actual data type of the column.\n",
        "            actual_dtype = etf_holdings_df[col].dtype\n",
        "            # Compare the actual dtype string representation with the expected one.\n",
        "            if str(actual_dtype) != expected_dtype:\n",
        "                # If they don't match, format a detailed error message.\n",
        "                errors.append(\n",
        "                    f\"Column '{col}' has incorrect dtype. Expected \"\n",
        "                    f\"'{expected_dtype}', but found '{actual_dtype}'.\"\n",
        "                )\n",
        "\n",
        "    # Return the list of all accumulated data type errors.\n",
        "    return errors\n",
        "\n",
        "\n",
        "def _is_valid_cusip(cusip: str) -> bool:\n",
        "    \"\"\"\n",
        "    Validates a single CUSIP identifier using its check digit algorithm.\n",
        "\n",
        "    The CUSIP check digit is calculated using a variant of the Luhn algorithm.\n",
        "    This function implements that algorithm precisely to verify the integrity\n",
        "    of a given CUSIP string.\n",
        "\n",
        "    The algorithm is as follows:\n",
        "    1.  Convert all letters to their numeric value (A=10, B=11, ...).\n",
        "    2.  Process the first 8 characters. For each character's numeric value:\n",
        "        a. Multiply the value by 2 if it is in an even position (2nd, 4th, ...).\n",
        "        b. If the result of the multiplication is a two-digit number, sum those\n",
        "           two digits.\n",
        "    3.  Sum the results of all 8 processed character values.\n",
        "    4.  The check digit is `(10 - (total_sum % 10)) % 10`.\n",
        "    5.  Compare this calculated digit to the 9th digit of the CUSIP.\n",
        "\n",
        "    Args:\n",
        "        cusip (str): The 9-character CUSIP string to validate.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the CUSIP's check digit is valid, False otherwise.\n",
        "    \"\"\"\n",
        "    # Pre-computation check: CUSIP must be a 9-character string.\n",
        "    if not isinstance(cusip, str) or len(cusip) != 9:\n",
        "        return False\n",
        "\n",
        "    # CUSIP character value mapping.\n",
        "    # ord(c) - ord('A') + 10 maps A->10, B->11, etc.\n",
        "    # Special characters '*' -> 36, '@' -> 37, '#' -> 38.\n",
        "    def get_value(char: str) -> int:\n",
        "        if '0' <= char <= '9':\n",
        "            return int(char)\n",
        "        if 'A' <= char <= 'Z':\n",
        "            return ord(char) - ord('A') + 10\n",
        "        if char == '*':\n",
        "            return 36\n",
        "        if char == '@':\n",
        "            return 37\n",
        "        if char == '#':\n",
        "            return 38\n",
        "        return -1 # Invalid character\n",
        "\n",
        "    total_sum = 0\n",
        "    # Iterate through the first 8 characters of the CUSIP.\n",
        "    for i, char in enumerate(cusip[:8]):\n",
        "        # Get the numeric value of the character.\n",
        "        val = get_value(char)\n",
        "        if val == -1:\n",
        "            return False # Invalid character found.\n",
        "\n",
        "        # Multiply the value by 2 for even positions (1-indexed, so 0-indexed i=1,3,5,7).\n",
        "        if (i + 1) % 2 == 0:\n",
        "            val *= 2\n",
        "\n",
        "        # Sum the digits of the resulting value (e.g., 14 becomes 1 + 4 = 5).\n",
        "        # This is equivalent to val // 10 + val % 10.\n",
        "        total_sum += (val // 10) + (val % 10)\n",
        "\n",
        "    # Calculate the expected check digit.\n",
        "    # The formula is (10 - (sum mod 10)) mod 10.\n",
        "    expected_check_digit = (10 - (total_sum % 10)) % 10\n",
        "\n",
        "    # Get the actual check digit from the CUSIP.\n",
        "    actual_check_digit = get_value(cusip[8])\n",
        "\n",
        "    # Compare the calculated check digit with the actual one.\n",
        "    return expected_check_digit == actual_check_digit\n",
        "\n",
        "\n",
        "def _validate_holdings_quality(\n",
        "    etf_holdings_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates data quality and business rules in the ETF holdings DataFrame.\n",
        "\n",
        "    This remediated function performs deep, content-level validation, now\n",
        "    including the critical CUSIP check digit validation. It checks:\n",
        "    1.  Identifier formats (length and check digit integrity).\n",
        "    2.  Numerical constraints (e.g., weights are in [0, 1]).\n",
        "    3.  Portfolio-level aggregations (e.g., weights for each ETF sum to 1.0).\n",
        "    4.  Temporal consistency (all holdings share the same valuation date).\n",
        "\n",
        "    Args:\n",
        "        etf_holdings_df (pd.DataFrame): The DataFrame containing ETF holdings data.\n",
        "        config (Dict[str, Any]): The validated research configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of detailed error messages for data quality violations.\n",
        "    \"\"\"\n",
        "    # Initialize a list to store validation error messages.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # --- Identifier Length and Null Checks ---\n",
        "    id_lengths = {'cusip': 9, 'isin': 12, 'sedol': 7}\n",
        "    for col, length in id_lengths.items():\n",
        "        if etf_holdings_df[col].isnull().any():\n",
        "            errors.append(f\"Column '{col}' contains missing (null) values.\")\n",
        "        incorrect_length_mask = (etf_holdings_df[col].str.len() != length) & (etf_holdings_df[col].notna())\n",
        "        if incorrect_length_mask.any():\n",
        "            count = incorrect_length_mask.sum()\n",
        "            errors.append(f\"Found {count} records in column '{col}' with incorrect length (expected {length}).\")\n",
        "\n",
        "    # --- CUSIP Check Digit Validation ---\n",
        "    # Apply the check digit validation function to all non-null CUSIPs.\n",
        "    valid_cusip_mask = etf_holdings_df['cusip'].notna()\n",
        "    # The .apply() method iterates through the Series, applying the function to each element.\n",
        "    is_valid_series = etf_holdings_df.loc[valid_cusip_mask, 'cusip'].apply(_is_valid_cusip)\n",
        "    # Identify the CUSIPs that failed the validation.\n",
        "    invalid_cusips = etf_holdings_df.loc[valid_cusip_mask, 'cusip'][~is_valid_series]\n",
        "    if not invalid_cusips.empty:\n",
        "        count = len(invalid_cusips)\n",
        "        errors.append(\n",
        "            f\"Found {count} CUSIPs with invalid check digits. \"\n",
        "            f\"Examples: {invalid_cusips.unique().tolist()[:5]}\"\n",
        "        )\n",
        "\n",
        "    # --- Numerical Column Constraints ---\n",
        "    if not etf_holdings_df['weight'].between(0.0, 1.0).all():\n",
        "        errors.append(\"Column 'weight' contains values outside the [0.0, 1.0] range.\")\n",
        "    if (etf_holdings_df['market_value_usd'] < 0).any():\n",
        "        errors.append(\"Column 'market_value_usd' contains negative values.\")\n",
        "    if (etf_holdings_df['shares_held'] <= 0).any():\n",
        "        errors.append(\"Column 'shares_held' contains non-positive values.\")\n",
        "\n",
        "    # --- Group-level Validation: Per-ETF Weight Summation ---\n",
        "    weight_sums = etf_holdings_df.groupby('etf_id')['weight'].sum()\n",
        "    tolerance = 1e-6\n",
        "    failing_etfs = weight_sums[~np.isclose(weight_sums, 1.0, atol=tolerance)]\n",
        "    if not failing_etfs.empty:\n",
        "        for etf_id, total_weight in failing_etfs.items():\n",
        "            errors.append(f\"ETF '{etf_id}' weights do not sum to 1.0 (sum: {total_weight:.8f}).\")\n",
        "\n",
        "    # --- Temporal Consistency Check ---\n",
        "    if etf_holdings_df['as_of_date'].nunique() != 1:\n",
        "        errors.append(\"Column 'as_of_date' contains multiple different dates.\")\n",
        "    else:\n",
        "        actual_date = etf_holdings_df['as_of_date'].iloc[0]\n",
        "        expected_date = pd.to_datetime(config['data_setup']['valuation_date'])\n",
        "        if actual_date != expected_date:\n",
        "            errors.append(f\"'as_of_date' ({actual_date.date()}) does not match the configured 'valuation_date' ({expected_date.date()}).\")\n",
        "\n",
        "    # Return the list of all accumulated data quality errors.\n",
        "    return errors\n",
        "\n",
        "\n",
        "def validate_etf_holdings_dataframe(\n",
        "    etf_holdings_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete validation of the ETF holdings DataFrame.\n",
        "\n",
        "    This master function provides a single entry point to rigorously validate\n",
        "    the ETF holdings data against the study's specifications. It executes a\n",
        "    sequence of validation steps, ensuring that the data is structurally sound,\n",
        "    has the correct data types, and adheres to all financial and logical\n",
        "    business rules before being used in the research pipeline.\n",
        "\n",
        "    The validation proceeds in three phases:\n",
        "    1.  **Structure Validation**: Checks column names, DataFrame shape, and ETF count.\n",
        "    2.  **Data Type Validation**: Verifies that each column has the expected dtype.\n",
        "    3.  **Data Quality Validation**: Performs deep content checks on identifiers,\n",
        "        numerical ranges, and portfolio-level constraints.\n",
        "\n",
        "    Args:\n",
        "        etf_holdings_df (pd.DataFrame): The DataFrame containing ETF portfolio\n",
        "                                        holdings data to be validated.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary,\n",
        "                                 which provides ground-truth parameters for\n",
        "                                 validation (e.g., number of ETFs).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[bool, List[str]]: A tuple containing:\n",
        "        - bool: True if the DataFrame is valid, False otherwise.\n",
        "        - List[str]: A comprehensive list of all validation errors found. An\n",
        "                     empty list indicates that the DataFrame passed all checks.\n",
        "    \"\"\"\n",
        "    # Perform initial input type check.\n",
        "    if not isinstance(etf_holdings_df, pd.DataFrame):\n",
        "        return False, [\"Input 'etf_holdings_df' must be a pandas DataFrame.\"]\n",
        "    if not isinstance(config, dict):\n",
        "        return False, [\"Input 'config' must be a dictionary.\"]\n",
        "\n",
        "    # --- Step 1: Validate the DataFrame's structure ---\n",
        "    # This is a prerequisite for subsequent checks. If the structure is wrong,\n",
        "    # other checks might fail with unexpected errors.\n",
        "    structure_errors = _validate_holdings_structure(etf_holdings_df, config)\n",
        "    if structure_errors:\n",
        "        # If critical structural errors are found, return immediately.\n",
        "        return False, structure_errors\n",
        "\n",
        "    # --- Step 2: Validate the data types of all columns ---\n",
        "    # This ensures that numerical operations and date logic will execute correctly.\n",
        "    dtype_errors = _validate_holdings_dtypes(etf_holdings_df)\n",
        "\n",
        "    # --- Step 3: Perform deep data quality and business rule checks ---\n",
        "    # This is the most intensive step, verifying the content of the data.\n",
        "    quality_errors = _validate_holdings_quality(etf_holdings_df, config)\n",
        "\n",
        "    # --- Aggregation of all errors ---\n",
        "    # Combine errors from all validation steps into a single report.\n",
        "    all_errors = dtype_errors + quality_errors\n",
        "\n",
        "    # --- Final Verdict ---\n",
        "    # The DataFrame is considered valid if and only if no errors were found.\n",
        "    is_valid = not all_errors\n",
        "\n",
        "    # Return the final validation status and the comprehensive list of errors.\n",
        "    return is_valid, all_errors\n"
      ],
      "metadata": {
        "id": "lK6aUNtgsgbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Bond Features DataFrame Validation\n",
        "\n",
        "def _validate_features_structure(\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the structural integrity of the bond features DataFrame.\n",
        "\n",
        "    This function ensures the DataFrame has the correct fundamental structure:\n",
        "    1.  It is not empty.\n",
        "    2.  It contains the exact set of required feature columns.\n",
        "    3.  The number of unique securities (bonds) matches the configuration.\n",
        "\n",
        "    Args:\n",
        "        bond_features_df (pd.DataFrame): The DataFrame with bond feature data.\n",
        "        config (Dict[str, Any]): The validated research configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages for structural violations.\n",
        "    \"\"\"\n",
        "    # Initialize a list to hold validation errors.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Check 1: Ensure the DataFrame is not empty.\n",
        "    if bond_features_df.empty:\n",
        "        errors.append(\"Bond features DataFrame is empty.\")\n",
        "        return errors\n",
        "\n",
        "    # Check 2: Verify the exact set of columns.\n",
        "    # This ensures all features needed for the model are present and no extra\n",
        "    # columns exist that could interfere with processing.\n",
        "    expected_columns: Set[str] = {\n",
        "        'cusip', 'issuer', 'industry', 'days_to_maturity', 'age_days',\n",
        "        'bond_rating_composite', 'country_of_risk', 'coupon_rate',\n",
        "        'coupon_frequency', 'amount_issued_usd', 'rule_144a_flag',\n",
        "        'oas_bp', 'yield_to_maturity'\n",
        "    }\n",
        "    actual_columns: Set[str] = set(bond_features_df.columns)\n",
        "\n",
        "    if actual_columns != expected_columns:\n",
        "        missing_cols = expected_columns - actual_columns\n",
        "        extra_cols = actual_columns - expected_columns\n",
        "        error_msg = \"Bond features DataFrame column mismatch.\"\n",
        "        if missing_cols:\n",
        "            error_msg += f\" Missing columns: {sorted(list(missing_cols))}.\"\n",
        "        if extra_cols:\n",
        "            error_msg += f\" Extra columns: {sorted(list(extra_cols))}.\"\n",
        "        errors.append(error_msg)\n",
        "\n",
        "    # Check 3: Verify the number of unique bonds.\n",
        "    # This confirms the dataset scope aligns with the study's specification.\n",
        "    expected_bond_count = config['data_setup']['total_unique_bonds']\n",
        "    actual_bond_count = bond_features_df['cusip'].nunique()\n",
        "\n",
        "    if actual_bond_count != expected_bond_count:\n",
        "        errors.append(\n",
        "            f\"Expected {expected_bond_count} unique bonds based on config, but \"\n",
        "            f\"found {actual_bond_count} unique CUSIPs.\"\n",
        "        )\n",
        "\n",
        "    # Check 4: Verify CUSIP column is unique (primary key constraint).\n",
        "    if bond_features_df['cusip'].duplicated().any():\n",
        "        duplicate_count = bond_features_df['cusip'].duplicated().sum()\n",
        "        errors.append(f\"Primary key 'cusip' is not unique. Found {duplicate_count} duplicates.\")\n",
        "\n",
        "    return errors\n",
        "\n",
        "\n",
        "def _validate_features_dtypes(\n",
        "    bond_features_df: pd.DataFrame\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the data types of columns in the bond features DataFrame.\n",
        "\n",
        "    This function performs rigorous data type validation, including special\n",
        "    handling for ordered categorical data, which is essential for models that\n",
        "    can leverage ordinal information (like Random Forests).\n",
        "\n",
        "    Args:\n",
        "        bond_features_df (pd.DataFrame): The DataFrame with bond feature data.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages for data type violations.\n",
        "    \"\"\"\n",
        "    # Initialize a list to hold validation errors.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Define expected dtypes for all columns.\n",
        "    expected_dtypes: Dict[str, str] = {\n",
        "        'cusip': 'object',\n",
        "        'issuer': 'object',\n",
        "        'industry': 'category',\n",
        "        'days_to_maturity': 'int32',\n",
        "        'age_days': 'int32',\n",
        "        'bond_rating_composite': 'category',\n",
        "        'country_of_risk': 'category',\n",
        "        'coupon_rate': 'float64',\n",
        "        'coupon_frequency': 'int8',\n",
        "        'amount_issued_usd': 'float64',\n",
        "        'rule_144a_flag': 'bool',\n",
        "        'oas_bp': 'float64',\n",
        "        'yield_to_maturity': 'float64'\n",
        "    }\n",
        "\n",
        "    # Define the required order for the bond rating categories.\n",
        "    # This is critical for ensuring the model correctly interprets credit risk.\n",
        "    rating_order: List[str] = [\n",
        "        'AAA', 'AA+', 'AA', 'AA-', 'A+', 'A', 'A-', 'BBB+', 'BBB', 'BBB-',\n",
        "        'BB+', 'BB', 'BB-', 'B+', 'B', 'B-', 'CCC+', 'CCC', 'CCC-', 'CC', 'C', 'D'\n",
        "    ]\n",
        "\n",
        "    # Iterate through columns and their expected dtypes.\n",
        "    for col, expected_dtype in expected_dtypes.items():\n",
        "        if col in bond_features_df.columns:\n",
        "            actual_dtype = bond_features_df[col].dtype\n",
        "\n",
        "            # Special validation for the ordered categorical 'bond_rating_composite'.\n",
        "            if col == 'bond_rating_composite':\n",
        "                if not pd.api.types.is_categorical_dtype(actual_dtype):\n",
        "                    errors.append(f\"Column '{col}' is not a categorical dtype.\")\n",
        "                # Check if the categories are correctly ordered.\n",
        "                elif not actual_dtype.ordered:\n",
        "                    errors.append(f\"Categorical column '{col}' is not ordered.\")\n",
        "                # Check if the set of categories matches the expected order exactly.\n",
        "                elif list(actual_dtype.categories) != rating_order:\n",
        "                    errors.append(\n",
        "                        f\"Column '{col}' has incorrect category order or items.\"\n",
        "                    )\n",
        "            # General dtype check for all other columns.\n",
        "            elif str(actual_dtype) != expected_dtype:\n",
        "                errors.append(\n",
        "                    f\"Column '{col}' has incorrect dtype. Expected \"\n",
        "                    f\"'{expected_dtype}', but found '{actual_dtype}'.\"\n",
        "                )\n",
        "\n",
        "    return errors\n",
        "\n",
        "\n",
        "def _validate_features_quality(\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates data quality and business rules for the bond features DataFrame.\n",
        "\n",
        "    This function executes a suite of content-level checks to ensure the data\n",
        "    is financially and logically sound. It validates numerical ranges, set\n",
        "    membership for categorical codes, and the absence of missing values as\n",
        "    per the study's strict requirements.\n",
        "\n",
        "    Args:\n",
        "        bond_features_df (pd.DataFrame): The DataFrame with bond feature data.\n",
        "        config (Dict[str, Any]): The validated research configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages for data quality violations.\n",
        "    \"\"\"\n",
        "    # Initialize a list to hold validation errors.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # --- Numerical Range Validations ---\n",
        "    # Each check uses a vectorized boolean mask for efficiency.\n",
        "    range_checks = {\n",
        "        'days_to_maturity': (1, 10950),\n",
        "        'age_days': (1, 7300),\n",
        "        'coupon_rate': (0.0, 8.5),\n",
        "        'amount_issued_usd': (100_000_000, 50_000_000_000),\n",
        "        'oas_bp': (-10, 3000),\n",
        "        'yield_to_maturity': (0.125, 12.875)\n",
        "    }\n",
        "    for col, (min_val, max_val) in range_checks.items():\n",
        "        if not bond_features_df[col].between(min_val, max_val).all():\n",
        "            # Find the number of violating entries for a more informative error.\n",
        "            violations = bond_features_df[~bond_features_df[col].between(min_val, max_val)]\n",
        "            errors.append(\n",
        "                f\"Column '{col}' has {len(violations)} values outside the expected \"\n",
        "                f\"range [{min_val}, {max_val}]. Example violation: \"\n",
        "                f\"{violations[col].iloc[0]}\"\n",
        "            )\n",
        "\n",
        "    # --- Set Membership Validation ---\n",
        "    # Check 'coupon_frequency' against a set of valid integer codes.\n",
        "    valid_frequencies = {1, 2, 4, 12}\n",
        "    if not bond_features_df['coupon_frequency'].isin(valid_frequencies).all():\n",
        "        violations = bond_features_df[~bond_features_df['coupon_frequency'].isin(valid_frequencies)]\n",
        "        errors.append(\n",
        "            f\"Column 'coupon_frequency' contains invalid values. \"\n",
        "            f\"Expected one of {valid_frequencies}. Found {len(violations)} violations. \"\n",
        "            f\"Example violation: {violations['coupon_frequency'].iloc[0]}\"\n",
        "        )\n",
        "\n",
        "    # --- Missing Value Validation ---\n",
        "    # This check is conditional on the configuration flag.\n",
        "    if config['feature_engineering']['validate_no_missing_values']:\n",
        "        # Calculate the total number of missing values in the entire DataFrame.\n",
        "        missing_count = bond_features_df.isnull().sum().sum()\n",
        "        if missing_count > 0:\n",
        "            # If missing values exist, report the count and columns affected.\n",
        "            missing_per_col = bond_features_df.isnull().sum()\n",
        "            affected_cols = missing_per_col[missing_per_col > 0].to_dict()\n",
        "            errors.append(\n",
        "                f\"Found {missing_count} total missing values, but config \"\n",
        "                f\"requires none. Affected columns: {affected_cols}.\"\n",
        "            )\n",
        "\n",
        "    return errors\n",
        "\n",
        "\n",
        "def validate_bond_features_dataframe(\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete validation of the bond features DataFrame.\n",
        "\n",
        "    This master function provides a single, reliable entry point to validate\n",
        "    the bond security master data. It ensures the data used to train the\n",
        "    constituent similarity model is of the highest quality and conforms\n",
        "    precisely to the research methodology.\n",
        "\n",
        "    The validation proceeds in three phases:\n",
        "    1.  **Structure Validation**: Checks column names, DataFrame shape, and the\n",
        "        uniqueness and count of the primary key (CUSIP).\n",
        "    2.  **Data Type Validation**: Verifies dtypes, with special attention to\n",
        "        the correct implementation of ordered categorical data for credit ratings.\n",
        "    3.  **Data Quality Validation**: Enforces strict numerical ranges, set\n",
        "        membership rules, and the critical no-missing-values constraint.\n",
        "\n",
        "    Args:\n",
        "        bond_features_df (pd.DataFrame): The DataFrame containing bond feature\n",
        "                                          data to be validated.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[bool, List[str]]: A tuple containing:\n",
        "        - bool: True if the DataFrame is valid, False otherwise.\n",
        "        - List[str]: A comprehensive list of all validation errors found.\n",
        "    \"\"\"\n",
        "    # Perform initial input type checks.\n",
        "    if not isinstance(bond_features_df, pd.DataFrame):\n",
        "        return False, [\"Input 'bond_features_df' must be a pandas DataFrame.\"]\n",
        "    if not isinstance(config, dict):\n",
        "        return False, [\"Input 'config' must be a dictionary.\"]\n",
        "\n",
        "    # --- Step 1: Validate the DataFrame's structure ---\n",
        "    structure_errors = _validate_features_structure(bond_features_df, config)\n",
        "    if structure_errors:\n",
        "        # Halt validation if the basic structure is incorrect.\n",
        "        return False, structure_errors\n",
        "\n",
        "    # --- Step 2: Validate the data types of all columns ---\n",
        "    dtype_errors = _validate_features_dtypes(bond_features_df)\n",
        "\n",
        "    # --- Step 3: Perform deep data quality and business rule checks ---\n",
        "    quality_errors = _validate_features_quality(bond_features_df, config)\n",
        "\n",
        "    # --- Aggregation of all errors ---\n",
        "    all_errors = dtype_errors + quality_errors\n",
        "\n",
        "    # --- Final Verdict ---\n",
        "    is_valid = not all_errors\n",
        "\n",
        "    return is_valid, all_errors\n"
      ],
      "metadata": {
        "id": "JuyX90y4tVtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Monthly Returns DataFrame Validation\n",
        "\n",
        "def _validate_returns_structure(\n",
        "    monthly_returns_df: pd.DataFrame,\n",
        "    etf_holdings_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the structure and dimensions of the monthly returns DataFrame.\n",
        "\n",
        "    This function ensures the returns DataFrame has the correct shape, columns,\n",
        "    and temporal length, and that its ETF identifiers are perfectly consistent\n",
        "    with the holdings data. It verifies:\n",
        "    1.  The DataFrame is not empty and contains a 'date' column.\n",
        "    2.  The number of ETF columns matches the configuration.\n",
        "    3.  The set of ETF columns exactly matches the ETF identifiers in holdings.\n",
        "    4.  The number of rows (observations) matches the expected time series length.\n",
        "\n",
        "    Args:\n",
        "        monthly_returns_df (pd.DataFrame): DataFrame of monthly ETF returns.\n",
        "        etf_holdings_df (pd.DataFrame): Validated DataFrame of ETF holdings.\n",
        "        config (Dict[str, Any]): The validated research configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages for structural violations.\n",
        "    \"\"\"\n",
        "    # Initialize a list to hold validation errors.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Check 1: Verify the DataFrame is not empty and has a 'date' column.\n",
        "    if monthly_returns_df.empty:\n",
        "        errors.append(\"Monthly returns DataFrame is empty.\")\n",
        "        return errors\n",
        "    if 'date' not in monthly_returns_df.columns:\n",
        "        errors.append(\"Monthly returns DataFrame is missing the 'date' column.\")\n",
        "        return errors\n",
        "\n",
        "    # Isolate the ETF return columns (all columns except 'date').\n",
        "    etf_return_columns = sorted([c for c in monthly_returns_df.columns if c != 'date'])\n",
        "\n",
        "    # Check 2: Verify the number of ETF columns.\n",
        "    expected_etf_count = config['data_setup']['total_corporate_bond_etfs']\n",
        "    if len(etf_return_columns) != expected_etf_count:\n",
        "        errors.append(\n",
        "            f\"Expected {expected_etf_count} ETF return columns, but found \"\n",
        "            f\"{len(etf_return_columns)}.\"\n",
        "        )\n",
        "        # Return early if column count is wrong, as consistency check will fail.\n",
        "        return errors\n",
        "\n",
        "    # Check 3: Verify consistency of ETF identifiers with holdings data.\n",
        "    etf_ids_in_holdings = sorted(list(etf_holdings_df['etf_id'].unique()))\n",
        "    if etf_return_columns != etf_ids_in_holdings:\n",
        "        # Use set operations for a detailed comparison.\n",
        "        set_returns = set(etf_return_columns)\n",
        "        set_holdings = set(etf_ids_in_holdings)\n",
        "        missing_in_returns = set_holdings - set_returns\n",
        "        extra_in_returns = set_returns - set_holdings\n",
        "        error_msg = \"ETF identifiers in returns do not match holdings.\"\n",
        "        if missing_in_returns:\n",
        "            error_msg += f\" Missing from returns: {sorted(list(missing_in_returns))}.\"\n",
        "        if extra_in_returns:\n",
        "            error_msg += f\" Extra in returns: {sorted(list(extra_in_returns))}.\"\n",
        "        errors.append(error_msg)\n",
        "\n",
        "    # Check 4: Verify the number of monthly observations.\n",
        "    # Generate the expected date range to determine the number of observations.\n",
        "    expected_dates = pd.date_range(\n",
        "        start=config['data_setup']['return_series_start_date'],\n",
        "        end=config['data_setup']['return_series_end_date'],\n",
        "        freq='M'\n",
        "    )\n",
        "    expected_obs_count = len(expected_dates)\n",
        "    actual_obs_count = len(monthly_returns_df)\n",
        "    if actual_obs_count != expected_obs_count:\n",
        "        errors.append(\n",
        "            f\"Expected {expected_obs_count} monthly observations, but found \"\n",
        "            f\"{actual_obs_count} rows.\"\n",
        "        )\n",
        "\n",
        "    return errors\n",
        "\n",
        "\n",
        "def _validate_returns_temporal_integrity(\n",
        "    monthly_returns_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the temporal integrity of the monthly returns DataFrame.\n",
        "\n",
        "    This function focuses on the 'date' column, ensuring it forms a valid,\n",
        "    clean, and consistent time axis for the analysis. It verifies:\n",
        "    1.  The 'date' column is a proper datetime type.\n",
        "    2.  The dates are unique and sorted chronologically.\n",
        "    3.  Each date is the last business day of its month.\n",
        "    4.  The start and end dates match the configuration exactly.\n",
        "\n",
        "    Args:\n",
        "        monthly_returns_df (pd.DataFrame): DataFrame of monthly ETF returns.\n",
        "        config (Dict[str, Any]): The validated research configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages for temporal violations.\n",
        "    \"\"\"\n",
        "    # Initialize a list to hold validation errors.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Make a copy to avoid modifying the original DataFrame.\n",
        "    df = monthly_returns_df.copy()\n",
        "\n",
        "    # Check 1: Ensure 'date' column can be parsed as datetime.\n",
        "    try:\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "    except Exception as e:\n",
        "        errors.append(f\"Could not parse 'date' column as datetime: {e}\")\n",
        "        return errors # Halt if dates are unparsable.\n",
        "\n",
        "    # Check 2: Verify dates are unique and sorted.\n",
        "    if not df['date'].is_unique:\n",
        "        errors.append(\"Dates in 'date' column are not unique.\")\n",
        "    if not df['date'].is_monotonic_increasing:\n",
        "        errors.append(\"Dates in 'date' column are not sorted chronologically.\")\n",
        "\n",
        "    # Check 3: Verify each date is a month-end business day.\n",
        "    # This is a strict check for financial time-series data.\n",
        "    month_end_checker = pd.tseries.offsets.MonthEnd(0)\n",
        "    non_monthend_dates = [\n",
        "        d.date() for d in df['date'] if not month_end_checker.is_on_offset(d)\n",
        "    ]\n",
        "    if non_monthend_dates:\n",
        "        errors.append(\n",
        "            f\"Found {len(non_monthend_dates)} dates that are not the last \"\n",
        "            f\"business day of the month. Examples: {non_monthend_dates[:3]}\"\n",
        "        )\n",
        "\n",
        "    # Check 4: Verify start and end dates match the configuration.\n",
        "    expected_start = pd.to_datetime(config['data_setup']['return_series_start_date'])\n",
        "    expected_end = pd.to_datetime(config['data_setup']['return_series_end_date'])\n",
        "    if df['date'].iloc[0] != expected_start:\n",
        "        errors.append(\n",
        "            f\"First date '{df['date'].iloc[0].date()}' does not match expected \"\n",
        "            f\"start date '{expected_start.date()}'.\"\n",
        "        )\n",
        "    if df['date'].iloc[-1] != expected_end:\n",
        "        errors.append(\n",
        "            f\"Last date '{df['date'].iloc[-1].date()}' does not match expected \"\n",
        "            f\"end date '{expected_end.date()}'.\"\n",
        "        )\n",
        "\n",
        "    return errors\n",
        "\n",
        "\n",
        "def _validate_returns_quality(\n",
        "    monthly_returns_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the quality of the numerical return values.\n",
        "\n",
        "    This remediated function inspects the return data itself, ensuring it is\n",
        "    clean, complete, and correctly formatted with the required precision. It verifies:\n",
        "    1.  There are absolutely no missing (NaN) values.\n",
        "    2.  The data types of all return columns are float64.\n",
        "    3.  The return values are in decimal format (e.g., 0.01), not percentage.\n",
        "    4.  **[NEW]** The return values adhere to the specified 4-decimal-place precision.\n",
        "\n",
        "    Args:\n",
        "        monthly_returns_df (pd.DataFrame): DataFrame of monthly ETF returns.\n",
        "        config (Dict[str, Any]): The research configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages for data quality violations.\n",
        "    \"\"\"\n",
        "    # Initialize a list to hold validation errors.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Isolate the ETF return columns for quality checks.\n",
        "    etf_return_columns = [c for c in monthly_returns_df.columns if c != 'date']\n",
        "    returns_data = monthly_returns_df[etf_return_columns]\n",
        "\n",
        "    # Check 1: Verify no missing values exist anywhere in the return data.\n",
        "    # This is a strict requirement of the paper's methodology.\n",
        "    if returns_data.isnull().values.any():\n",
        "        missing_counts = returns_data.isnull().sum()\n",
        "        affected_cols = missing_counts[missing_counts > 0].to_dict()\n",
        "        errors.append(f\"Missing values found in return data. Columns: {affected_cols}.\")\n",
        "\n",
        "    # Check 2: Verify data types of all return columns are float.\n",
        "    for col in etf_return_columns:\n",
        "        if not pd.api.types.is_float_dtype(returns_data[col]):\n",
        "            errors.append(\n",
        "                f\"Return column '{col}' is not a float dtype \"\n",
        "                f\"(found {returns_data[col].dtype}).\"\n",
        "            )\n",
        "\n",
        "    # If there are non-float columns, subsequent checks may fail. Return early.\n",
        "    if errors:\n",
        "        return errors\n",
        "\n",
        "    # Check 3: Validate that returns are in decimal format.\n",
        "    # A robust heuristic is to check if the absolute maximum return is within a\n",
        "    # plausible range for decimal returns (e.g., < 50% or 0.5).\n",
        "    max_abs_return = returns_data.abs().max().max()\n",
        "    if max_abs_return > 0.5:\n",
        "        errors.append(\n",
        "            f\"Maximum absolute return is {max_abs_return:.2%}, which suggests \"\n",
        "            \"data may be in percentage format instead of decimal.\"\n",
        "        )\n",
        "\n",
        "    # --- Check 4: Validate 4-decimal-place precision ---\n",
        "    # This check ensures the data conforms to the specified numerical precision.\n",
        "    # Get the required precision for correlation calculations from the config.\n",
        "    precision = config['computational_config']['correlation_precision']\n",
        "    if precision != 4:\n",
        "        errors.append(f\"Configuration expects correlation_precision of 4, but is {precision}. Precision check may be invalid.\")\n",
        "\n",
        "    # Compare the original data to the data rounded to the specified precision.\n",
        "    # Using numpy.allclose is essential for robust floating-point comparison,\n",
        "    # avoiding issues with representation errors. A very small tolerance\n",
        "    # effectively checks for equality for numbers with limited precision.\n",
        "    returns_rounded = returns_data.round(precision)\n",
        "    if not np.allclose(returns_data, returns_rounded, atol=1e-9, equal_nan=True):\n",
        "        # Find the first cell that violates the precision for an actionable error message.\n",
        "        diff_mask = ~np.isclose(returns_data, returns_rounded, atol=1e-9, equal_nan=True)\n",
        "        first_violation_col = diff_mask.any(axis=0).idxmax()\n",
        "        first_violation_row = diff_mask[first_violation_col].idxmax()\n",
        "        original_val = returns_data.loc[first_violation_row, first_violation_col]\n",
        "\n",
        "        errors.append(\n",
        "            f\"Return values do not adhere to the required {precision}-decimal-place \"\n",
        "            f\"precision. Example violation in column '{first_violation_col}' \"\n",
        "            f\"at index '{first_violation_row}' with value {original_val}.\"\n",
        "        )\n",
        "\n",
        "    return errors\n",
        "\n",
        "\n",
        "def validate_monthly_returns_dataframe(\n",
        "    monthly_returns_df: pd.DataFrame,\n",
        "    etf_holdings_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete validation of the monthly returns DataFrame.\n",
        "\n",
        "    This master function provides a single entry point to rigorously validate\n",
        "    the time-series return data, which serves as the ground truth for the\n",
        "    study's final evaluation. A failure in this data's integrity would\n",
        "    invalidate all results.\n",
        "\n",
        "    The validation proceeds in three phases:\n",
        "    1.  **Structure Validation**: Checks column names, ETF consistency with\n",
        "        holdings, and the exact number of time-series observations.\n",
        "    2.  **Temporal Integrity Validation**: Ensures the date index is clean,\n",
        "        chronological, and adheres to the month-end business day convention.\n",
        "    3.  **Data Quality Validation**: Verifies the absence of missing values and\n",
        "        the correct decimal format of the return figures.\n",
        "\n",
        "    Args:\n",
        "        monthly_returns_df (pd.DataFrame): The DataFrame containing monthly\n",
        "                                           return data to be validated.\n",
        "        etf_holdings_df (pd.DataFrame): The validated DataFrame of ETF holdings,\n",
        "                                        used for cross-referencing ETF identifiers.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[bool, List[str]]: A tuple containing:\n",
        "        - bool: True if the DataFrame is valid, False otherwise.\n",
        "        - List[str]: A comprehensive list of all validation errors found.\n",
        "    \"\"\"\n",
        "    # Perform initial input type checks.\n",
        "    if not isinstance(monthly_returns_df, pd.DataFrame):\n",
        "        return False, [\"Input 'monthly_returns_df' must be a pandas DataFrame.\"]\n",
        "    if not isinstance(etf_holdings_df, pd.DataFrame):\n",
        "        return False, [\"Input 'etf_holdings_df' must be a pandas DataFrame.\"]\n",
        "    if not isinstance(config, dict):\n",
        "        return False, [\"Input 'config' must be a dictionary.\"]\n",
        "\n",
        "    # --- Step 1: Validate the DataFrame's structure and consistency ---\n",
        "    structure_errors = _validate_returns_structure(\n",
        "        monthly_returns_df, etf_holdings_df, config\n",
        "    )\n",
        "    if structure_errors:\n",
        "        # Halt if the basic structure is incorrect.\n",
        "        return False, structure_errors\n",
        "\n",
        "    # --- Step 2: Validate the temporal integrity of the date index ---\n",
        "    temporal_errors = _validate_returns_temporal_integrity(\n",
        "        monthly_returns_df, config\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Perform deep data quality checks on return values ---\n",
        "    quality_errors = _validate_returns_quality(monthly_returns_df)\n",
        "\n",
        "    # --- Aggregation of all errors ---\n",
        "    all_errors = temporal_errors + quality_errors\n",
        "\n",
        "    # --- Final Verdict ---\n",
        "    is_valid = not all_errors\n",
        "\n",
        "    return is_valid, all_errors\n"
      ],
      "metadata": {
        "id": "DSOpG61juIOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Cross-Dataset Consistency Validation\n",
        "\n",
        "def _validate_cusip_consistency(\n",
        "    etf_holdings_df: pd.DataFrame,\n",
        "    bond_features_df: pd.DataFrame\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the consistency of the CUSIP universe across datasets.\n",
        "\n",
        "    This function ensures that the universe of securities is coherent between the\n",
        "    portfolio holdings and the security master feature data. It enforces the\n",
        "    critical rule that every bond listed in a portfolio must have a corresponding\n",
        "    entry in the bond features DataFrame.\n",
        "\n",
        "    Args:\n",
        "        etf_holdings_df (pd.DataFrame): The validated DataFrame of ETF holdings.\n",
        "        bond_features_df (pd.DataFrame): The validated DataFrame of bond features.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages related to CUSIP universe mismatches.\n",
        "    \"\"\"\n",
        "    # Initialize a list to hold validation errors.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Extract unique CUSIPs from both DataFrames into sets for efficient comparison.\n",
        "    # Sets provide highly optimized operations for membership and difference checks.\n",
        "    cusips_in_holdings: Set[str] = set(etf_holdings_df['cusip'].unique())\n",
        "    cusips_in_features: Set[str] = set(bond_features_df['cusip'].unique())\n",
        "\n",
        "    # Check 1: Every CUSIP in holdings must exist in the features master file.\n",
        "    # This is a non-negotiable referential integrity constraint.\n",
        "    # We check if the holdings set is a subset of the features set.\n",
        "    if not cusips_in_holdings.issubset(cusips_in_features):\n",
        "        # If not, identify the specific CUSIPs that are missing from the features data.\n",
        "        missing_cusips = cusips_in_holdings - cusips_in_features\n",
        "        # Format a detailed and actionable error message.\n",
        "        errors.append(\n",
        "            f\"Found {len(missing_cusips)} CUSIPs in the holdings data that are \"\n",
        "            f\"missing from the bond features data. Examples: \"\n",
        "            f\"{sorted(list(missing_cusips))[:5]}\"\n",
        "        )\n",
        "\n",
        "    return errors\n",
        "\n",
        "\n",
        "def _validate_temporal_alignment(\n",
        "    etf_holdings_df: pd.DataFrame,\n",
        "    monthly_returns_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the temporal (date) alignment across all datasets.\n",
        "\n",
        "    This function ensures that the portfolio snapshot (holdings) is perfectly\n",
        "    aligned in time with the final observation of the performance data (returns),\n",
        "    a cornerstone of point-in-time financial analysis.\n",
        "\n",
        "    Args:\n",
        "        etf_holdings_df (pd.DataFrame): The validated DataFrame of ETF holdings.\n",
        "        monthly_returns_df (pd.DataFrame): The validated DataFrame of returns.\n",
        "        config (Dict[str, Any]): The validated research configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages for temporal misalignments.\n",
        "    \"\"\"\n",
        "    # Initialize a list to hold validation errors.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Check 1: Verify the holdings data corresponds to a single valuation date.\n",
        "    # The unique 'as_of_date' from holdings is the anchor point for our analysis.\n",
        "    holdings_date = etf_holdings_df['as_of_date'].unique()\n",
        "    if len(holdings_date) != 1:\n",
        "        # This was already checked in Task 2, but is re-verified here for completeness.\n",
        "        errors.append(\"Holdings data must contain only a single 'as_of_date'.\")\n",
        "        return errors # Halt if this fundamental assumption is violated.\n",
        "\n",
        "    # Extract the single holdings date and the expected valuation date from config.\n",
        "    holdings_date = pd.to_datetime(holdings_date[0])\n",
        "    expected_valuation_date = pd.to_datetime(config['data_setup']['valuation_date'])\n",
        "\n",
        "    # Check 2: Ensure the holdings date matches the configuration.\n",
        "    if holdings_date != expected_valuation_date:\n",
        "        errors.append(\n",
        "            f\"Holdings 'as_of_date' ({holdings_date.date()}) does not match \"\n",
        "            f\"configured 'valuation_date' ({expected_valuation_date.date()}).\"\n",
        "        )\n",
        "\n",
        "    # Check 3: Ensure the final date in the returns series aligns with the holdings date.\n",
        "    # The ground truth evaluation period must end at the same time as the portfolio snapshot.\n",
        "    last_return_date = pd.to_datetime(monthly_returns_df['date'].iloc[-1])\n",
        "    if last_return_date != holdings_date:\n",
        "        errors.append(\n",
        "            f\"The last date in the monthly returns series ({last_return_date.date()}) \"\n",
        "            f\"does not match the holdings 'as_of_date' ({holdings_date.date()}).\"\n",
        "        )\n",
        "\n",
        "    return errors\n",
        "\n",
        "\n",
        "def _validate_etf_id_consistency(\n",
        "    etf_holdings_df: pd.DataFrame,\n",
        "    monthly_returns_df: pd.DataFrame\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the consistency of the ETF identifier universe across datasets.\n",
        "\n",
        "    This function ensures a perfect one-to-one correspondence between the ETFs\n",
        "    defined in the holdings data and the ETFs for which return series are\n",
        "    provided. This prevents misalignments where an ETF might have holdings but\n",
        "    no returns, or vice-versa.\n",
        "\n",
        "    Args:\n",
        "        etf_holdings_df (pd.DataFrame): The validated DataFrame of ETF holdings.\n",
        "        monthly_returns_df (pd.DataFrame): The validated DataFrame of returns.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages for ETF universe inconsistencies.\n",
        "    \"\"\"\n",
        "    # Initialize a list to hold validation errors.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Extract the universe of ETF identifiers from the holdings data.\n",
        "    etf_ids_in_holdings: Set[str] = set(etf_holdings_df['etf_id'].unique())\n",
        "\n",
        "    # Extract the universe of ETF identifiers from the returns data columns.\n",
        "    etf_ids_in_returns: Set[str] = set(\n",
        "        [c for c in monthly_returns_df.columns if c != 'date']\n",
        "    )\n",
        "\n",
        "    # Check 1: The two sets of ETF identifiers must be identical.\n",
        "    # This is a strict equality check, ensuring no discrepancies.\n",
        "    if etf_ids_in_holdings != etf_ids_in_returns:\n",
        "        # If they are not identical, perform a detailed diagnosis.\n",
        "        missing_from_returns = etf_ids_in_holdings - etf_ids_in_returns\n",
        "        missing_from_holdings = etf_ids_in_returns - etf_ids_in_holdings\n",
        "        error_msg = \"ETF identifier universe is inconsistent between holdings and returns.\"\n",
        "        if missing_from_returns:\n",
        "            error_msg += (\n",
        "                f\" ETFs in holdings but not in returns: {sorted(list(missing_from_returns))}.\"\n",
        "            )\n",
        "        if missing_from_holdings:\n",
        "            error_msg += (\n",
        "                f\" ETFs in returns but not in holdings: {sorted(list(missing_from_holdings))}.\"\n",
        "            )\n",
        "        errors.append(error_msg)\n",
        "\n",
        "    return errors\n",
        "\n",
        "\n",
        "def validate_cross_dataset_consistency(\n",
        "    etf_holdings_df: pd.DataFrame,\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    monthly_returns_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of referential integrity across all datasets.\n",
        "\n",
        "    This master function serves as the final gate in the data validation phase.\n",
        "    It ensures that the three core datasets (holdings, features, returns) are\n",
        "    not only internally consistent but also perfectly aligned with each other.\n",
        "    This cross-validation is essential for the integrity of the entire\n",
        "    research pipeline.\n",
        "\n",
        "    The validation proceeds in three phases:\n",
        "    1.  **CUSIP Consistency**: Verifies that all securities in portfolios have\n",
        "        corresponding feature data.\n",
        "    2.  **Temporal Alignment**: Ensures the portfolio snapshot date aligns with\n",
        "        the returns data and configuration.\n",
        "    3.  **ETF ID Consistency**: Confirms a perfect one-to-one mapping of ETFs\n",
        "        across the holdings and returns datasets.\n",
        "\n",
        "    Args:\n",
        "        etf_holdings_df (pd.DataFrame): The validated ETF holdings DataFrame.\n",
        "        bond_features_df (pd.DataFrame): The validated bond features DataFrame.\n",
        "        monthly_returns_df (pd.DataFrame): The validated monthly returns DataFrame.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[bool, List[str]]: A tuple containing:\n",
        "        - bool: True if all datasets are consistent with each other, False otherwise.\n",
        "        - List[str]: A comprehensive list of all consistency errors found.\n",
        "    \"\"\"\n",
        "    # Perform initial input type checks to ensure all inputs are DataFrames.\n",
        "    if not all(isinstance(df, pd.DataFrame) for df in [etf_holdings_df, bond_features_df, monthly_returns_df]):\n",
        "        return False, [\"All data inputs must be pandas DataFrames.\"]\n",
        "    if not isinstance(config, dict):\n",
        "        return False, [\"Input 'config' must be a dictionary.\"]\n",
        "\n",
        "    # --- Step 1: Validate the CUSIP universe consistency ---\n",
        "    cusip_errors = _validate_cusip_consistency(etf_holdings_df, bond_features_df)\n",
        "\n",
        "    # --- Step 2: Validate the temporal (date) alignment ---\n",
        "    temporal_errors = _validate_temporal_alignment(\n",
        "        etf_holdings_df, monthly_returns_df, config\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Validate the ETF identifier universe consistency ---\n",
        "    etf_id_errors = _validate_etf_id_consistency(\n",
        "        etf_holdings_df, monthly_returns_df\n",
        "    )\n",
        "\n",
        "    # --- Aggregation of all errors ---\n",
        "    all_errors = cusip_errors + temporal_errors + etf_id_errors\n",
        "\n",
        "    # --- Final Verdict ---\n",
        "    is_valid = not all_errors\n",
        "\n",
        "    return is_valid, all_errors\n"
      ],
      "metadata": {
        "id": "qbxsd-TBu3mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: ETF Holdings Data Cleansing\n",
        "\n",
        "def _cleanse_holdings_identifiers(\n",
        "    etf_holdings_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Standardizes string identifier fields in the ETF holdings DataFrame.\n",
        "\n",
        "    This function performs essential cleaning on security and fund identifiers\n",
        "    to ensure they are in a canonical format for reliable matching and joining.\n",
        "    It applies two main operations:\n",
        "    1.  Removes leading and trailing whitespace.\n",
        "    2.  Converts identifiers to uppercase for case-insensitive consistency.\n",
        "\n",
        "    Args:\n",
        "        etf_holdings_df (pd.DataFrame): The ETF holdings DataFrame with raw\n",
        "                                        identifier strings.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with cleansed and standardized identifiers.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original DataFrame in place.\n",
        "    df = etf_holdings_df.copy()\n",
        "\n",
        "    # Define the list of identifier columns that require standardization.\n",
        "    identifier_columns: List[str] = ['cusip', 'isin', 'sedol', 'etf_id']\n",
        "\n",
        "    # Iterate through each identifier column to apply cleansing operations.\n",
        "    for col in identifier_columns:\n",
        "        # Ensure the column is of string type before applying string operations.\n",
        "        # This prevents errors if a column was unexpectedly numeric.\n",
        "        df[col] = df[col].astype(str)\n",
        "\n",
        "        # Chain vectorized string operations for maximum efficiency.\n",
        "        # .str.strip() removes leading/trailing whitespace.\n",
        "        # .str.upper() converts all characters to uppercase.\n",
        "        df[col] = df[col].str.strip().str.upper()\n",
        "\n",
        "    # Cleanse the 'etf_name' column separately, preserving title case.\n",
        "    # Remove leading/trailing whitespace.\n",
        "    # Replace multiple spaces with a single space for consistency.\n",
        "    df['etf_name'] = df['etf_name'].str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def _normalize_holdings_weights(\n",
        "    etf_holdings_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalizes portfolio weights for each ETF to ensure they sum exactly to 1.0.\n",
        "\n",
        "    This function corrects for minor floating-point discrepancies in source data\n",
        "    by recalculating each constituent's weight as a fraction of the portfolio's\n",
        "    total weight. This is a critical step for ensuring portfolios represent\n",
        "    valid probability distributions.\n",
        "\n",
        "    Args:\n",
        "        etf_holdings_df (pd.DataFrame): The ETF holdings DataFrame.\n",
        "        config (Dict[str, Any]): The research configuration dictionary, used for\n",
        "                                 numerical precision parameters.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with normalized and rounded portfolio weights.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original DataFrame.\n",
        "    df = etf_holdings_df.copy()\n",
        "\n",
        "    # Calculate the sum of weights for each ETF group.\n",
        "    # .groupby('etf_id')['weight'] selects the weight column for each ETF.\n",
        "    # .transform('sum') computes the sum for each group and broadcasts the result\n",
        "    # back to a Series with the same index as the original DataFrame.\n",
        "    weight_sums = df.groupby('etf_id')['weight'].transform('sum')\n",
        "\n",
        "    # To prevent division by zero, replace any zero sums with NaN, which will\n",
        "    # result in NaN weights that can be handled or flagged.\n",
        "    weight_sums = weight_sums.replace(0, np.nan)\n",
        "\n",
        "    # Normalize the weights by dividing each weight by its group's sum.\n",
        "    df['weight'] = df['weight'] / weight_sums\n",
        "\n",
        "    # Round the normalized weights to the specified precision from the config.\n",
        "    # This ensures consistent numerical representation.\n",
        "    precision = config['computational_config']['weight_precision']\n",
        "    df['weight'] = df['weight'].round(precision)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def _aggregate_duplicate_holdings(\n",
        "    etf_holdings_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates duplicate security holdings within each ETF portfolio.\n",
        "\n",
        "    If a security (CUSIP) appears multiple times for the same ETF, this function\n",
        "    consolidates these entries into a single record. It sums numerical fields\n",
        "    like weights and market values, and preserves the first observed value for\n",
        "    descriptive fields.\n",
        "\n",
        "    Args:\n",
        "        etf_holdings_df (pd.DataFrame): The ETF holdings DataFrame which may\n",
        "                                        contain duplicate entries.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with unique holdings per ETF.\n",
        "    \"\"\"\n",
        "    # Check if there are any duplicates to aggregate.\n",
        "    # The composite key for a unique holding is the ETF and the security CUSIP.\n",
        "    if not etf_holdings_df.duplicated(subset=['etf_id', 'cusip']).any():\n",
        "        # If no duplicates exist, return the original DataFrame.\n",
        "        return etf_holdings_df\n",
        "\n",
        "    # Define the aggregation logic for each column.\n",
        "    # Numerical values are summed.\n",
        "    # Descriptive/identifier values take the first observed instance.\n",
        "    aggregation_rules: Dict[str, str] = {\n",
        "        'weight': 'sum',\n",
        "        'market_value_usd': 'sum',\n",
        "        'shares_held': 'sum',\n",
        "        'etf_name': 'first',\n",
        "        'isin': 'first',\n",
        "        'sedol': 'first',\n",
        "        'as_of_date': 'first'\n",
        "    }\n",
        "\n",
        "    # Group by the composite key and apply the aggregation rules.\n",
        "    # .reset_index() converts the grouped keys back into columns.\n",
        "    df_aggregated = etf_holdings_df.groupby(['etf_id', 'cusip'], as_index=False).agg(\n",
        "        aggregation_rules\n",
        "    )\n",
        "\n",
        "    return df_aggregated\n",
        "\n",
        "\n",
        "def cleanse_etf_holdings_dataframe(\n",
        "    etf_holdings_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete cleansing of the ETF holdings DataFrame.\n",
        "\n",
        "    This master function executes a sequential cleansing pipeline to transform\n",
        "    raw (but validated) holdings data into a standardized, analytically pure\n",
        "    format. The pipeline ensures that all identifiers are canonical, all\n",
        "    portfolios are numerically sound, and all entries are unique.\n",
        "\n",
        "    The cleansing pipeline proceeds in four steps:\n",
        "    1.  **Identifier Standardization**: Cleans and standardizes all string-based\n",
        "        security and fund identifiers (e.g., CUSIP, ISIN, ETF ID).\n",
        "    2.  **Duplicate Aggregation**: Consolidates any duplicate security entries\n",
        "        within each ETF portfolio, summing numerical values.\n",
        "    3.  **Weight Normalization (Post-Aggregation)**: Re-normalizes portfolio\n",
        "        weights after aggregation to ensure they sum precisely to 1.0. This\n",
        "        step is crucial as the summation in the previous step can alter totals.\n",
        "    4.  **Final Validation**: A final check ensures the cleansed data still\n",
        "        adheres to the most critical constraints (e.g., weight sums).\n",
        "\n",
        "    Args:\n",
        "        etf_holdings_df (pd.DataFrame): The raw (but validated) ETF holdings\n",
        "                                        DataFrame to be cleansed.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A cleansed, standardized, and analytically ready\n",
        "                      DataFrame of ETF holdings.\n",
        "    \"\"\"\n",
        "    # Input validation for the function itself.\n",
        "    if not isinstance(etf_holdings_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'etf_holdings_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Standardize all string identifiers ---\n",
        "    # This ensures that CUSIPs, ISINs, etc., are in a consistent format.\n",
        "    df_cleaned = _cleanse_holdings_identifiers(etf_holdings_df)\n",
        "\n",
        "    # --- Step 2: Aggregate any duplicate holdings ---\n",
        "    # This ensures each security appears only once per ETF.\n",
        "    df_cleaned = _aggregate_duplicate_holdings(df_cleaned)\n",
        "\n",
        "    # --- Step 3: Normalize portfolio weights ---\n",
        "    # This is performed *after* aggregation because summing weights can\n",
        "    # change the portfolio total. This step ensures all portfolios sum to 1.0.\n",
        "    df_cleaned = _normalize_holdings_weights(df_cleaned, config)\n",
        "\n",
        "    # --- Step 4: Final Sanity Check ---\n",
        "    # After all transformations, perform a final check on the weight sums.\n",
        "    weight_sums = df_cleaned.groupby('etf_id')['weight'].sum()\n",
        "    if not np.allclose(weight_sums, 1.0, atol=1e-6):\n",
        "        raise ValueError(\n",
        "            \"Weights do not sum to 1.0 for all ETFs after cleansing. \"\n",
        "            \"Please check the aggregation and normalization logic.\"\n",
        "        )\n",
        "\n",
        "    return df_cleaned\n"
      ],
      "metadata": {
        "id": "c_2y_2rcvcky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Bond Features Data Cleansing\n",
        "\n",
        "def _cleanse_features_categorical(\n",
        "    bond_features_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Standardizes categorical and identifier text fields in the bond features DataFrame.\n",
        "\n",
        "    This function applies consistent formatting rules to all categorical and\n",
        "    string-based identifier columns to ensure they are in a canonical format.\n",
        "    This is essential for accurate grouping, joining, and feature encoding.\n",
        "\n",
        "    - CUSIPs and Ratings: Converted to uppercase.\n",
        "    - Issuer, Industry, Country: Converted to title case after stripping whitespace.\n",
        "\n",
        "    Args:\n",
        "        bond_features_df (pd.DataFrame): The bond features DataFrame with raw\n",
        "                                          categorical text data.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with cleansed and standardized text features.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original DataFrame in place.\n",
        "    df = bond_features_df.copy()\n",
        "\n",
        "    # Define columns to be converted to uppercase.\n",
        "    # These are typically codes or identifiers.\n",
        "    upper_case_cols: List[str] = ['cusip', 'bond_rating_composite']\n",
        "    for col in upper_case_cols:\n",
        "        if col in df.columns:\n",
        "            # Chain vectorized string operations for efficiency.\n",
        "            df[col] = df[col].str.strip().str.upper()\n",
        "\n",
        "    # Define columns to be converted to title case.\n",
        "    # This is suitable for proper nouns like company or country names.\n",
        "    title_case_cols: List[str] = ['issuer', 'industry', 'country_of_risk']\n",
        "    for col in title_case_cols:\n",
        "        if col in df.columns:\n",
        "            # Chain operations: strip whitespace, replace multiple spaces, convert to title case.\n",
        "            df[col] = (\n",
        "                df[col].str.strip()\n",
        "                .str.replace(r'\\s+', ' ', regex=True)\n",
        "                .str.title()\n",
        "            )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def _correct_features_numerical(\n",
        "    bond_features_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validates and corrects numerical features based on business logic.\n",
        "\n",
        "    This function applies specific, rule-based corrections to numerical columns\n",
        "    to enforce logical constraints and handle invalid data points as per the\n",
        "    task specification.\n",
        "\n",
        "    - `age_days`: Enforces a minimum value of 1.\n",
        "    - `coupon_frequency`: Imputes invalid values with the column's mode.\n",
        "    - `coupon_rate`, `amount_issued_usd`: Rounds to specified precision.\n",
        "\n",
        "    Args:\n",
        "        bond_features_df (pd.DataFrame): The bond features DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with corrected numerical features.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original DataFrame.\n",
        "    df = bond_features_df.copy()\n",
        "\n",
        "    # Correction 1: Enforce minimum age for 'age_days'.\n",
        "    # A bond's age cannot be zero or negative. We enforce a minimum of 1 day.\n",
        "    # numpy.where is a highly efficient vectorized conditional operation.\n",
        "    if 'age_days' in df.columns:\n",
        "        df['age_days'] = np.where(df['age_days'] <= 0, 1, df['age_days'])\n",
        "\n",
        "    # Correction 2: Impute invalid 'coupon_frequency' values.\n",
        "    if 'coupon_frequency' in df.columns:\n",
        "        # Define the set of valid frequencies.\n",
        "        valid_frequencies = {1, 2, 4, 12}\n",
        "        # Create a boolean mask to identify rows with invalid frequencies.\n",
        "        invalid_mask = ~df['coupon_frequency'].isin(valid_frequencies)\n",
        "\n",
        "        # Proceed only if there are invalid values to correct.\n",
        "        if invalid_mask.any():\n",
        "            # Calculate the mode of the *valid* frequencies.\n",
        "            # This is the most common payment frequency (typically 2 for semi-annual).\n",
        "            valid_mode = df.loc[~invalid_mask, 'coupon_frequency'].mode()[0]\n",
        "\n",
        "            # Use the mask with .loc to replace only the invalid values.\n",
        "            df.loc[invalid_mask, 'coupon_frequency'] = valid_mode\n",
        "\n",
        "    # Correction 3: Round numerical columns to their specified precision.\n",
        "    if 'coupon_rate' in df.columns:\n",
        "        df['coupon_rate'] = df['coupon_rate'].round(3)\n",
        "    if 'amount_issued_usd' in df.columns:\n",
        "        df['amount_issued_usd'] = df['amount_issued_usd'].round(2)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def _validate_features_targets(\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the integrity of the target variable columns.\n",
        "\n",
        "    This function enforces the critical assumption from the paper that the\n",
        "    dataset contains no missing values for the target variables (OAS and Yield).\n",
        "    If the corresponding configuration flag is set, it will check for NaNs and\n",
        "    raise a ValueError if any are found, halting the pipeline.\n",
        "\n",
        "    Args:\n",
        "        bond_features_df (pd.DataFrame): The bond features DataFrame.\n",
        "        config (Dict[str, Any]): The research configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If `validate_no_missing_values` is True and missing\n",
        "                    values are found in the target columns.\n",
        "    \"\"\"\n",
        "    # This check is conditional on the configuration setting.\n",
        "    if config['feature_engineering']['validate_no_missing_values']:\n",
        "        # Define the target variable columns.\n",
        "        target_cols = ['oas_bp', 'yield_to_maturity']\n",
        "\n",
        "        # Check for missing values in the specified target columns.\n",
        "        missing_in_targets = bond_features_df[target_cols].isnull().sum()\n",
        "\n",
        "        # Filter for columns that actually have missing values.\n",
        "        affected_cols = missing_in_targets[missing_in_targets > 0]\n",
        "\n",
        "        # If any target columns have missing values, raise a critical error.\n",
        "        if not affected_cols.empty:\n",
        "            raise ValueError(\n",
        "                \"Missing values found in target variables, violating a core \"\n",
        "                \"assumption of the study. Please clean the source data. \"\n",
        "                f\"Affected columns and counts: {affected_cols.to_dict()}\"\n",
        "            )\n",
        "\n",
        "\n",
        "def cleanse_bond_features_dataframe(\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete cleansing of the bond features DataFrame.\n",
        "\n",
        "    This master function executes a sequential cleansing pipeline to transform\n",
        "    the raw (but validated) bond features data into a pristine, standardized\n",
        "    format suitable for machine learning.\n",
        "\n",
        "    The cleansing pipeline proceeds in three steps:\n",
        "    1.  **Categorical Standardization**: Cleans and standardizes all text-based\n",
        "        categorical features and identifiers.\n",
        "    2.  **Numerical Correction**: Applies specific, rule-based corrections to\n",
        "        numerical features to enforce business logic (e.g., minimum age).\n",
        "    3.  **Target Variable Validation**: Performs a final, critical check to\n",
        "        ensure the target variables for the ML model are complete and contain\n",
        "        no missing values, as per the paper's methodology.\n",
        "\n",
        "    Args:\n",
        "        bond_features_df (pd.DataFrame): The raw (but validated) bond features\n",
        "                                          DataFrame to be cleansed.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A cleansed, standardized, and analytically ready\n",
        "                      DataFrame of bond features.\n",
        "    \"\"\"\n",
        "    # Input validation for the function itself.\n",
        "    if not isinstance(bond_features_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'bond_features_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Standardize categorical and identifier text fields ---\n",
        "    df_cleaned = _cleanse_features_categorical(bond_features_df)\n",
        "\n",
        "    # --- Step 2: Apply rule-based corrections to numerical features ---\n",
        "    df_cleaned = _correct_features_numerical(df_cleaned)\n",
        "\n",
        "    # --- Step 3: Validate integrity of target variables (fail-fast) ---\n",
        "    # This step does not return a value but will raise an error if validation fails.\n",
        "    _validate_features_targets(df_cleaned, config)\n",
        "\n",
        "    return df_cleaned\n"
      ],
      "metadata": {
        "id": "77FqCRNiwbf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Monthly Returns Data Cleansing\n",
        "\n",
        "def _standardize_returns_index(\n",
        "    monthly_returns_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Standardizes the time series structure of the monthly returns DataFrame.\n",
        "\n",
        "    This function ensures the DataFrame is optimally structured for time-series\n",
        "    analysis by:\n",
        "    1.  Converting the 'date' column to a pandas DatetimeIndex.\n",
        "    2.  Sorting the DataFrame chronologically by the new index.\n",
        "    3.  Verifying the integrity of the index (uniqueness and monotonicity).\n",
        "\n",
        "    Args:\n",
        "        monthly_returns_df (pd.DataFrame): The monthly returns DataFrame with\n",
        "                                           a 'date' column.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with a clean, sorted DatetimeIndex.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original DataFrame.\n",
        "    df = monthly_returns_df.copy()\n",
        "\n",
        "    # Convert the 'date' column to datetime objects, if not already.\n",
        "    # This is idempotent and ensures the correct dtype for indexing.\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    # Sort the DataFrame by date to ensure chronological order.\n",
        "    df = df.sort_values(by='date')\n",
        "\n",
        "    # Set the 'date' column as the DataFrame's index.\n",
        "    df = df.set_index('date')\n",
        "\n",
        "    # Perform final integrity checks on the newly created index.\n",
        "    if not df.index.is_unique:\n",
        "        raise ValueError(\"The 'date' index contains duplicate values after sorting.\")\n",
        "    if not df.index.is_monotonic_increasing:\n",
        "        raise ValueError(\"The 'date' index is not monotonically increasing.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def _handle_missing_returns(\n",
        "    monthly_returns_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Enforces the no-missing-values policy for the returns data.\n",
        "\n",
        "    The source paper's methodology strictly requires a complete return series for\n",
        "    all ETFs. This function enforces that constraint. By default, it will raise\n",
        "    a ValueError if any missing values (NaNs) are found. A conditional imputation\n",
        "    path is included for flexibility but is not the default behavior.\n",
        "\n",
        "    Args:\n",
        "        monthly_returns_df (pd.DataFrame): The returns DataFrame, indexed by date.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The validated DataFrame, guaranteed to have no missing values.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any missing values are found in the return columns.\n",
        "    \"\"\"\n",
        "    # Check for any missing values across all return columns.\n",
        "    # .values.any() is a highly optimized way to check the entire DataFrame.\n",
        "    if monthly_returns_df.isnull().values.any():\n",
        "        # If missing values are found, raise a critical error.\n",
        "        # This enforces the strict methodology of the paper.\n",
        "        missing_counts = monthly_returns_df.isnull().sum()\n",
        "        affected_cols = missing_counts[missing_counts > 0].to_dict()\n",
        "        raise ValueError(\n",
        "            \"Missing values detected in the monthly returns data, which violates \"\n",
        "            f\"the study's core requirement. Affected columns: {affected_cols}\"\n",
        "        )\n",
        "\n",
        "    # If no missing values are found, return the DataFrame as is.\n",
        "    return monthly_returns_df\n",
        "\n",
        "\n",
        "def _format_return_values(\n",
        "    monthly_returns_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Standardizes the numerical format and precision of return values.\n",
        "\n",
        "    This function ensures all return data is numerically consistent by:\n",
        "    1.  Rounding all return values to a specified number of decimal places.\n",
        "    2.  Performing a final validation of the value ranges to confirm the\n",
        "        decimal format.\n",
        "\n",
        "    Args:\n",
        "        monthly_returns_df (pd.DataFrame): The returns DataFrame, indexed by date.\n",
        "        config (Dict[str, Any]): The research configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with formatted return values.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original DataFrame.\n",
        "    df = monthly_returns_df.copy()\n",
        "\n",
        "    # Get the required precision for correlation calculations from the config.\n",
        "    precision = config['computational_config']['correlation_precision']\n",
        "\n",
        "    # Apply the rounding operation to all numerical columns.\n",
        "    # The index is not affected.\n",
        "    df = df.round(precision)\n",
        "\n",
        "    # Perform a final sanity check on the magnitude of the returns.\n",
        "    # This confirms that the data is in decimal format (e.g., 0.0234) and not\n",
        "    # percentage format (e.g., 2.34).\n",
        "    max_abs_return = df.abs().max().max()\n",
        "    if max_abs_return >= 1.0:\n",
        "        # A return of 100% or more in a month for a bond ETF is highly\n",
        "        # improbable and likely indicates a data formatting error.\n",
        "        raise ValueError(\n",
        "            f\"Maximum absolute return is {max_abs_return:.2%}. Data appears to be \"\n",
        "            \"in percentage format or contains extreme outliers. Please verify.\"\n",
        "        )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def cleanse_monthly_returns_dataframe(\n",
        "    monthly_returns_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete cleansing of the monthly returns DataFrame.\n",
        "\n",
        "    This master function prepares the ground-truth returns data for econometric\n",
        "    analysis. It transforms the validated raw data into a pristine,\n",
        "    analysis-ready time-series format.\n",
        "\n",
        "    The cleansing pipeline proceeds in three steps:\n",
        "    1.  **Index Standardization**: Converts the 'date' column into a clean,\n",
        "        sorted, and unique DatetimeIndex, which is the standard for\n",
        "        time-series operations in pandas.\n",
        "    2.  **Missing Value Enforcement**: Strictly enforces the paper's requirement\n",
        "        of no missing data by raising an error if any NaNs are present.\n",
        "    3.  **Value Formatting**: Standardizes the numerical precision of all return\n",
        "        values by rounding them to the specified number of decimal places.\n",
        "\n",
        "    Args:\n",
        "        monthly_returns_df (pd.DataFrame): The raw (but validated) monthly\n",
        "                                           returns DataFrame.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A cleansed, indexed, and formatted DataFrame ready for\n",
        "                      correlation analysis.\n",
        "    \"\"\"\n",
        "    # Input validation for the function itself.\n",
        "    if not isinstance(monthly_returns_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'monthly_returns_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Standardize the date index ---\n",
        "    # This creates a proper time-series structure.\n",
        "    df_cleaned = _standardize_returns_index(monthly_returns_df)\n",
        "\n",
        "    # --- Step 2: Enforce the no-missing-values policy ---\n",
        "    # This is a critical, non-negotiable check based on the paper's methodology.\n",
        "    df_cleaned = _handle_missing_returns(df_cleaned)\n",
        "\n",
        "    # --- Step 3: Standardize the numerical format of the return values ---\n",
        "    # This ensures consistent precision for all subsequent calculations.\n",
        "    df_cleaned = _format_return_values(df_cleaned, config)\n",
        "\n",
        "    return df_cleaned\n"
      ],
      "metadata": {
        "id": "lWFBha2KxaAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Prepare Categorical Features for One-Hot Encoding\n",
        "\n",
        "def _get_categorical_features_to_encode(\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[List[str], Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Identifies categorical features for encoding based on the configuration.\n",
        "\n",
        "    This function maps the conceptual feature names from the configuration to the\n",
        "    actual column names in the DataFrame. It handles potential discrepancies and\n",
        "    gracefully ignores configured features that are not present in the data,\n",
        "    logging a warning for transparency.\n",
        "\n",
        "    Args:\n",
        "        bond_features_df (pd.DataFrame): The cleansed bond features DataFrame.\n",
        "        config (Dict[str, Any]): The validated research configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[str], Dict[str, str]]: A tuple containing:\n",
        "        - A list of actual column names in the DataFrame to be encoded.\n",
        "        - The mapping dictionary used for the translation.\n",
        "    \"\"\"\n",
        "    # Define the explicit mapping from conceptual config names to actual column names.\n",
        "    # This provides a robust layer of abstraction.\n",
        "    column_mapping: Dict[str, str] = {\n",
        "        'issuer': 'issuer',\n",
        "        'industry': 'industry',\n",
        "        'bond_rating': 'bond_rating_composite',\n",
        "        'country': 'country_of_risk',\n",
        "        'flag_144a': 'rule_144a_flag',\n",
        "        # The following are in the config but not in the provided data schema.\n",
        "        'market': 'market',\n",
        "        'currency': 'currency'\n",
        "    }\n",
        "\n",
        "    # Retrieve the list of conceptual feature names from the config.\n",
        "    features_from_config = config['feature_engineering']['categorical_features_to_one_hot_encode']\n",
        "\n",
        "    # Initialize the list to hold the actual column names found in the DataFrame.\n",
        "    columns_to_encode: List[str] = []\n",
        "\n",
        "    # Iterate through the configured features to find their corresponding columns.\n",
        "    for feature_name in features_from_config:\n",
        "        # Get the actual column name from the mapping.\n",
        "        actual_col_name = column_mapping.get(feature_name)\n",
        "\n",
        "        # Check if the mapped column name exists in the DataFrame.\n",
        "        if actual_col_name and actual_col_name in bond_features_df.columns:\n",
        "            columns_to_encode.append(actual_col_name)\n",
        "        else:\n",
        "            # If a configured feature is not found, log a warning.\n",
        "            # This makes the pipeline robust to minor schema changes.\n",
        "            logging.warning(\n",
        "                f\"Categorical feature '{feature_name}' from config not found \"\n",
        "                f\"in the bond features DataFrame. It will be skipped.\"\n",
        "            )\n",
        "\n",
        "    return columns_to_encode, column_mapping\n",
        "\n",
        "\n",
        "def _analyze_categorical_cardinality(\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    columns_to_encode: List[str]\n",
        ") -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Analyzes and reports the cardinality of categorical features.\n",
        "\n",
        "    This function calculates the number of unique values for each categorical\n",
        "    feature designated for encoding. This information is crucial for anticipating\n",
        "    the dimensionality of the resulting feature matrix and potential memory usage.\n",
        "\n",
        "    Args:\n",
        "        bond_features_df (pd.DataFrame): The cleansed bond features DataFrame.\n",
        "        columns_to_encode (List[str]): The list of column names to analyze.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, int]: A dictionary mapping each column name to its cardinality.\n",
        "    \"\"\"\n",
        "    # Use a dictionary comprehension for a concise and efficient calculation.\n",
        "    # .nunique() is an optimized pandas method for counting unique values.\n",
        "    cardinality_report = {\n",
        "        col: bond_features_df[col].nunique() for col in columns_to_encode\n",
        "    }\n",
        "\n",
        "    # Log a summary of the analysis for the user.\n",
        "    logging.info(\"Categorical feature cardinality analysis:\")\n",
        "    for col, count in cardinality_report.items():\n",
        "        logging.info(f\"  - Feature '{col}': {count} unique values.\")\n",
        "\n",
        "    total_new_features = sum(cardinality_report.values())\n",
        "    logging.info(\n",
        "        f\"Total new features to be created by one-hot encoding: {total_new_features}\"\n",
        "    )\n",
        "\n",
        "    return cardinality_report\n",
        "\n",
        "\n",
        "def _apply_one_hot_encoding(\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    columns_to_encode: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies one-hot encoding to the specified categorical columns.\n",
        "\n",
        "    This function transforms categorical data into a numerical format suitable\n",
        "    for machine learning models using the pandas `get_dummies` function. It is\n",
        "    optimized for memory efficiency by using a uint8 data type.\n",
        "\n",
        "    Args:\n",
        "        bond_features_df (pd.DataFrame): The cleansed bond features DataFrame.\n",
        "        columns_to_encode (List[str]): The list of column names to encode.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing only the new one-hot encoded columns.\n",
        "                      The index is preserved for later alignment.\n",
        "    \"\"\"\n",
        "    # Apply one-hot encoding using pandas.get_dummies.\n",
        "    # This function is highly optimized for this task.\n",
        "    # - columns: Specifies which columns to encode.\n",
        "    # - prefix/prefix_sep: Creates clear column names (e.g., 'industry_Technology').\n",
        "    # - dtype=np.uint8: A critical memory optimization. Uses 1 byte per value\n",
        "    #   instead of the default 8 (for int64), reducing memory usage significantly.\n",
        "    encoded_df = pd.get_dummies(\n",
        "        bond_features_df[columns_to_encode],\n",
        "        columns=columns_to_encode,\n",
        "        prefix=columns_to_encode,\n",
        "        prefix_sep='_',\n",
        "        dtype=np.uint8\n",
        "    )\n",
        "\n",
        "    return encoded_df\n",
        "\n",
        "\n",
        "def prepare_categorical_features(\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the preparation of categorical features for ML model training.\n",
        "\n",
        "    This master function executes a complete pipeline to transform raw categorical\n",
        "    data into a one-hot encoded numerical format. It ensures that the process is\n",
        "    transparent, robust to minor schema changes, and memory-efficient.\n",
        "\n",
        "    The pipeline proceeds in three steps:\n",
        "    1.  **Feature Identification**: Maps conceptual features from the config to\n",
        "        actual DataFrame columns, handling any discrepancies.\n",
        "    2.  **Cardinality Analysis**: Reports the number of unique values per feature\n",
        "        to provide insight into the resulting dimensionality.\n",
        "    3.  **One-Hot Encoding**: Performs the transformation into a numerical format,\n",
        "        optimized for memory usage.\n",
        "\n",
        "    Args:\n",
        "        bond_features_df (pd.DataFrame): The cleansed bond features DataFrame.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the one-hot encoded features, with\n",
        "                      the original index preserved for alignment with other\n",
        "                      feature sets.\n",
        "    \"\"\"\n",
        "    # Input validation for the function itself.\n",
        "    if not isinstance(bond_features_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'bond_features_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Identify the actual columns to be encoded based on config ---\n",
        "    columns_to_encode, _ = _get_categorical_features_to_encode(\n",
        "        bond_features_df, config\n",
        "    )\n",
        "\n",
        "    if not columns_to_encode:\n",
        "        logging.warning(\"No categorical features found to encode. Returning an empty DataFrame.\")\n",
        "        return pd.DataFrame(index=bond_features_df.index)\n",
        "\n",
        "    # --- Step 2: Analyze the cardinality of these features ---\n",
        "    # This provides a crucial checkpoint before the memory-intensive encoding step.\n",
        "    _ = _analyze_categorical_cardinality(bond_features_df, columns_to_encode)\n",
        "\n",
        "    # --- Step 3: Apply the one-hot encoding transformation ---\n",
        "    # This is the core transformation step.\n",
        "    categorical_features_encoded = _apply_one_hot_encoding(\n",
        "        bond_features_df, columns_to_encode\n",
        "    )\n",
        "\n",
        "    logging.info(\n",
        "        f\"Successfully created {categorical_features_encoded.shape[1]} \"\n",
        "        \"one-hot encoded features.\"\n",
        "    )\n",
        "\n",
        "    return categorical_features_encoded\n"
      ],
      "metadata": {
        "id": "LgZZEl6pyULl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Prepare Numerical Features for Normalization\n",
        "\n",
        "def _get_numerical_features_to_scale(\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Identifies numerical features for scaling based on the configuration.\n",
        "\n",
        "    This function maps conceptual feature names from the config to actual\n",
        "    DataFrame columns and verifies that these columns are of a numeric data type,\n",
        "    making them suitable for normalization.\n",
        "\n",
        "    Args:\n",
        "        bond_features_df (pd.DataFrame): The cleansed bond features DataFrame.\n",
        "        config (Dict[str, Any]): The validated research configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of actual, validated numerical column names.\n",
        "    \"\"\"\n",
        "    # Define the explicit mapping from conceptual config names to actual column names.\n",
        "    column_mapping: Dict[str, str] = {\n",
        "        'days_to_maturity': 'days_to_maturity',\n",
        "        'age': 'age_days',\n",
        "        'coupon': 'coupon_rate',\n",
        "        'amount_issued': 'amount_issued_usd',\n",
        "        'coupon_frequency': 'coupon_frequency'\n",
        "    }\n",
        "\n",
        "    # Retrieve the list of conceptual feature names from the config.\n",
        "    features_from_config = config['feature_engineering']['numerical_features']\n",
        "\n",
        "    # Initialize lists for columns to scale and those that fail validation.\n",
        "    columns_to_scale: List[str] = []\n",
        "    invalid_columns: List[str] = []\n",
        "\n",
        "    # Iterate through configured features to find and validate their columns.\n",
        "    for feature_name in features_from_config:\n",
        "        actual_col_name = column_mapping.get(feature_name)\n",
        "        if actual_col_name and actual_col_name in bond_features_df.columns:\n",
        "            # Verify that the column is numeric before adding it to the list.\n",
        "            if pd.api.types.is_numeric_dtype(bond_features_df[actual_col_name]):\n",
        "                columns_to_scale.append(actual_col_name)\n",
        "            else:\n",
        "                invalid_columns.append(actual_col_name)\n",
        "        else:\n",
        "            logging.warning(\n",
        "                f\"Numerical feature '{feature_name}' from config not found \"\n",
        "                f\"in the bond features DataFrame. It will be skipped.\"\n",
        "            )\n",
        "\n",
        "    # If any configured numerical columns were found to be non-numeric, raise an error.\n",
        "    if invalid_columns:\n",
        "        raise TypeError(\n",
        "            f\"The following columns configured as numerical are not of a numeric \"\n",
        "            f\"dtype: {invalid_columns}\"\n",
        "        )\n",
        "\n",
        "    return columns_to_scale\n",
        "\n",
        "\n",
        "def _compute_scaling_parameters(\n",
        "    numerical_features_df: pd.DataFrame\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the maximum value for each numerical feature.\n",
        "\n",
        "    These maximum values serve as the scaling parameters for normalization. This\n",
        "    function should ONLY be called on the training dataset to prevent data\n",
        "\n",
        "    leakage from the test set.\n",
        "\n",
        "    Args:\n",
        "        numerical_features_df (pd.DataFrame): A DataFrame containing only the\n",
        "                                              numerical features from the\n",
        "                                              TRAINING data.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Series where the index is the feature name and the value\n",
        "                   is the maximum value of that feature.\n",
        "    \"\"\"\n",
        "    # Calculate the maximum value for each column. .max() is highly optimized.\n",
        "    scaling_params = numerical_features_df.max()\n",
        "\n",
        "    # Handle the edge case where a maximum value is zero to prevent division by zero.\n",
        "    # Replace any zero with a small epsilon for numerical stability.\n",
        "    epsilon = 1e-9\n",
        "    zero_max_cols = scaling_params[scaling_params == 0].index.tolist()\n",
        "    if zero_max_cols:\n",
        "        logging.warning(\n",
        "            f\"Features {zero_max_cols} have a maximum value of 0. Replacing with \"\n",
        "            f\"{epsilon} for stable division.\"\n",
        "        )\n",
        "        scaling_params[zero_max_cols] = epsilon\n",
        "\n",
        "    return scaling_params\n",
        "\n",
        "\n",
        "def _apply_max_scaling(\n",
        "    features_df: pd.DataFrame,\n",
        "    scaling_params: pd.Series\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies max normalization to a DataFrame of numerical features.\n",
        "\n",
        "    This function scales each feature by dividing it by its corresponding\n",
        "    maximum value, as provided by the scaling_params Series. This transforms\n",
        "    the feature values to a [0, 1] range.\n",
        "\n",
        "    Args:\n",
        "        features_df (pd.DataFrame): The DataFrame of numerical features to scale\n",
        "                                    (can be train or test data).\n",
        "        scaling_params (pd.Series): The scaling parameters (max values) learned\n",
        "                                    from the training data.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with scaled numerical features.\n",
        "    \"\"\"\n",
        "    # Verify that the columns to be scaled exist in the scaling parameters.\n",
        "    if not set(features_df.columns).issubset(set(scaling_params.index)):\n",
        "        missing_params = set(features_df.columns) - set(scaling_params.index)\n",
        "        raise ValueError(\n",
        "            f\"Scaling parameters are missing for the following columns: {missing_params}\"\n",
        "        )\n",
        "\n",
        "    # Apply the vectorized division. Pandas aligns the operation by the\n",
        "    # Series index (column names), ensuring correctness.\n",
        "    # The formula being implemented is: scaled_feature = feature / max(feature)\n",
        "    scaled_df = features_df / scaling_params\n",
        "\n",
        "    # Validate that all scaled values are within the [0, 1] range.\n",
        "    # Use a small tolerance to account for potential floating-point inaccuracies.\n",
        "    if not (scaled_df.min().min() >= -1e-9 and scaled_df.max().max() <= 1.0 + 1e-9):\n",
        "        raise ValueError(\"Scaled values fall outside the expected [0, 1] range.\")\n",
        "\n",
        "    return scaled_df\n",
        "\n",
        "\n",
        "def prepare_numerical_features(\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    scaling_params: pd.Series = None\n",
        ") -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Orchestrates the preparation of numerical features for ML model training.\n",
        "\n",
        "    This master function executes a pipeline to identify, scale, and validate\n",
        "    numerical features. It operates in two modes:\n",
        "    1.  **Training Mode** (`scaling_params` is None): It computes the scaling\n",
        "        parameters (max values) from the provided DataFrame and then applies\n",
        "        the scaling. It returns both the scaled features and the learned parameters.\n",
        "    2.  **Inference Mode** (`scaling_params` is provided): It uses the pre-computed\n",
        "        scaling parameters to scale the provided DataFrame. This prevents data\n",
        "        leakage and ensures consistent scaling between training and test sets.\n",
        "\n",
        "    Args:\n",
        "        bond_features_df (pd.DataFrame): The cleansed bond features DataFrame.\n",
        "        config (Dict[str, Any]): The validated research configuration dictionary.\n",
        "        scaling_params (pd.Series, optional): Pre-computed scaling parameters.\n",
        "                                              If None, they will be computed from\n",
        "                                              the data. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.Series]: A tuple containing:\n",
        "        - The DataFrame with scaled numerical features.\n",
        "        - The Series of scaling parameters used (either computed or passed in).\n",
        "    \"\"\"\n",
        "    # Input validation for the function itself.\n",
        "    if not isinstance(bond_features_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'bond_features_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Identify the numerical columns to be scaled ---\n",
        "    columns_to_scale = _get_numerical_features_to_scale(bond_features_df, config)\n",
        "    numerical_features_df = bond_features_df[columns_to_scale]\n",
        "\n",
        "    # --- Step 2: Compute or validate scaling parameters ---\n",
        "    if scaling_params is None:\n",
        "        # Training Mode: Compute scaling parameters from the current data.\n",
        "        logging.info(\"Computing scaling parameters from the provided data (Training Mode).\")\n",
        "        computed_scaling_params = _compute_scaling_parameters(numerical_features_df)\n",
        "    else:\n",
        "        # Inference Mode: Use the provided scaling parameters.\n",
        "        logging.info(\"Using pre-computed scaling parameters (Inference Mode).\")\n",
        "        if not isinstance(scaling_params, pd.Series):\n",
        "            raise TypeError(\"Input 'scaling_params' must be a pandas Series.\")\n",
        "        computed_scaling_params = scaling_params\n",
        "\n",
        "    # --- Step 3: Apply the max normalization ---\n",
        "    numerical_features_scaled = _apply_max_scaling(\n",
        "        numerical_features_df, computed_scaling_params\n",
        "    )\n",
        "\n",
        "    logging.info(\n",
        "        f\"Successfully scaled {len(columns_to_scale)} numerical features.\"\n",
        "    )\n",
        "\n",
        "    return numerical_features_scaled, computed_scaling_params\n"
      ],
      "metadata": {
        "id": "4HV_D4K-zG1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Construct Complete Feature Matrix\n",
        "\n",
        "def _merge_feature_sets(\n",
        "    categorical_features: pd.DataFrame,\n",
        "    numerical_features: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Merges one-hot encoded categorical and scaled numerical feature sets.\n",
        "\n",
        "    This function combines the two separately processed feature DataFrames into a\n",
        "    single, wide feature matrix (X). It relies on the pandas index (which should\n",
        "    be the CUSIP) to ensure perfect row-wise alignment of the features for each bond.\n",
        "\n",
        "    Args:\n",
        "        categorical_features (pd.DataFrame): DataFrame of one-hot encoded features.\n",
        "        numerical_features (pd.DataFrame): DataFrame of scaled numerical features.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The complete, merged feature matrix (X).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the indices of the input DataFrames do not match.\n",
        "    \"\"\"\n",
        "    # Pre-computation check: Verify that the indices are identical.\n",
        "    # This is a critical guardrail to prevent misalignment.\n",
        "    if not categorical_features.index.equals(numerical_features.index):\n",
        "        raise ValueError(\n",
        "            \"Indices of categorical and numerical feature DataFrames do not match. \"\n",
        "            \"Cannot merge.\"\n",
        "        )\n",
        "\n",
        "    # Concatenate the two DataFrames horizontally (axis=1).\n",
        "    # pandas automatically aligns the data based on the shared index.\n",
        "    complete_feature_matrix = pd.concat(\n",
        "        [categorical_features, numerical_features], axis=1\n",
        "    )\n",
        "\n",
        "    # Final validation of the merged matrix shape.\n",
        "    expected_rows = len(categorical_features)\n",
        "    expected_cols = len(categorical_features.columns) + len(numerical_features.columns)\n",
        "    if complete_feature_matrix.shape != (expected_rows, expected_cols):\n",
        "        raise RuntimeError(\"Merged feature matrix has an unexpected shape.\")\n",
        "\n",
        "    logging.info(\n",
        "        f\"Successfully merged feature sets into a complete matrix with shape: \"\n",
        "        f\"{complete_feature_matrix.shape}\"\n",
        "    )\n",
        "\n",
        "    return complete_feature_matrix\n",
        "\n",
        "\n",
        "def _extract_and_align_targets(\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    complete_feature_matrix: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts the target variables and aligns them with the feature matrix.\n",
        "\n",
        "    This function selects the target columns (OAS and Yield) from the original\n",
        "    features DataFrame and ensures their row order perfectly matches the final,\n",
        "    assembled feature matrix (X). This alignment is critical to ensure the model\n",
        "    learns the correct feature-target relationships.\n",
        "\n",
        "    Args:\n",
        "        bond_features_df (pd.DataFrame): The cleansed, original bond features DataFrame.\n",
        "        complete_feature_matrix (pd.DataFrame): The fully assembled feature matrix (X).\n",
        "        config (Dict[str, Any]): The validated research configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The target matrix (Y), perfectly aligned with X.\n",
        "    \"\"\"\n",
        "    # Define the mapping from conceptual target names to actual column names.\n",
        "    target_mapping: Dict[str, str] = {\n",
        "        'oas': 'oas_bp',\n",
        "        'yield': 'yield_to_maturity'\n",
        "    }\n",
        "\n",
        "    # Get the list of actual target column names.\n",
        "    target_cols = [\n",
        "        target_mapping[t] for t in config['feature_engineering']['target_variables']\n",
        "    ]\n",
        "\n",
        "    # Extract the target columns from the original features DataFrame.\n",
        "    target_matrix = bond_features_df[target_cols]\n",
        "\n",
        "    # CRITICAL STEP: Align the target matrix (Y) to the feature matrix (X) index.\n",
        "    # .loc[complete_feature_matrix.index] reorders the rows of the target matrix\n",
        "    # to match the exact order of the feature matrix, guaranteeing perfect alignment.\n",
        "    aligned_target_matrix = target_matrix.loc[complete_feature_matrix.index]\n",
        "\n",
        "    logging.info(\n",
        "        f\"Successfully extracted and aligned target matrix (Y) with shape: \"\n",
        "        f\"{aligned_target_matrix.shape}\"\n",
        "    )\n",
        "\n",
        "    return aligned_target_matrix\n",
        "\n",
        "\n",
        "def _perform_data_split(\n",
        "    feature_matrix: pd.DataFrame,\n",
        "    target_matrix: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Splits the feature and target matrices into training and testing sets.\n",
        "\n",
        "    This function uses scikit-learn's `train_test_split` to partition the data\n",
        "    according to the ratios and random state specified in the configuration.\n",
        "    This ensures a reproducible split, which is fundamental for the integrity\n",
        "    of the model evaluation process.\n",
        "\n",
        "    Args:\n",
        "        feature_matrix (pd.DataFrame): The complete feature matrix (X).\n",
        "        target_matrix (pd.DataFrame): The complete target matrix (Y).\n",
        "        config (Dict[str, Any]): The validated research configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: A tuple\n",
        "        containing the four resulting DataFrames in the standard order:\n",
        "        (X_train, X_test, y_train, y_test).\n",
        "    \"\"\"\n",
        "    # Retrieve splitting parameters from the configuration.\n",
        "    test_size = config['feature_engineering']['testing_set_fraction']\n",
        "    random_state = config['feature_engineering']['random_state_for_shuffle']\n",
        "    should_shuffle = config['feature_engineering']['shuffle_before_split']\n",
        "\n",
        "    # Perform the split using scikit-learn's train_test_split.\n",
        "    # Providing both X and Y ensures the split is applied consistently to both.\n",
        "    # The function returns pandas DataFrames because the inputs are DataFrames,\n",
        "    # which preserves the CUSIP index for traceability.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        feature_matrix,\n",
        "        target_matrix,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state,\n",
        "        shuffle=should_shuffle\n",
        "    )\n",
        "\n",
        "    logging.info(\"Successfully split data into training and testing sets.\")\n",
        "    logging.info(f\"  - X_train shape: {X_train.shape}\")\n",
        "    logging.info(f\"  - X_test shape:  {X_test.shape}\")\n",
        "    logging.info(f\"  - y_train shape: {y_train.shape}\")\n",
        "    logging.info(f\"  - y_test shape:  {y_test.shape}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def construct_and_split_data(\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    categorical_features_encoded: pd.DataFrame,\n",
        "    numerical_features_scaled: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction and splitting of the final ML datasets.\n",
        "\n",
        "    This master function brings together all prepared feature sets, assembles\n",
        "    the final feature and target matrices, and partitions them into training\n",
        "    and testing sets suitable for model training and evaluation.\n",
        "\n",
        "    The pipeline proceeds in three steps:\n",
        "    1.  **Feature Merging**: Combines the one-hot encoded categorical features\n",
        "        and the scaled numerical features into a single feature matrix (X).\n",
        "    2.  **Target Extraction and Alignment**: Selects the target variables (OAS,\n",
        "        Yield) and ensures their row order perfectly matches the feature matrix.\n",
        "    3.  **Data Splitting**: Partitions the complete X and Y matrices into\n",
        "        reproducible training and testing sets based on the configuration.\n",
        "\n",
        "    Args:\n",
        "        bond_features_df (pd.DataFrame): The original, cleansed bond features DataFrame.\n",
        "        categorical_features_encoded (pd.DataFrame): DataFrame of one-hot encoded features.\n",
        "        numerical_features_scaled (pd.DataFrame): DataFrame of scaled numerical features.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: A tuple\n",
        "        containing the four final DataFrames for the ML pipeline:\n",
        "        (X_train, X_test, y_train, y_test).\n",
        "    \"\"\"\n",
        "    # Input validation for the function itself.\n",
        "    if not all(isinstance(df, pd.DataFrame) for df in [bond_features_df, categorical_features_encoded, numerical_features_scaled]):\n",
        "        raise TypeError(\"All data inputs must be pandas DataFrames.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Merge the prepared feature sets into a complete matrix (X) ---\n",
        "    # The CUSIP should be the index for alignment.\n",
        "    bond_features_df = bond_features_df.set_index('cusip')\n",
        "\n",
        "    complete_feature_matrix = _merge_feature_sets(\n",
        "        categorical_features_encoded, numerical_features_scaled\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Extract the target variables (Y) and align them with X ---\n",
        "    target_matrix = _extract_and_align_targets(\n",
        "        bond_features_df, complete_feature_matrix, config\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Split the complete X and Y matrices into train/test sets ---\n",
        "    X_train, X_test, y_train, y_test = _perform_data_split(\n",
        "        complete_feature_matrix, target_matrix, config\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n"
      ],
      "metadata": {
        "id": "aa1JzFJ-7FiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Random Forest Hyperparameter Optimization\n",
        "\n",
        "def optimize_random_forest_hyperparameters(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Optimizes Random Forest Regressor hyperparameters using grid search.\n",
        "\n",
        "    This function performs a systematic search for the optimal `n_estimators` and\n",
        "    `max_depth` for a multi-output Random Forest model, strictly following the\n",
        "    methodology outlined in the paper. It uses k-fold cross-validation on the\n",
        "    training data to evaluate each combination of hyperparameters, selecting the\n",
        "    combination that minimizes the Root Mean Squared Error (RMSE).\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  Instantiate a base `RandomForestRegressor` with a fixed random state for\n",
        "        reproducibility.\n",
        "    2.  Define the parameter grid to search over, as specified in the config.\n",
        "    3.  Configure and execute `GridSearchCV` with 5-fold cross-validation,\n",
        "        using all available CPU cores for efficiency.\n",
        "    4.  The search optimizes for the 'neg_root_mean_squared_error' scoring\n",
        "        metric, which is equivalent to minimizing RMSE.\n",
        "    5.  The function extracts, logs, and returns the best-performing parameters.\n",
        "\n",
        "    Args:\n",
        "        X_train (pd.DataFrame): The training feature matrix.\n",
        "        y_train (pd.DataFrame): The training target matrix.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the optimal hyperparameters\n",
        "                        (e.g., {'n_estimators': 200, 'max_depth': 15}).\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(X_train, pd.DataFrame) or not isinstance(y_train, pd.DataFrame):\n",
        "        raise TypeError(\"X_train and y_train must be pandas DataFrames.\")\n",
        "    if X_train.shape[0] != y_train.shape[0]:\n",
        "        raise ValueError(\"X_train and y_train must have the same number of rows.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    logging.info(\"Starting Random Forest hyperparameter optimization...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Step 1: Configure the Grid Search ---\n",
        "    # Retrieve the Random Forest configuration details.\n",
        "    rf_config = config['random_forest_config']\n",
        "\n",
        "    # Define the parameter grid to search. This is taken directly from the config\n",
        "    # to ensure adherence to the study's methodology.\n",
        "    param_grid: Dict[str, Any] = rf_config['hyperparameters_to_tune']\n",
        "    logging.info(f\"Parameter grid for search: {param_grid}\")\n",
        "\n",
        "    # Instantiate the base model. A fixed random_state is crucial for reproducibility\n",
        "    # of the model's internal mechanisms (e.g., bootstrapping).\n",
        "    estimator = RandomForestRegressor(random_state=rf_config['random_state'])\n",
        "\n",
        "    # Instantiate the GridSearchCV object.\n",
        "    # - estimator: The model to tune.\n",
        "    # - param_grid: The hyperparameter space to search.\n",
        "    # - cv: The number of cross-validation folds (5, as per the paper).\n",
        "    # - scoring: The metric to optimize. 'neg_root_mean_squared_error' will be\n",
        "    #   maximized, which is equivalent to minimizing RMSE. Scikit-learn's\n",
        "    #   scorers for multi-output regressors average the scores by default.\n",
        "    # - n_jobs: -1 uses all available CPU cores for parallel execution.\n",
        "    # - verbose: Provides progress updates during the search.\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=estimator,\n",
        "        param_grid=param_grid,\n",
        "        cv=rf_config['cross_validation_folds'],\n",
        "        scoring=rf_config['cv_scoring_metric'],\n",
        "        n_jobs=rf_config['n_jobs'],\n",
        "        verbose=2  # Provide more detailed output during the fit.\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Execute the Grid Search on the Training Data ---\n",
        "    logging.info(f\"Executing {rf_config['cross_validation_folds']}-fold cross-validation grid search...\")\n",
        "    # The .fit() method runs the entire search process. It will train\n",
        "    # (num_param_combinations * cv_folds) models in total.\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # --- Step 3: Extract and Report the Optimal Parameters ---\n",
        "    # The best parameters are stored in the .best_params_ attribute.\n",
        "    best_params: Dict[str, Any] = grid_search.best_params_\n",
        "    # The best score is the average cross-validated score for the best parameters.\n",
        "    # It will be negative, so we take its absolute value to get the RMSE.\n",
        "    best_score_rmse = abs(grid_search.best_score_)\n",
        "\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    logging.info(\"Hyperparameter optimization completed.\")\n",
        "    logging.info(f\"  - Duration: {duration:.2f} seconds.\")\n",
        "    logging.info(f\"  - Best cross-validated RMSE: {best_score_rmse:.4f}\")\n",
        "    logging.info(f\"  - Best parameters found: {best_params}\")\n",
        "\n",
        "    # Final validation to ensure the result is a dictionary.\n",
        "    if not isinstance(best_params, dict):\n",
        "        raise TypeError(\"Grid search did not return a dictionary of best parameters.\")\n",
        "\n",
        "    return best_params\n"
      ],
      "metadata": {
        "id": "xN4xDi3e8exh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Train Final Random Forest Model\n",
        "\n",
        "def _calculate_multi_output_metrics(\n",
        "    y_true: pd.DataFrame,\n",
        "    y_pred: np.ndarray,\n",
        "    target_names: List[str]\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Calculates average RMSE and MAPE for a multi-output regression model.\n",
        "\n",
        "    Args:\n",
        "        y_true (pd.DataFrame): The ground truth target values.\n",
        "        y_pred (np.ndarray): The predicted target values from the model.\n",
        "        target_names (List[str]): The names of the target columns.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]: A tuple containing the average RMSE and average MAPE.\n",
        "    \"\"\"\n",
        "    # Create a DataFrame for predictions to align with y_true's columns.\n",
        "    y_pred_df = pd.DataFrame(y_pred, index=y_true.index, columns=y_true.columns)\n",
        "\n",
        "    rmses, mapes = [], []\n",
        "\n",
        "    # Calculate metrics for each target variable individually.\n",
        "    for i, target in enumerate(target_names):\n",
        "        # Calculate RMSE for the current target.\n",
        "        # RMSE = sqrt(MSE)\n",
        "        rmse = np.sqrt(mean_squared_error(y_true[target], y_pred_df[target]))\n",
        "        rmses.append(rmse)\n",
        "\n",
        "        # Calculate MAPE for the current target.\n",
        "        # MAPE = mean(|(y_true - y_pred) / y_true|) * 100\n",
        "        # Add a small epsilon to the denominator to avoid division by zero.\n",
        "        epsilon = 1e-9\n",
        "        mape = np.mean(np.abs((y_true[target] - y_pred_df[target]) / (y_true[target] + epsilon)))\n",
        "        mapes.append(mape)\n",
        "\n",
        "    # Return the average of the metrics across all target outputs.\n",
        "    return np.mean(rmses), np.mean(mapes)\n",
        "\n",
        "\n",
        "def train_final_model_and_evaluate(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.DataFrame,\n",
        "    X_test: pd.DataFrame,\n",
        "    y_test: pd.DataFrame,\n",
        "    optimal_params: Dict[str, Any],\n",
        "    config: Dict[str, Any]\n",
        ") -> RandomForestRegressor:\n",
        "    \"\"\"\n",
        "    Trains the final Random Forest model and evaluates its performance.\n",
        "\n",
        "    This function takes the optimal hyperparameters, trains a new Random Forest\n",
        "    model on the entire training dataset, and then evaluates its performance on\n",
        "    both the training and testing sets. It validates the performance against the\n",
        "    benchmarks reported in the source paper to ensure the model's fidelity.\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  Instantiate `RandomForestRegressor` with the optimal parameters.\n",
        "    2.  Train the model on the full `X_train` and `y_train` datasets.\n",
        "    3.  Generate predictions for both training and testing sets.\n",
        "    4.  Calculate multi-output RMSE and MAPE for both sets.\n",
        "    5.  Compare the calculated metrics against the expected values from the\n",
        "        configuration, logging warnings if deviations exceed tolerance.\n",
        "\n",
        "    Args:\n",
        "        X_train (pd.DataFrame): The training feature matrix.\n",
        "        y_train (pd.DataFrame): The training target matrix.\n",
        "        X_test (pd.DataFrame): The testing feature matrix.\n",
        "        y_test (pd.DataFrame): The testing target matrix.\n",
        "        optimal_params (Dict[str, Any]): The optimal hyperparameters from grid search.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        RandomForestRegressor: The final, trained scikit-learn model object.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not all(isinstance(df, pd.DataFrame) for df in [X_train, y_train, X_test, y_test]):\n",
        "        raise TypeError(\"All data inputs must be pandas DataFrames.\")\n",
        "    if not isinstance(optimal_params, dict):\n",
        "        raise TypeError(\"Input 'optimal_params' must be a dictionary.\")\n",
        "\n",
        "    logging.info(\"Training final Random Forest model with optimal parameters...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Step 1: Instantiate the final model ---\n",
        "    rf_config = config['random_forest_config']\n",
        "    final_model = RandomForestRegressor(\n",
        "        n_estimators=optimal_params['n_estimators'],\n",
        "        max_depth=optimal_params['max_depth'],\n",
        "        random_state=rf_config['random_state'],\n",
        "        n_jobs=rf_config['n_jobs']\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Train the model on the full training set ---\n",
        "    final_model.fit(X_train, y_train)\n",
        "\n",
        "    end_time = time.time()\n",
        "    logging.info(f\"Model training completed in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    # --- Step 3: Evaluate model performance ---\n",
        "    logging.info(\"Evaluating model performance on training and testing sets...\")\n",
        "    # Generate predictions for both sets.\n",
        "    y_train_pred = final_model.predict(X_train)\n",
        "    y_test_pred = final_model.predict(X_test)\n",
        "\n",
        "    # Get target names for metric calculation.\n",
        "    target_names = y_train.columns.tolist()\n",
        "\n",
        "    # Calculate performance metrics for the training set.\n",
        "    train_rmse, train_mape = _calculate_multi_output_metrics(y_train, y_train_pred, target_names)\n",
        "    logging.info(f\"  - Training Set   | RMSE: {train_rmse:.4f}, MAPE: {train_mape:.4f}\")\n",
        "\n",
        "    # Calculate performance metrics for the testing set.\n",
        "    test_rmse, test_mape = _calculate_multi_output_metrics(y_test, y_test_pred, target_names)\n",
        "    logging.info(f\"  - Testing Set    | RMSE: {test_rmse:.4f}, MAPE: {test_mape:.4f}\")\n",
        "\n",
        "    # --- Step 4: Validate performance against paper benchmarks ---\n",
        "    expected_train_rmse = rf_config['expected_training_rmse']\n",
        "    expected_test_rmse = rf_config['expected_testing_rmse']\n",
        "\n",
        "    # Check training RMSE against the expected value from the paper.\n",
        "    if not np.isclose(train_rmse, expected_train_rmse, atol=0.05):\n",
        "        logging.warning(\n",
        "            f\"Training RMSE ({train_rmse:.4f}) deviates significantly from \"\n",
        "            f\"the paper's reported value ({expected_train_rmse:.4f}).\"\n",
        "        )\n",
        "\n",
        "    # Check testing RMSE against the expected value from the paper.\n",
        "    if not np.isclose(test_rmse, expected_test_rmse, atol=0.05):\n",
        "        logging.warning(\n",
        "            f\"Testing RMSE ({test_rmse:.4f}) deviates significantly from \"\n",
        "            f\"the paper's reported value ({expected_test_rmse:.4f}).\"\n",
        "        )\n",
        "\n",
        "    logging.info(\"Performance evaluation and validation complete.\")\n",
        "\n",
        "    return final_model\n"
      ],
      "metadata": {
        "id": "D4ZKKFwy9dqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Generate Proximity Matrix from Trained Random Forest\n",
        "\n",
        "@njit(parallel=True)\n",
        "def _accelerated_proximity_loop(\n",
        "    leaf_nodes: np.ndarray,\n",
        "    n_samples: int,\n",
        "    n_trees: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculates the proximity matrix using a Numba-accelerated loop.\n",
        "\n",
        "    This function iterates through each tree's leaf assignments and efficiently\n",
        "    updates a co-occurrence matrix. It is designed to be memory-efficient and\n",
        "    fast, leveraging Numba for just-in-time compilation and parallel execution.\n",
        "\n",
        "    Args:\n",
        "        leaf_nodes (np.ndarray): Integer matrix of shape (n_samples, n_trees)\n",
        "                                 containing terminal node indices.\n",
        "        n_samples (int): The number of samples (bonds).\n",
        "        n_trees (int): The number of trees in the forest.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A square matrix of shape (n_samples, n_samples) containing\n",
        "                    the count of times each pair of samples ended in the same leaf.\n",
        "    \"\"\"\n",
        "    # Initialize the proximity count matrix with zeros.\n",
        "    proximity_counts = np.zeros((n_samples, n_samples), dtype=np.int32)\n",
        "\n",
        "    # Iterate through each tree in the forest.\n",
        "    for i in range(n_trees):\n",
        "        # Iterate through each pair of samples (bonds).\n",
        "        for j in range(n_samples):\n",
        "            for k in range(j, n_samples):\n",
        "                # If two samples land in the same leaf node for the current tree...\n",
        "                if leaf_nodes[j, i] == leaf_nodes[k, i]:\n",
        "                    # ...increment the counter for that pair.\n",
        "                    proximity_counts[j, k] += 1\n",
        "\n",
        "    return proximity_counts\n",
        "\n",
        "\n",
        "def generate_proximity_matrix(\n",
        "    final_model: RandomForestRegressor,\n",
        "    X_complete: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates the proximity matrix from the trained Random Forest model.\n",
        "\n",
        "    This function operationalizes Equation (2) from the paper, calculating the\n",
        "    similarity between every pair of bonds based on how frequently they land in\n",
        "    the same terminal node across all trees in the forest.\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  **Leaf Node Extraction**: Use the model's `.apply()` method to get the\n",
        "        terminal node index for each bond in each tree.\n",
        "    2.  **Proximity Calculation**: Employ a memory-efficient, Numba-accelerated\n",
        "        algorithm to count the co-occurrences of sample pairs in the same leaves.\n",
        "        This avoids the extreme memory consumption of a fully vectorized approach.\n",
        "    3.  **Normalization and Optimization**: Divide the co-occurrence counts by the\n",
        "        total number of trees to get the final proximity scores. The resulting\n",
        "        matrix is made perfectly symmetric and memory-optimized.\n",
        "    4.  **Validation**: The final matrix is validated for its essential properties\n",
        "        (symmetry, diagonal of ones, value range).\n",
        "\n",
        "    Args:\n",
        "        final_model (RandomForestRegressor): The final, trained scikit-learn model.\n",
        "        X_complete (pd.DataFrame): The complete feature matrix for ALL bonds\n",
        "                                   (both train and test).\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The final, validated proximity matrix as a DataFrame with\n",
        "                      CUSIPs as the index and columns.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(final_model, RandomForestRegressor):\n",
        "        raise TypeError(\"Input 'final_model' must be a RandomForestRegressor.\")\n",
        "    if not isinstance(X_complete, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'X_complete' must be a pandas DataFrame.\")\n",
        "\n",
        "    logging.info(\"Generating proximity matrix from the trained Random Forest...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Step 1: Extract Leaf Node Assignments ---\n",
        "    # The .apply() method returns an integer array of shape (n_samples, n_estimators)\n",
        "    # where each element is the index of the leaf that a sample ends up in.\n",
        "    logging.info(\"Step 1/4: Extracting terminal leaf node assignments...\")\n",
        "    leaf_nodes = final_model.apply(X_complete)\n",
        "    n_samples, n_trees = leaf_nodes.shape\n",
        "    logging.info(f\"Extracted leaf nodes for {n_samples} samples across {n_trees} trees.\")\n",
        "\n",
        "    # --- Step 2: Calculate Proximity Counts using Accelerated Loop ---\n",
        "    # This is a memory-efficient alternative to full vectorization.\n",
        "    logging.info(\"Step 2/4: Calculating pairwise proximity counts (this may take time)...\")\n",
        "    # Call the Numba-jitted function for high-performance computation.\n",
        "    proximity_counts = _accelerated_proximity_loop(leaf_nodes, n_samples, n_trees)\n",
        "\n",
        "    # --- Step 3: Normalize and Optimize the Matrix ---\n",
        "    logging.info(\"Step 3/4: Normalizing and optimizing the proximity matrix...\")\n",
        "    # The proximity is the count of co-occurrences divided by the number of trees.\n",
        "    # Convert to float32 for memory efficiency (halves memory vs. float64).\n",
        "    proximity_matrix = (proximity_counts / n_trees).astype(np.float32)\n",
        "\n",
        "    # Exploit symmetry: The loop only computes the upper triangle, so we copy it\n",
        "    # to the lower triangle to make the matrix fully symmetric.\n",
        "    # This is faster and more robust than re-calculating.\n",
        "    proximity_matrix = proximity_matrix + proximity_matrix.T - np.diag(proximity_matrix.diagonal())\n",
        "\n",
        "    # --- Step 4: Validate the Final Matrix ---\n",
        "    logging.info(\"Step 4/4: Validating final proximity matrix properties...\")\n",
        "    # Check 1: Symmetry\n",
        "    if not np.allclose(proximity_matrix, proximity_matrix.T, atol=1e-6):\n",
        "        raise ValueError(\"Proximity matrix is not symmetric.\")\n",
        "    # Check 2: Diagonal must be all ones.\n",
        "    if not np.allclose(np.diag(proximity_matrix), 1.0, atol=1e-6):\n",
        "        raise ValueError(\"Diagonal of the proximity matrix is not all ones.\")\n",
        "    # Check 3: All values must be in the [0, 1] range.\n",
        "    if not (proximity_matrix.min() >= -1e-6 and proximity_matrix.max() <= 1.0 + 1e-6):\n",
        "        raise ValueError(\"Proximity matrix contains values outside the [0, 1] range.\")\n",
        "\n",
        "    # Convert the final numpy array to a pandas DataFrame, using the CUSIPs\n",
        "    # from the input DataFrame's index for both the new index and columns.\n",
        "    proximity_df = pd.DataFrame(\n",
        "        proximity_matrix,\n",
        "        index=X_complete.index,\n",
        "        columns=X_complete.index\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    logging.info(f\"Proximity matrix generation completed in {end_time - start_time:.2f} seconds.\")\n",
        "    logging.info(f\"Final matrix shape: {proximity_df.shape}\")\n",
        "\n",
        "    return proximity_df\n"
      ],
      "metadata": {
        "id": "F6jKiOgB-ULx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15: Extract Portfolio-Level Data for Similarity Analysis\n",
        "\n",
        "# Define a type hint for the complex nested dictionary structure for clarity.\n",
        "PortfolioData = Dict[str, Dict[str, Union[List[str], np.ndarray]]]\n",
        "\n",
        "\n",
        "def extract_portfolio_level_data(\n",
        "    etf_holdings_df: pd.DataFrame,\n",
        "    proximity_matrix: pd.DataFrame\n",
        ") -> PortfolioData:\n",
        "    \"\"\"\n",
        "    Extracts and structures portfolio data for similarity analysis.\n",
        "\n",
        "    This function transforms the flat holdings DataFrame into a nested dictionary,\n",
        "    which serves as a highly efficient, analysis-ready data structure for the\n",
        "    subsequent similarity algorithms. For each ETF, it compiles its constituent\n",
        "    CUSIPs, their corresponding weights, and their integer indices within the\n",
        "    proximity matrix.\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  **Create CUSIP-to-Index Map**: A mapping is built from the CUSIPs in the\n",
        "        proximity matrix's index to their integer positions (0, 1, 2, ...).\n",
        "        This allows for O(1) lookups.\n",
        "    2.  **Group by ETF**: The holdings DataFrame is grouped by 'etf_id' to\n",
        "        process each portfolio individually.\n",
        "    3.  **Extract and Translate**: For each ETF, the function extracts its list\n",
        "        of CUSIPs and weights. It then uses the map to translate the CUSIPs\n",
        "        into their corresponding integer indices.\n",
        "    4.  **Assemble Structure**: The extracted CUSIPs, weights (as a NumPy array),\n",
        "        and indices (as a NumPy array) are stored in a nested dictionary keyed\n",
        "        by the ETF identifier.\n",
        "    5.  **Validation**: A final check ensures the weights for each extracted\n",
        "        portfolio sum to approximately 1.0.\n",
        "\n",
        "    Args:\n",
        "        etf_holdings_df (pd.DataFrame): The cleansed and validated DataFrame of\n",
        "                                        ETF holdings.\n",
        "        proximity_matrix (pd.DataFrame): The final, validated proximity matrix\n",
        "                                         with CUSIPs as its index and columns.\n",
        "\n",
        "    Returns:\n",
        "        PortfolioData: A nested dictionary where each key is an ETF ID and each\n",
        "                       value is a dictionary containing the portfolio's 'cusips',\n",
        "                       'weights', and 'indices'.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(etf_holdings_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'etf_holdings_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(proximity_matrix, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'proximity_matrix' must be a pandas DataFrame.\")\n",
        "    if not proximity_matrix.index.equals(proximity_matrix.columns):\n",
        "        raise ValueError(\"Proximity matrix must be square with identical index and columns.\")\n",
        "\n",
        "    logging.info(\"Extracting and structuring portfolio-level data...\")\n",
        "\n",
        "    # --- Step 1: Create the CUSIP-to-Index Mapping ---\n",
        "    # This dictionary provides an O(1) lookup for converting a CUSIP string\n",
        "    # to its integer row/column index in the proximity matrix.\n",
        "    cusip_to_idx: Dict[str, int] = {\n",
        "        cusip: i for i, cusip in enumerate(proximity_matrix.index)\n",
        "    }\n",
        "\n",
        "    # Initialize the main dictionary to store the structured portfolio data.\n",
        "    portfolios: PortfolioData = {}\n",
        "\n",
        "    # --- Step 2: Group by ETF and Process Each Portfolio ---\n",
        "    # .groupby() is a highly efficient way to iterate over each ETF's holdings.\n",
        "    for etf_id, group in etf_holdings_df.groupby('etf_id'):\n",
        "        # Ensure the group is sorted by CUSIP for deterministic ordering, although\n",
        "        # the algorithms do not strictly require it.\n",
        "        group = group.sort_values(by='cusip').reset_index(drop=True)\n",
        "\n",
        "        # --- Step 3: Extract CUSIPs, Weights, and Translate to Indices ---\n",
        "        # Extract the list of CUSIPs for the current ETF.\n",
        "        cusips: List[str] = group['cusip'].tolist()\n",
        "        # Extract the corresponding weights as a NumPy array for efficient math.\n",
        "        weights: np.ndarray = group['weight'].to_numpy(dtype=np.float64)\n",
        "\n",
        "        try:\n",
        "            # Translate the list of CUSIP strings into a NumPy array of integers.\n",
        "            # This pre-computation is a critical performance optimization.\n",
        "            indices: np.ndarray = np.array(\n",
        "                [cusip_to_idx[c] for c in cusips], dtype=np.int32\n",
        "            )\n",
        "        except KeyError as e:\n",
        "            # This error should not be reached if cross-dataset validation passed,\n",
        "            # but it serves as a final, robust guardrail.\n",
        "            raise KeyError(\n",
        "                f\"CUSIP {e} from portfolio '{etf_id}' not found in the \"\n",
        "                \"proximity matrix. Data is inconsistent.\"\n",
        "            ) from e\n",
        "\n",
        "        # --- Step 4: Final Validation for the current portfolio ---\n",
        "        # Verify that the weights for this portfolio sum to 1.0.\n",
        "        if not np.isclose(weights.sum(), 1.0, atol=1e-6):\n",
        "            logging.warning(\n",
        "                f\"Weights for ETF '{etf_id}' sum to {weights.sum():.8f}, which is \"\n",
        "                \"not 1.0. This may indicate an issue in the cleansing process.\"\n",
        "            )\n",
        "\n",
        "        # --- Step 5: Assemble the final data structure for this ETF ---\n",
        "        portfolios[etf_id] = {\n",
        "            'cusips': cusips,\n",
        "            'weights': weights,\n",
        "            'indices': indices\n",
        "        }\n",
        "\n",
        "    logging.info(f\"Successfully processed {len(portfolios)} portfolios.\")\n",
        "\n",
        "    # Final validation of the output structure.\n",
        "    if len(portfolios) != etf_holdings_df['etf_id'].nunique():\n",
        "        raise RuntimeError(\"The number of processed portfolios does not match the number of unique ETFs.\")\n",
        "\n",
        "    return portfolios\n"
      ],
      "metadata": {
        "id": "1hjedfNt_D_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16: Implement STRAPSim Algorithm\n",
        "\n",
        "# Define type hints for clarity\n",
        "Portfolio = Dict[str, Union[List[str], np.ndarray]]\n",
        "STRAPSimResult = Tuple[float, float, float]\n",
        "\n",
        "\n",
        "def compute_strapsim_between_pair(\n",
        "    portfolio_x: Portfolio,\n",
        "    portfolio_y: Portfolio,\n",
        "    proximity_matrix: np.ndarray,\n",
        "    config: Dict[str, Any]\n",
        ") -> STRAPSimResult:\n",
        "    \"\"\"\n",
        "    Computes the STRAPSim score between a single pair of portfolios.\n",
        "\n",
        "    This function is the core implementation of the STRAPSim algorithm as\n",
        "    described in Section 2.1 and Equations (1a) and (1b) of the paper. It\n",
        "    quantifies the similarity between two weighted sets (portfolios) through an\n",
        "    iterative, greedy matching process that is both weight- and residual-aware.\n",
        "\n",
        "    The algorithm proceeds as follows:\n",
        "    1.  **Initialization**: Copies of the portfolio weight vectors are created to\n",
        "        track residual (unmatched) weights. The total similarity score is set to 0.\n",
        "    2.  **Candidate Generation**: All possible pairwise similarity scores between\n",
        "        constituents of the two portfolios are extracted from the global\n",
        "        proximity matrix.\n",
        "    3.  **Prioritization**: These candidate matches are sorted in descending order\n",
        "        of their similarity score. This pre-sorting step is a crucial\n",
        "        performance optimization that implements the argmax logic of the greedy\n",
        "        approach efficiently.\n",
        "    4.  **Greedy Matching Loop**: The function iterates through the prioritized list\n",
        "        of matches. For each potential match:\n",
        "        a. It determines the amount of weight that can be transferred, which is\n",
        "           the minimum of the two constituents' current residual weights.\n",
        "        b. The similarity score is incremented by this transfer weight multiplied\n",
        "           by the pair's similarity.\n",
        "        c. The residual weights of both constituents are decremented by the\n",
        "           transfer weight, fulfilling the \"residual-aware\" property.\n",
        "    5.  **Finalization**: After iterating through all possible matches, the final\n",
        "        similarity score and the remaining residual weights for each portfolio\n",
        "        are calculated and returned.\n",
        "\n",
        "    Args:\n",
        "        portfolio_x (Portfolio): The structured data for the first portfolio.\n",
        "        portfolio_y (Portfolio): The structured data for the second portfolio.\n",
        "        proximity_matrix (np.ndarray): The global (N x N) matrix of constituent\n",
        "                                       similarities.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        STRAPSimResult: A tuple containing:\n",
        "        - The final STRAPSim similarity score.\n",
        "        - The total residual (unmatched) weight from portfolio X.\n",
        "        - The total residual (unmatched) weight from portfolio Y.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Initialization ---\n",
        "    # Retrieve algorithm parameters from the configuration.\n",
        "    strapsim_config = config['strapsim_algorithm']\n",
        "    weight_tol = strapsim_config['weight_precision_tolerance']\n",
        "\n",
        "    # Create copies of the weight vectors to serve as residual weights.\n",
        "    # This is crucial to avoid modifying the original portfolio data.\n",
        "    residual_weights_x = portfolio_x['weights'].copy()\n",
        "    residual_weights_y = portfolio_y['weights'].copy()\n",
        "\n",
        "    # Initialize the cumulative similarity score.\n",
        "    similarity_score = 0.0\n",
        "\n",
        "    # Extract the integer indices for the constituents of each portfolio.\n",
        "    indices_x = portfolio_x['indices']\n",
        "    indices_y = portfolio_y['indices']\n",
        "\n",
        "    # --- Step 2: Candidate Generation and Prioritization ---\n",
        "    # Create a grid of all pairwise index combinations between the two portfolios.\n",
        "    # This is a highly efficient way to get all pairs for slicing.\n",
        "    idx_pairs = np.ix_(indices_x, indices_y)\n",
        "\n",
        "    # Slice the global proximity matrix to get the sub-matrix of relevant similarities.\n",
        "    sub_matrix = proximity_matrix[idx_pairs]\n",
        "\n",
        "    # Create a list of all candidate matches with their similarity scores and\n",
        "    # their *local* indices within the sub-matrix.\n",
        "    candidate_matches = [\n",
        "        (sub_matrix[i, j], i, j)\n",
        "        for i in range(len(indices_x))\n",
        "        for j in range(len(indices_y))\n",
        "    ]\n",
        "\n",
        "    # Sort the candidates in descending order of similarity. This is the core\n",
        "    # of the greedy approach, ensuring we always process the best match first.\n",
        "    candidate_matches.sort(key=lambda item: item[0], reverse=True)\n",
        "\n",
        "    # --- Step 3: Greedy Matching Loop ---\n",
        "    # Iterate through the prioritized list of all possible matches.\n",
        "    for sim, local_idx_x, local_idx_y in candidate_matches:\n",
        "        # Get the current residual weights for the constituents in this pair.\n",
        "        weight_x = residual_weights_x[local_idx_x]\n",
        "        weight_y = residual_weights_y[local_idx_y]\n",
        "\n",
        "        # If either constituent has already been fully matched, skip this pair.\n",
        "        # We use a tolerance for robust floating-point comparison.\n",
        "        if weight_x < weight_tol or weight_y < weight_tol:\n",
        "            continue\n",
        "\n",
        "        # Determine the amount of weight to transfer: the minimum available.\n",
        "        # This is the core of Equation (1a): min(w_x(i), w_y(j))\n",
        "        transfer_weight = min(weight_x, weight_y)\n",
        "\n",
        "        # Increment the total similarity score.\n",
        "        # This is the core of Equation (1a): score += S_ij * min(...)\n",
        "        similarity_score += sim * transfer_weight\n",
        "\n",
        "        # Decrement the residual weights of both constituents.\n",
        "        # This is the \"residual-aware\" update from Equation (1b).\n",
        "        residual_weights_x[local_idx_x] -= transfer_weight\n",
        "        residual_weights_y[local_idx_y] -= transfer_weight\n",
        "\n",
        "    # --- Step 4: Finalization ---\n",
        "    # Calculate the total remaining (unmatched) weight for each portfolio.\n",
        "    residual_x = np.sum(residual_weights_x)\n",
        "    residual_y = np.sum(residual_weights_y)\n",
        "\n",
        "    # Optional: Perform a weight conservation check for debugging and validation.\n",
        "    if strapsim_config['weight_conservation_validation']:\n",
        "        total_initial_weight = np.sum(portfolio_x['weights']) + np.sum(portfolio_y['weights'])\n",
        "        total_final_value = similarity_score + residual_x + residual_y\n",
        "        # Note: The sum is not 2.0 because similarity_score is not a weight.\n",
        "        # The correct check is that the sum of residual weights equals the sum of initial weights\n",
        "        # minus the total weight that was \"consumed\" and converted into the score.\n",
        "        # A simpler check is not readily available, but we can ensure residuals are non-negative.\n",
        "        if residual_x < -weight_tol or residual_y < -weight_tol:\n",
        "            raise ValueError(\"Weight conservation violated: residual weights are negative.\")\n",
        "\n",
        "    # Round the final score to the specified precision.\n",
        "    precision = strapsim_config['precision_digits']\n",
        "\n",
        "    return round(similarity_score, precision), residual_x, residual_y\n"
      ],
      "metadata": {
        "id": "nPwFxC92ACiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17: Compute STRAPSim for All Portfolio Pairs\n",
        "\n",
        "# Define type hints from previous tasks for clarity.\n",
        "Portfolio = Dict[str, Union[List[str], np.ndarray]]\n",
        "PortfolioData = Dict[str, Portfolio]\n",
        "STRAPSimResult = Tuple[float, float, float]\n",
        "\n",
        "def compute_all_strapsim_pairs(\n",
        "    portfolios: PortfolioData,\n",
        "    proximity_matrix: np.ndarray,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of STRAPSim scores for all unique portfolio pairs.\n",
        "\n",
        "    This function systematically calculates the similarity between every pair of\n",
        "    ETFs in the provided portfolio data structure. It is designed for efficiency\n",
        "    by avoiding redundant calculations and provides clear progress monitoring.\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  **Initialization**: Creates empty matrices for similarity scores and\n",
        "        residuals. It also establishes a deterministic mapping from ETF IDs to\n",
        "        matrix indices.\n",
        "    2.  **Diagonal Population**: Sets the diagonal of the similarity matrix to 1.0\n",
        "        (self-similarity) and the residual matrix to 0.0.\n",
        "    3.  **Pairwise Computation**: Iterates through all unique pairs of ETFs using\n",
        "        `itertools.combinations`. For each pair, it calls the core\n",
        "        `compute_strapsim_between_pair` function.\n",
        "    4.  **Symmetric Storage**: Stores the result for each pair `(i, j)` in both\n",
        "        `matrix[i, j]` and `matrix[j, i]` to ensure the final matrices are symmetric.\n",
        "    5.  **Finalization**: Converts the completed NumPy arrays into well-labeled\n",
        "        pandas DataFrames, with ETF IDs as both the index and columns.\n",
        "\n",
        "    Args:\n",
        "        portfolios (PortfolioData): The nested dictionary of structured portfolio data.\n",
        "        proximity_matrix (np.ndarray): The global (N x N) matrix of constituent\n",
        "                                       similarities.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing:\n",
        "        - A square DataFrame of pairwise STRAPSim similarity scores.\n",
        "        - A square DataFrame of pairwise total residual weights.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(portfolios, dict):\n",
        "        raise TypeError(\"Input 'portfolios' must be a dictionary.\")\n",
        "    if not isinstance(proximity_matrix, np.ndarray):\n",
        "        raise TypeError(\"Input 'proximity_matrix' must be a numpy array.\")\n",
        "\n",
        "    logging.info(\"Starting computation of STRAPSim scores for all portfolio pairs...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Step 1: Initialization ---\n",
        "    # Get a sorted list of ETF IDs to ensure deterministic matrix order.\n",
        "    etf_ids: List[str] = sorted(portfolios.keys())\n",
        "    n_etfs = len(etf_ids)\n",
        "\n",
        "    # Create a mapping from ETF ID to its integer index in the matrices.\n",
        "    etf_to_matrix_idx: Dict[str, int] = {etf_id: i for i, etf_id in enumerate(etf_ids)}\n",
        "\n",
        "    # Initialize empty NumPy arrays to store the results.\n",
        "    # Using np.nan helps identify any pairs that were missed by the loop.\n",
        "    similarity_matrix = np.full((n_etfs, n_etfs), np.nan, dtype=np.float64)\n",
        "    residual_matrix = np.full((n_etfs, n_etfs), np.nan, dtype=np.float64)\n",
        "\n",
        "    # --- Step 2: Diagonal Population ---\n",
        "    # A portfolio is perfectly similar to itself with zero residual.\n",
        "    np.fill_diagonal(similarity_matrix, 1.0)\n",
        "    np.fill_diagonal(residual_matrix, 0.0)\n",
        "\n",
        "    # --- Step 3: Pairwise Computation ---\n",
        "    # Generate all unique pairs of ETF IDs to compute.\n",
        "    etf_pairs = list(itertools.combinations(etf_ids, 2))\n",
        "\n",
        "    logging.info(f\"Calculating STRAPSim for {len(etf_pairs)} unique ETF pairs...\")\n",
        "\n",
        "    # Use tqdm to create a progress bar for the loop.\n",
        "    for etf_id_x, etf_id_y in tqdm(etf_pairs, desc=\"Computing STRAPSim Pairs\"):\n",
        "        # Retrieve the structured data for the two portfolios.\n",
        "        portfolio_x = portfolios[etf_id_x]\n",
        "        portfolio_y = portfolios[etf_id_y]\n",
        "\n",
        "        # Call the core algorithm to compute the similarity.\n",
        "        score, residual_x, residual_y = compute_strapsim_between_pair(\n",
        "            portfolio_x, portfolio_y, proximity_matrix, config\n",
        "        )\n",
        "\n",
        "        # Get the integer indices for the matrix.\n",
        "        idx_x = etf_to_matrix_idx[etf_id_x]\n",
        "        idx_y = etf_to_matrix_idx[etf_id_y]\n",
        "\n",
        "        # --- Step 4: Symmetric Storage ---\n",
        "        # Store the score in both (i, j) and (j, i) positions.\n",
        "        similarity_matrix[idx_x, idx_y] = score\n",
        "        similarity_matrix[idx_y, idx_x] = score\n",
        "\n",
        "        # Store the total residual weight.\n",
        "        total_residual = residual_x + residual_y\n",
        "        residual_matrix[idx_x, idx_y] = total_residual\n",
        "        residual_matrix[idx_y, idx_x] = total_residual\n",
        "\n",
        "    # --- Final Validation ---\n",
        "    if np.isnan(similarity_matrix).any() or np.isnan(residual_matrix).any():\n",
        "        raise RuntimeError(\"Calculation failed: Not all pairs were computed.\")\n",
        "\n",
        "    # --- Step 5: Finalization ---\n",
        "    # Convert the NumPy arrays to labeled pandas DataFrames.\n",
        "    similarity_df = pd.DataFrame(\n",
        "        similarity_matrix, index=etf_ids, columns=etf_ids\n",
        "    )\n",
        "    residual_df = pd.DataFrame(\n",
        "        residual_matrix, index=etf_ids, columns=etf_ids\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    logging.info(f\"All-pairs STRAPSim computation completed in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    return similarity_df, residual_df\n"
      ],
      "metadata": {
        "id": "hb1QMt2OBQ3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18: Compute Jaccard Index Similarity\n",
        "\n",
        "# Define type hints from previous tasks for clarity.\n",
        "Portfolio = Dict[str, Union[List[str], np.ndarray]]\n",
        "PortfolioData = Dict[str, Portfolio]\n",
        "\n",
        "\n",
        "def compute_jaccard_matrix(\n",
        "    portfolios: PortfolioData,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Computes the Jaccard Index similarity for all unique portfolio pairs.\n",
        "\n",
        "    This function implements the standard Jaccard Index, a baseline similarity\n",
        "    metric defined in Equation (3) of the paper. It measures similarity as the\n",
        "    ratio of the size of the intersection to the size of the union of the two\n",
        "    sets of constituents. This metric ignores both portfolio weights and any\n",
        "    notion of similarity between non-identical constituents.\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  **Initialization**: Creates empty matrices for Jaccard scores and their\n",
        "        residuals. Establishes a deterministic mapping from ETF IDs to matrix indices.\n",
        "    2.  **Pairwise Computation**: Iterates through all unique pairs of ETFs. For\n",
        "        each pair:\n",
        "        a. Converts the lists of constituent CUSIPs into sets.\n",
        "        b. Uses efficient set operations to find the intersection and union.\n",
        "        c. Calculates the Jaccard Index: |Intersection| / |Union|.\n",
        "    3.  **Symmetric Storage**: Stores the result for each pair `(i, j)` in both\n",
        "        `matrix[i, j]` and `matrix[j, i]`.\n",
        "    4.  **Finalization**: Converts the completed NumPy arrays into well-labeled\n",
        "        pandas DataFrames.\n",
        "\n",
        "    Args:\n",
        "        portfolios (PortfolioData): The nested dictionary of structured portfolio data.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing:\n",
        "        - A square DataFrame of pairwise Jaccard Index similarity scores.\n",
        "        - A square DataFrame of the corresponding Jaccard residuals (1 - score).\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(portfolios, dict):\n",
        "        raise TypeError(\"Input 'portfolios' must be a dictionary.\")\n",
        "\n",
        "    logging.info(\"Starting computation of Jaccard Index for all portfolio pairs...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Step 1: Initialization ---\n",
        "    # Get a sorted list of ETF IDs for deterministic matrix order.\n",
        "    etf_ids: List[str] = sorted(portfolios.keys())\n",
        "    n_etfs = len(etf_ids)\n",
        "\n",
        "    # Create a mapping from ETF ID to its integer index in the matrices.\n",
        "    etf_to_matrix_idx: Dict[str, int] = {etf_id: i for i, etf_id in enumerate(etf_ids)}\n",
        "\n",
        "    # Initialize empty NumPy arrays for the results.\n",
        "    jaccard_matrix = np.full((n_etfs, n_etfs), np.nan, dtype=np.float64)\n",
        "    residual_matrix = np.full((n_etfs, n_etfs), np.nan, dtype=np.float64)\n",
        "\n",
        "    # Set the diagonal to 1.0 for similarity and 0.0 for residual.\n",
        "    np.fill_diagonal(jaccard_matrix, 1.0)\n",
        "    np.fill_diagonal(residual_matrix, 0.0)\n",
        "\n",
        "    # --- Step 2: Pairwise Computation ---\n",
        "    # Generate all unique pairs of ETF IDs.\n",
        "    etf_pairs = list(itertools.combinations(etf_ids, 2))\n",
        "\n",
        "    logging.info(f\"Calculating Jaccard Index for {len(etf_pairs)} unique ETF pairs...\")\n",
        "\n",
        "    # Use tqdm for a progress bar.\n",
        "    for etf_id_x, etf_id_y in tqdm(etf_pairs, desc=\"Computing Jaccard Pairs\"):\n",
        "        # Retrieve the constituent CUSIP lists for the two portfolios.\n",
        "        cusips_x: Set[str] = set(portfolios[etf_id_x]['cusips'])\n",
        "        cusips_y: Set[str] = set(portfolios[etf_id_y]['cusips'])\n",
        "\n",
        "        # Use Python's highly optimized set operations.\n",
        "        # Intersection: elements common to both sets.\n",
        "        intersection_size = len(cusips_x.intersection(cusips_y))\n",
        "        # Union: all unique elements from both sets.\n",
        "        union_size = len(cusips_x.union(cusips_y))\n",
        "\n",
        "        # Calculate the Jaccard Index.\n",
        "        # J(X, Y) = |X intersect Y| / |X union Y|\n",
        "        if union_size == 0:\n",
        "            # Edge case: If both sets are empty, their union is empty.\n",
        "            # They are identical, so their similarity is 1.0.\n",
        "            jaccard_score = 1.0\n",
        "        else:\n",
        "            jaccard_score = intersection_size / union_size\n",
        "\n",
        "        # The residual is simply 1 minus the score.\n",
        "        residual = 1.0 - jaccard_score\n",
        "\n",
        "        # Get the integer indices for matrix storage.\n",
        "        idx_x = etf_to_matrix_idx[etf_id_x]\n",
        "        idx_y = etf_to_matrix_idx[etf_id_y]\n",
        "\n",
        "        # --- Step 3: Symmetric Storage ---\n",
        "        # Populate the matrices symmetrically.\n",
        "        jaccard_matrix[idx_x, idx_y] = jaccard_score\n",
        "        jaccard_matrix[idx_y, idx_x] = jaccard_score\n",
        "        residual_matrix[idx_x, idx_y] = residual\n",
        "        residual_matrix[idx_y, idx_x] = residual\n",
        "\n",
        "    # --- Final Validation ---\n",
        "    if np.isnan(jaccard_matrix).any():\n",
        "        raise RuntimeError(\"Jaccard matrix calculation failed: Not all pairs were computed.\")\n",
        "\n",
        "    # --- Step 4: Finalization ---\n",
        "    # Convert the NumPy arrays to labeled pandas DataFrames.\n",
        "    jaccard_df = pd.DataFrame(\n",
        "        jaccard_matrix, index=etf_ids, columns=etf_ids\n",
        "    )\n",
        "    residual_df = pd.DataFrame(\n",
        "        residual_matrix, index=etf_ids, columns=etf_ids\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    logging.info(f\"All-pairs Jaccard Index computation completed in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    return jaccard_df, residual_df\n"
      ],
      "metadata": {
        "id": "Y_HyNOXACb8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19: Compute Weighted Jaccard Similarity\n",
        "\n",
        "# Define type hints from previous tasks for clarity.\n",
        "Portfolio = Dict[str, Union[List[str], np.ndarray]]\n",
        "PortfolioData = Dict[str, Portfolio]\n",
        "\n",
        "\n",
        "def compute_weighted_jaccard_matrix(\n",
        "    portfolios: PortfolioData,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Computes the Weighted Jaccard similarity for all unique portfolio pairs.\n",
        "\n",
        "    This function implements the Weighted Jaccard Index, a baseline similarity\n",
        "    metric defined in Equation (4) of the paper. It extends the standard Jaccard\n",
        "    Index by incorporating the weights of the constituents.\n",
        "\n",
        "    The formula is:\n",
        "    J_w(X, Y) = sum(min(w_x(k), w_y(k))) for k in intersection(X, Y) /\n",
        "                sum(max(w_x(k), w_y(k))) for k in union(X, Y)\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  **Initialization**: Creates empty matrices for scores and residuals.\n",
        "    2.  **Pairwise Computation**: For each unique pair of ETFs:\n",
        "        a. Creates efficient CUSIP-to-weight lookup dictionaries.\n",
        "        b. Determines the intersection and union of the constituent sets.\n",
        "        c. Calculates the weighted intersection (numerator) by summing the\n",
        "           minimum weight for each common constituent.\n",
        "        d. Calculates the weighted union (denominator) by summing the\n",
        "           maximum weight for each constituent present in either portfolio.\n",
        "        e. Computes the final score as the ratio of the two sums.\n",
        "    3.  **Symmetric Storage and Finalization**: Stores results symmetrically and\n",
        "        converts the final matrices into labeled pandas DataFrames.\n",
        "\n",
        "    Args:\n",
        "        portfolios (PortfolioData): The nested dictionary of structured portfolio data.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing:\n",
        "        - A square DataFrame of pairwise Weighted Jaccard similarity scores.\n",
        "        - A square DataFrame of the corresponding residuals (1 - score).\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(portfolios, dict):\n",
        "        raise TypeError(\"Input 'portfolios' must be a dictionary.\")\n",
        "\n",
        "    logging.info(\"Starting computation of Weighted Jaccard Index for all portfolio pairs...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Step 1: Initialization ---\n",
        "    etf_ids: List[str] = sorted(portfolios.keys())\n",
        "    n_etfs = len(etf_ids)\n",
        "    etf_to_matrix_idx: Dict[str, int] = {etf_id: i for i, etf_id in enumerate(etf_ids)}\n",
        "\n",
        "    wj_matrix = np.full((n_etfs, n_etfs), np.nan, dtype=np.float64)\n",
        "    residual_matrix = np.full((n_etfs, n_etfs), np.nan, dtype=np.float64)\n",
        "\n",
        "    np.fill_diagonal(wj_matrix, 1.0)\n",
        "    np.fill_diagonal(residual_matrix, 0.0)\n",
        "\n",
        "    # --- Step 2: Pairwise Computation ---\n",
        "    etf_pairs = list(itertools.combinations(etf_ids, 2))\n",
        "\n",
        "    logging.info(f\"Calculating Weighted Jaccard for {len(etf_pairs)} unique ETF pairs...\")\n",
        "\n",
        "    for etf_id_x, etf_id_y in tqdm(etf_pairs, desc=\"Computing Weighted Jaccard Pairs\"):\n",
        "        # Create efficient CUSIP-to-weight lookup dictionaries for the pair.\n",
        "        weights_x: Dict[str, float] = dict(zip(portfolios[etf_id_x]['cusips'], portfolios[etf_id_x]['weights']))\n",
        "        weights_y: Dict[str, float] = dict(zip(portfolios[etf_id_y]['cusips'], portfolios[etf_id_y]['weights']))\n",
        "\n",
        "        # Determine the intersection and union of the constituent CUSIPs.\n",
        "        cusips_x: Set[str] = set(weights_x.keys())\n",
        "        cusips_y: Set[str] = set(weights_y.keys())\n",
        "        intersection_cusips = cusips_x.intersection(cusips_y)\n",
        "        union_cusips = cusips_x.union(cusips_y)\n",
        "\n",
        "        # Calculate the numerator: the sum of minimum weights over the intersection.\n",
        "        # This is the weighted size of the intersection.\n",
        "        weighted_intersection = sum(min(weights_x[c], weights_y[c]) for c in intersection_cusips)\n",
        "\n",
        "        # Calculate the denominator: the sum of maximum weights over the union.\n",
        "        # This is the weighted size of the union. The .get(c, 0.0) method\n",
        "        # elegantly handles cases where a CUSIP exists in only one portfolio.\n",
        "        weighted_union = sum(max(weights_x.get(c, 0.0), weights_y.get(c, 0.0)) for c in union_cusips)\n",
        "\n",
        "        # Calculate the Weighted Jaccard score.\n",
        "        if weighted_union == 0:\n",
        "            # Edge case: If both portfolios are empty, their similarity is 1.0.\n",
        "            wj_score = 1.0\n",
        "        else:\n",
        "            wj_score = weighted_intersection / weighted_union\n",
        "\n",
        "        residual = 1.0 - wj_score\n",
        "\n",
        "        # --- Step 3: Symmetric Storage ---\n",
        "        idx_x = etf_to_matrix_idx[etf_id_x]\n",
        "        idx_y = etf_to_matrix_idx[etf_id_y]\n",
        "        wj_matrix[idx_x, idx_y] = wj_score\n",
        "        wj_matrix[idx_y, idx_x] = wj_score\n",
        "        residual_matrix[idx_x, idx_y] = residual\n",
        "        residual_matrix[idx_y, idx_x] = residual\n",
        "\n",
        "    # --- Final Validation ---\n",
        "    if np.isnan(wj_matrix).any():\n",
        "        raise RuntimeError(\"Weighted Jaccard matrix calculation failed: Not all pairs were computed.\")\n",
        "\n",
        "    # --- Step 4: Finalization ---\n",
        "    wj_df = pd.DataFrame(\n",
        "        wj_matrix, index=etf_ids, columns=etf_ids\n",
        "    )\n",
        "    residual_df = pd.DataFrame(\n",
        "        residual_matrix, index=etf_ids, columns=etf_ids\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    logging.info(f\"All-pairs Weighted Jaccard computation completed in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    return wj_df, residual_df\n"
      ],
      "metadata": {
        "id": "1u0GU3h9DZsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20: Compute Adapted BERTScore Similarity\n",
        "\n",
        "# Define type hints from previous tasks for clarity.\n",
        "Portfolio = Dict[str, Union[List[str], np.ndarray]]\n",
        "PortfolioData = Dict[str, Portfolio]\n",
        "BERTScoreFullResult = Tuple[float, float, float, float, float, float]\n",
        "\n",
        "\n",
        "def _compute_bertscore_for_pair(\n",
        "    portfolio_x: Portfolio,\n",
        "    portfolio_y: Portfolio,\n",
        "    proximity_matrix: np.ndarray\n",
        ") -> BERTScoreFullResult:\n",
        "    \"\"\"\n",
        "    Computes the adapted BERTScore and its residuals for a single pair.\n",
        "\n",
        "    This remediated function provides a complete implementation of the adapted\n",
        "    BERTScore metric as described in Section 2.4 of the paper, including both\n",
        "    the primary scores (Equations 6a, 6b, 6c) and the residual scores\n",
        "    (Equations 7a, 7b, 7c).\n",
        "\n",
        "    Primary Scores (Vectorized Calculation):\n",
        "    - Recall (X -> Y): Weighted average of the best-match similarity for each item in X.\n",
        "    - Precision (Y -> X): Weighted average of the best-match similarity for each item in Y.\n",
        "    - F1 Score: The harmonic mean of Precision and Recall.\n",
        "\n",
        "    Residual Scores (Iterative Calculation):\n",
        "    - Recall Residual: Calculates the total weight remaining in X after each of its\n",
        "      constituents is matched once to its best partner in Y.\n",
        "    - Precision Residual: The symmetric calculation for portfolio Y.\n",
        "    - F1 Residual: The harmonic mean of the two residual scores.\n",
        "\n",
        "    Args:\n",
        "        portfolio_x (Portfolio): The structured data for the reference portfolio (X).\n",
        "        portfolio_y (Portfolio): The structured data for the candidate portfolio (Y).\n",
        "        proximity_matrix (np.ndarray): The global (N x N) matrix of similarities.\n",
        "\n",
        "    Returns:\n",
        "        BERTScoreFullResult: A tuple containing six scores:\n",
        "        (Recall, Precision, F1, Recall_Residual, Precision_Residual, F1_Residual).\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    indices_x = portfolio_x['indices']\n",
        "    indices_y = portfolio_y['indices']\n",
        "    weights_x = portfolio_x['weights']\n",
        "    weights_y = portfolio_y['weights']\n",
        "\n",
        "    # Handle edge case of empty portfolios.\n",
        "    if len(indices_x) == 0 or len(indices_y) == 0:\n",
        "        is_identical = len(indices_x) == len(indices_y)\n",
        "        score = 1.0 if is_identical else 0.0\n",
        "        residual = 0.0 if is_identical else 1.0\n",
        "        return (score, score, score, residual, residual, residual)\n",
        "\n",
        "    # Slice the proximity matrix to get the relevant sub-matrix.\n",
        "    sub_matrix = proximity_matrix[np.ix_(indices_x, indices_y)]\n",
        "\n",
        "    # --- Primary Score Calculation (Vectorized) ---\n",
        "    # Recall (Equation 6a): Weighted average of max similarities for each item in X.\n",
        "    max_sim_for_x = sub_matrix.max(axis=1)\n",
        "    recall = np.dot(weights_x, max_sim_for_x)\n",
        "\n",
        "    # Precision (Equation 6b): Weighted average of max similarities for each item in Y.\n",
        "    max_sim_for_y = sub_matrix.max(axis=0)\n",
        "    precision = np.dot(weights_y, max_sim_for_y)\n",
        "\n",
        "    # F1 Score (Equation 6c): Harmonic mean of Precision and Recall.\n",
        "    if (precision + recall) == 0:\n",
        "        f1_score = 0.0\n",
        "    else:\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    # --- Residual Score Calculation (Iterative) ---\n",
        "    # Recall Residual (Equation 7a)\n",
        "    residual_weights_x = weights_x.copy()\n",
        "    best_match_indices_for_x = np.argmax(sub_matrix, axis=1)\n",
        "    for i in range(len(indices_x)):\n",
        "        best_match_j = best_match_indices_for_x[i]\n",
        "        # Per the paper's formula, the weight consumed is min(w_x, w_y_match).\n",
        "        transfer_weight = min(weights_x[i], weights_y[best_match_j])\n",
        "        residual_weights_x[i] -= transfer_weight\n",
        "    # The residual score is 1 - (total remaining weight / total initial weight).\n",
        "    # Since total initial weight is 1.0, this simplifies to 1 - sum(residuals).\n",
        "    recall_residual = 1.0 - np.sum(np.maximum(0, residual_weights_x))\n",
        "\n",
        "    # Precision Residual (Equation 7b)\n",
        "    residual_weights_y = weights_y.copy()\n",
        "    best_match_indices_for_y = np.argmax(sub_matrix, axis=0)\n",
        "    for j in range(len(indices_y)):\n",
        "        best_match_i = best_match_indices_for_y[j]\n",
        "        transfer_weight = min(weights_y[j], weights_x[best_match_i])\n",
        "        residual_weights_y[j] -= transfer_weight\n",
        "    precision_residual = 1.0 - np.sum(np.maximum(0, residual_weights_y))\n",
        "\n",
        "    # F1 Residual (Equation 7c)\n",
        "    if (precision_residual + recall_residual) == 0:\n",
        "        f1_residual = 0.0\n",
        "    else:\n",
        "        f1_residual = 2 * (precision_residual * recall_residual) / (precision_residual + recall_residual)\n",
        "\n",
        "    return recall, precision, f1_score, recall_residual, precision_residual, f1_residual\n",
        "\n",
        "\n",
        "\n",
        "def compute_bertscore_matrix(\n",
        "    portfolios: PortfolioData,\n",
        "    proximity_matrix: np.ndarray,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of adapted BERTScore and residuals for all pairs.\n",
        "\n",
        "    This remediated function systematically calculates the full suite of six\n",
        "    BERTScore-related metrics (Recall, Precision, F1, and their corresponding\n",
        "    residuals) for every unique pair of ETFs. It serves as the main entry point\n",
        "    for generating all BERTScore baseline similarity and residual matrices.\n",
        "\n",
        "    Args:\n",
        "        portfolios (PortfolioData): The nested dictionary of structured portfolio data.\n",
        "        proximity_matrix (np.ndarray): The global (N x N) matrix of similarities.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, ...]: A tuple containing the six square DataFrames:\n",
        "        (Recall_Scores, Precision_Scores, F1_Scores, Recall_Residuals,\n",
        "         Precision_Residuals, F1_Residuals).\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(portfolios, dict):\n",
        "        raise TypeError(\"Input 'portfolios' must be a dictionary.\")\n",
        "    if not isinstance(proximity_matrix, np.ndarray):\n",
        "        raise TypeError(\"Input 'proximity_matrix' must be a numpy array.\")\n",
        "\n",
        "    logging.info(\"Starting computation of adapted BERTScore and residuals for all portfolio pairs...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Initialization ---\n",
        "    etf_ids: List[str] = sorted(portfolios.keys())\n",
        "    n_etfs = len(etf_ids)\n",
        "    etf_to_matrix_idx: Dict[str, int] = {etf_id: i for i, etf_id in enumerate(etf_ids)}\n",
        "\n",
        "    # Initialize six empty matrices for all scores and residuals.\n",
        "    matrices = {\n",
        "        \"recall\": np.full((n_etfs, n_etfs), np.nan, dtype=np.float64),\n",
        "        \"precision\": np.full((n_etfs, n_etfs), np.nan, dtype=np.float64),\n",
        "        \"f1\": np.full((n_etfs, n_etfs), np.nan, dtype=np.float64),\n",
        "        \"recall_residual\": np.full((n_etfs, n_etfs), np.nan, dtype=np.float64),\n",
        "        \"precision_residual\": np.full((n_etfs, n_etfs), np.nan, dtype=np.float64),\n",
        "        \"f1_residual\": np.full((n_etfs, n_etfs), np.nan, dtype=np.float64),\n",
        "    }\n",
        "\n",
        "    # Set diagonals: 1.0 for scores, 0.0 for residuals.\n",
        "    for name, matrix in matrices.items():\n",
        "        fill_value = 1.0 if \"residual\" not in name else 0.0\n",
        "        np.fill_diagonal(matrix, fill_value)\n",
        "\n",
        "    # --- Pairwise Computation ---\n",
        "    etf_pairs = list(itertools.combinations(etf_ids, 2))\n",
        "    logging.info(f\"Calculating BERTScore suite for {len(etf_pairs)} unique ETF pairs...\")\n",
        "\n",
        "    for etf_id_x, etf_id_y in tqdm(etf_pairs, desc=\"Computing BERTScore Suite\"):\n",
        "        portfolio_x = portfolios[etf_id_x]\n",
        "        portfolio_y = portfolios[etf_id_y]\n",
        "\n",
        "        # Unpack all six results from the remediated helper function.\n",
        "        r_xy, p_xy, f1_xy, rr_xy, pr_xy, fr_xy = _compute_bertscore_for_pair(\n",
        "            portfolio_x, portfolio_y, proximity_matrix\n",
        "        )\n",
        "\n",
        "        # Compute in the reverse direction to get asymmetric scores.\n",
        "        r_yx, p_yx, f1_yx, rr_yx, pr_yx, fr_yx = _compute_bertscore_for_pair(\n",
        "            portfolio_y, portfolio_x, proximity_matrix\n",
        "        )\n",
        "\n",
        "        # Get matrix indices.\n",
        "        idx_x = etf_to_matrix_idx[etf_id_x]\n",
        "        idx_y = etf_to_matrix_idx[etf_id_y]\n",
        "\n",
        "        # Store directional scores (Recall and Precision + their residuals).\n",
        "        matrices[\"recall\"][idx_x, idx_y], matrices[\"recall\"][idx_y, idx_x] = r_xy, r_yx\n",
        "        matrices[\"precision\"][idx_x, idx_y], matrices[\"precision\"][idx_y, idx_x] = p_xy, p_yx\n",
        "        matrices[\"recall_residual\"][idx_x, idx_y], matrices[\"recall_residual\"][idx_y, idx_x] = rr_xy, rr_yx\n",
        "        matrices[\"precision_residual\"][idx_x, idx_y], matrices[\"precision_residual\"][idx_y, idx_x] = pr_xy, pr_yx\n",
        "\n",
        "        # Store symmetric scores (F1 and its residual). Average for robustness.\n",
        "        f1_avg = (f1_xy + f1_yx) / 2.0\n",
        "        matrices[\"f1\"][idx_x, idx_y] = matrices[\"f1\"][idx_y, idx_x] = f1_avg\n",
        "        fr_avg = (fr_xy + fr_yx) / 2.0\n",
        "        matrices[\"f1_residual\"][idx_x, idx_y] = matrices[\"f1_residual\"][idx_y, idx_x] = fr_avg\n",
        "\n",
        "    # --- Finalization ---\n",
        "    # Convert all matrices to labeled DataFrames.\n",
        "    dataframes = {\n",
        "        name: pd.DataFrame(matrix, index=etf_ids, columns=etf_ids)\n",
        "        for name, matrix in matrices.items()\n",
        "    }\n",
        "\n",
        "    end_time = time.time()\n",
        "    logging.info(f\"All-pairs BERTScore suite computation completed in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    return (\n",
        "        dataframes[\"recall\"],\n",
        "        dataframes[\"precision\"],\n",
        "        dataframes[\"f1\"],\n",
        "        dataframes[\"recall_residual\"],\n",
        "        dataframes[\"precision_residual\"],\n",
        "        dataframes[\"f1_residual\"],\n",
        "    )\n"
      ],
      "metadata": {
        "id": "UmClyxO6EtQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21: Compute Return Correlation Matrix\n",
        "\n",
        "def compute_return_correlation_matrix(\n",
        "    monthly_returns_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the pairwise return correlation matrix for all ETFs.\n",
        "\n",
        "    This function calculates the Pearson correlation coefficient for every pair of\n",
        "    ETF return series in the input DataFrame. This resulting matrix serves as the\n",
        "    \"ground truth\" or \"calibration benchmark\" for economic similarity, against\n",
        "    which all model-based similarity metrics will be evaluated, as described in\n",
        "    Section 4.3 of the paper.\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  **Input Validation**: Ensures the input is a properly indexed time-series\n",
        "        DataFrame with no missing values.\n",
        "    2.  **Correlation Calculation**: Leverages the highly optimized `.corr()` method\n",
        "        of pandas DataFrames to compute the entire pairwise correlation matrix in a\n",
        "        single operation.\n",
        "    3.  **Output Validation**: Performs a series of rigorous checks on the resulting\n",
        "        matrix to ensure its mathematical properties are correct (e.g., symmetry,\n",
        "        diagonal of ones, values in [-1, 1]).\n",
        "\n",
        "    Args:\n",
        "        monthly_returns_df (pd.DataFrame): The cleansed, date-indexed DataFrame\n",
        "                                           of monthly ETF returns.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A square, symmetric DataFrame containing the pairwise\n",
        "                      Pearson correlation coefficients.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input is not a pandas DataFrame.\n",
        "        ValueError: If the input DataFrame has issues like a non-datetime index,\n",
        "                    missing values, or if the resulting correlation matrix fails\n",
        "                    validation checks.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Input Validation ---\n",
        "    if not isinstance(monthly_returns_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'monthly_returns_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(monthly_returns_df.index, pd.DatetimeIndex):\n",
        "        raise ValueError(\"Input DataFrame must have a DatetimeIndex.\")\n",
        "    if monthly_returns_df.isnull().values.any():\n",
        "        # This check is critical as correlation is sensitive to missing data.\n",
        "        raise ValueError(\"Input DataFrame contains missing values.\")\n",
        "\n",
        "    logging.info(\"Computing pairwise return correlation matrix...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Step 2: Correlation Calculation ---\n",
        "    # Retrieve the specified correlation method from the configuration.\n",
        "    method = config['evaluation_config']['return_correlation_method']\n",
        "    if method != 'pearson':\n",
        "        logging.warning(f\"Configuration specifies correlation method '{method}', not 'pearson'.\")\n",
        "\n",
        "    # Use the built-in .corr() method, which is highly optimized and numerically stable.\n",
        "    # It computes the correlation between all pairs of columns.\n",
        "    correlation_matrix = monthly_returns_df.corr(method=method)\n",
        "\n",
        "    # --- Step 3: Output Validation ---\n",
        "    # Check for any NaN values, which can occur if a column has zero variance.\n",
        "    if correlation_matrix.isnull().values.any():\n",
        "        nan_cols = correlation_matrix.columns[correlation_matrix.isnull().any()].tolist()\n",
        "        raise ValueError(\n",
        "            f\"Correlation matrix contains NaN values. This may be caused by \"\n",
        "            f\"zero-variance return series in columns: {nan_cols}\"\n",
        "        )\n",
        "\n",
        "    # Check 1: The matrix must be square.\n",
        "    if correlation_matrix.shape[0] != correlation_matrix.shape[1]:\n",
        "        raise ValueError(\"Resulting correlation matrix is not square.\")\n",
        "\n",
        "    # Check 2: The matrix must be symmetric.\n",
        "    if not np.allclose(correlation_matrix.values, correlation_matrix.values.T, atol=1e-9):\n",
        "        raise ValueError(\"Resulting correlation matrix is not symmetric.\")\n",
        "\n",
        "    # Check 3: The diagonal elements must all be 1.0.\n",
        "    if not np.allclose(np.diag(correlation_matrix.values), 1.0, atol=1e-9):\n",
        "        raise ValueError(\"The diagonal of the correlation matrix is not all ones.\")\n",
        "\n",
        "    # Check 4: All values must be within the valid range [-1, 1].\n",
        "    if not ((correlation_matrix.values >= -1.0 - 1e-9) & (correlation_matrix.values <= 1.0 + 1e-9)).all():\n",
        "        raise ValueError(\"Correlation matrix contains values outside the [-1, 1] range.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    logging.info(f\"Return correlation matrix computation completed in {end_time - start_time:.2f} seconds.\")\n",
        "    logging.info(f\"Final matrix shape: {correlation_matrix.shape}\")\n",
        "\n",
        "    return correlation_matrix\n"
      ],
      "metadata": {
        "id": "VuDJRHLaMRIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22: Prepare Data for Spearman Correlation Analysis\n",
        "\n",
        "def prepare_data_for_spearman_analysis(\n",
        "    similarity_matrices: Dict[str, pd.DataFrame],\n",
        "    correlation_matrix: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepares and ranks data for Spearman correlation analysis.\n",
        "\n",
        "    This function transforms the wide-format similarity and correlation matrices\n",
        "    into a single, long-format (\"tidy\") DataFrame. It then calculates the ranks\n",
        "    of similarity scores and correlation values for each reference ETF, which is\n",
        "    the necessary input for the Spearman rank correlation test.\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  **Unpivot Matrices**: Each similarity matrix and the correlation matrix are\n",
        "        transformed from a square (N x N) format into a long format with columns\n",
        "        for the reference ETF, comparison ETF, and the corresponding value.\n",
        "        Self-comparisons are excluded.\n",
        "    2.  **Combine Data**: The unpivoted data from all similarity methods are\n",
        "        concatenated into a single DataFrame.\n",
        "    3.  **Merge Ground Truth**: This combined DataFrame is merged with the unpivoted\n",
        "        correlation data, aligning the ground-truth correlation value with each\n",
        "        pair's similarity score.\n",
        "    4.  **Calculate Ranks**: For each reference ETF and each similarity method, the\n",
        "        function calculates the descending rank of both the similarity scores and\n",
        "        the correlation values. Ties are handled by assigning the average rank.\n",
        "\n",
        "    Args:\n",
        "        similarity_matrices (Dict[str, pd.DataFrame]): A dictionary where keys are\n",
        "            the names of similarity methods (e.g., 'STRAPSim') and values are\n",
        "            the corresponding N x N similarity matrices.\n",
        "        correlation_matrix (pd.DataFrame): The N x N matrix of pairwise return\n",
        "                                            correlations (the ground truth).\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A tidy DataFrame containing the original values and the\n",
        "                      calculated ranks, ready for statistical testing.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(similarity_matrices, dict) or not all(isinstance(df, pd.DataFrame) for df in similarity_matrices.values()):\n",
        "        raise TypeError(\"Input 'similarity_matrices' must be a dictionary of pandas DataFrames.\")\n",
        "    if not isinstance(correlation_matrix, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'correlation_matrix' must be a pandas DataFrame.\")\n",
        "\n",
        "    logging.info(\"Preparing and ranking data for Spearman correlation analysis...\")\n",
        "\n",
        "    # --- Step 1: Unpivot All Matrices ---\n",
        "    all_long_dfs: List[pd.DataFrame] = []\n",
        "\n",
        "    # Unpivot the ground-truth correlation matrix first.\n",
        "    # .stack() is an efficient way to unpivot, creating a Series.\n",
        "    corr_long = correlation_matrix.stack().reset_index()\n",
        "    corr_long.columns = ['reference_etf', 'comparison_etf', 'correlation_value']\n",
        "\n",
        "    # Exclude self-comparisons.\n",
        "    corr_long = corr_long[corr_long['reference_etf'] != corr_long['comparison_etf']]\n",
        "\n",
        "    # Unpivot each similarity matrix.\n",
        "    for method_name, sim_matrix in similarity_matrices.items():\n",
        "        sim_long = sim_matrix.stack().reset_index()\n",
        "        sim_long.columns = ['reference_etf', 'comparison_etf', 'similarity_score']\n",
        "        sim_long['method'] = method_name\n",
        "        all_long_dfs.append(sim_long)\n",
        "\n",
        "    # --- Step 2: Combine Similarity Data ---\n",
        "    # Concatenate all the long-format similarity DataFrames.\n",
        "    combined_sim_df = pd.concat(all_long_dfs, ignore_index=True)\n",
        "\n",
        "    # Exclude self-comparisons from the similarity data as well.\n",
        "    combined_sim_df = combined_sim_df[combined_sim_df['reference_etf'] != combined_sim_df['comparison_etf']]\n",
        "\n",
        "    # --- Step 3: Merge Ground Truth ---\n",
        "    # Merge the similarity data with the correlation data on the ETF pair keys.\n",
        "    # This aligns the similarity score from each method with the ground-truth correlation.\n",
        "    analysis_df = pd.merge(\n",
        "        combined_sim_df,\n",
        "        corr_long,\n",
        "        on=['reference_etf', 'comparison_etf']\n",
        "    )\n",
        "\n",
        "    # --- Step 4: Calculate Ranks ---\n",
        "    # Retrieve ranking parameters from the configuration.\n",
        "    ranking_method = config['evaluation_config']['handle_ranking_ties']\n",
        "    is_descending = config['evaluation_config']['ranking_method'] == 'descending'\n",
        "\n",
        "    # Calculate ranks within each group of (reference_etf, method).\n",
        "    # .groupby() partitions the data for each reference ETF's ranking task.\n",
        "    # .rank() is then applied to each partition.\n",
        "    # 'ascending=False' means a higher score/correlation gets a better (lower) rank.\n",
        "    analysis_df['similarity_rank'] = analysis_df.groupby(['reference_etf', 'method'])['similarity_score'].rank(\n",
        "        method=ranking_method, ascending=not is_descending\n",
        "    )\n",
        "\n",
        "    # The correlation ranks need to be calculated for each reference ETF as well.\n",
        "    # We group only by 'reference_etf' here, as the correlation is the same for all methods.\n",
        "    analysis_df['correlation_rank'] = analysis_df.groupby('reference_etf')['correlation_value'].rank(\n",
        "        method=ranking_method, ascending=not is_descending\n",
        "    )\n",
        "\n",
        "    logging.info(\"Data preparation and ranking complete.\")\n",
        "    logging.info(f\"Created tidy analysis DataFrame with shape: {analysis_df.shape}\")\n",
        "\n",
        "    # Final validation of the output.\n",
        "    expected_rows = len(similarity_matrices) * (len(correlation_matrix) * (len(correlation_matrix) - 1))\n",
        "    if analysis_df.shape[0] != expected_rows:\n",
        "        raise ValueError(\"The final analysis DataFrame has an unexpected number of rows.\")\n",
        "    if analysis_df.isnull().values.any():\n",
        "        raise ValueError(\"The final analysis DataFrame contains unexpected null values.\")\n",
        "\n",
        "    return analysis_df\n"
      ],
      "metadata": {
        "id": "q2HvrKS8Nddn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 23: Compute Spearman Rank Correlation\n",
        "\n",
        "def _calculate_spearman_for_group(\n",
        "    group: pd.DataFrame\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Helper function to calculate Spearman correlation for a single group.\n",
        "\n",
        "    This function is designed to be used with `groupby().apply()`. It takes a\n",
        "    sub-DataFrame corresponding to one reference ETF and one similarity method,\n",
        "    and computes the Spearman rank correlation and its p-value between the\n",
        "    similarity ranks and the ground-truth correlation ranks.\n",
        "\n",
        "    Args:\n",
        "        group (pd.DataFrame): A sub-DataFrame containing 'similarity_rank' and\n",
        "                              'correlation_rank' columns for a single test case.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Series containing the calculated 'correlation' and 'p_value'.\n",
        "    \"\"\"\n",
        "    # Use scipy.stats.spearmanr to compute the correlation and p-value.\n",
        "    # This is the industry-standard function for this statistical test.\n",
        "    # It correctly handles the calculation based on the provided ranks.\n",
        "    correlation, p_value = spearmanr(\n",
        "        group['similarity_rank'],\n",
        "        group['correlation_rank']\n",
        "    )\n",
        "\n",
        "    # Return the results as a pandas Series with named items.\n",
        "    # This allows .apply() to elegantly construct a results DataFrame.\n",
        "    return pd.Series({'correlation': correlation, 'p_value': p_value})\n",
        "\n",
        "\n",
        "def compute_spearman_rank_correlation(\n",
        "    analysis_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the Spearman rank correlation for each method and reference ETF.\n",
        "\n",
        "    This function serves as the primary evaluation engine of the study. It takes\n",
        "    the tidy, ranked data and systematically performs a Spearman rank correlation\n",
        "    test for each similarity method against the ground-truth return correlations,\n",
        "    from the perspective of each individual reference ETF.\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  **Group Data**: The input DataFrame is grouped by `['reference_etf', 'method']`,\n",
        "        creating a distinct experimental unit for each test.\n",
        "    2.  **Apply Statistical Test**: The `scipy.stats.spearmanr` function is applied\n",
        "        to the rank columns of each group to compute the correlation coefficient\n",
        "        (rho) and the two-tailed p-value.\n",
        "    3.  **Determine Significance**: Based on the computed p-values, boolean flags\n",
        "        are added to indicate statistical significance at the alpha levels\n",
        "        specified in the configuration (e.g., 5% and 10%).\n",
        "    4.  **Format Output**: The results are compiled into a clean, multi-indexed\n",
        "        DataFrame for subsequent aggregation and analysis.\n",
        "\n",
        "    Args:\n",
        "        analysis_df (pd.DataFrame): The tidy DataFrame from the ranking preparation step.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing the results of the Spearman tests,\n",
        "                      with one row per reference_etf-method combination.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(analysis_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'analysis_df' must be a pandas DataFrame.\")\n",
        "    required_cols = {'reference_etf', 'method', 'similarity_rank', 'correlation_rank'}\n",
        "    if not required_cols.issubset(analysis_df.columns):\n",
        "        raise ValueError(f\"Input DataFrame is missing required columns. Expected: {required_cols}\")\n",
        "\n",
        "    logging.info(\"Computing Spearman rank correlations for all methods...\")\n",
        "\n",
        "    # --- Step 1 & 2: Group Data and Apply Statistical Test ---\n",
        "    # Group by the experimental unit: each reference ETF and each method.\n",
        "    # .apply() passes each resulting sub-DataFrame to our helper function.\n",
        "    # The results are automatically combined into a new DataFrame.\n",
        "    results_df = analysis_df.groupby(['reference_etf', 'method']).apply(\n",
        "        _calculate_spearman_for_group\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Determine Statistical Significance ---\n",
        "    # Retrieve the significance levels from the configuration.\n",
        "    alpha_levels = config['evaluation_config']['significance_levels']\n",
        "\n",
        "    # For each specified alpha level, create a boolean flag column.\n",
        "    # The column name is dynamically created (e.g., 'significant_at_5pct').\n",
        "    for alpha in alpha_levels:\n",
        "        col_name = f\"significant_at_{int(alpha*100)}pct\"\n",
        "        results_df[col_name] = results_df['p_value'] < alpha\n",
        "\n",
        "    # Reset the index to turn the multi-index ('reference_etf', 'method')\n",
        "    # back into columns for a standard flat DataFrame structure.\n",
        "    results_df = results_df.reset_index()\n",
        "\n",
        "    logging.info(\"Spearman rank correlation computation complete.\")\n",
        "    logging.info(f\"Generated results for {len(results_df)} test cases.\")\n",
        "\n",
        "    # --- Final Validation ---\n",
        "    expected_rows = analysis_df['reference_etf'].nunique() * analysis_df['method'].nunique()\n",
        "    if len(results_df) != expected_rows:\n",
        "        raise ValueError(\"The results DataFrame has an unexpected number of rows.\")\n",
        "    if results_df['correlation'].isnull().any() or results_df['p_value'].isnull().any():\n",
        "        raise ValueError(\"The results DataFrame contains unexpected null values.\")\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "E6OdbqCoOp47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 24: Aggregate Statistics Across ETFs\n",
        "\n",
        "def aggregate_spearman_results(\n",
        "    spearman_results_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates Spearman correlation results to produce the final summary table.\n",
        "\n",
        "    This function takes the detailed, per-ETF test results and computes summary\n",
        "    statistics for each similarity method, replicating Table 4 from the paper.\n",
        "    It provides the ultimate quantitative comparison of STRAPSim against the\n",
        "    baseline methods.\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  **Group by Method**: The detailed results are grouped by the 'method' column.\n",
        "    2.  **Aggregate Statistics**: For each method, the function calculates:\n",
        "        - The average Spearman correlation coefficient.\n",
        "        - The average p-value.\n",
        "        - The percentage of tests that were statistically significant at each\n",
        "          alpha level specified in the configuration (e.g., 5% and 10%).\n",
        "    3.  **Format Table**: The resulting summary data is formatted into a\n",
        "        presentation-quality DataFrame with column names and ordering that\n",
        "        exactly match Table 4 from the paper.\n",
        "\n",
        "    Args:\n",
        "        spearman_results_df (pd.DataFrame): The DataFrame of detailed Spearman\n",
        "                                            test results from the previous task.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A summary DataFrame presenting the aggregated performance\n",
        "                      of each similarity method.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(spearman_results_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'spearman_results_df' must be a pandas DataFrame.\")\n",
        "    required_cols = {'method', 'correlation', 'p_value'}\n",
        "    if not required_cols.issubset(spearman_results_df.columns):\n",
        "        raise ValueError(f\"Input DataFrame is missing required columns. Expected at least: {required_cols}\")\n",
        "\n",
        "    logging.info(\"Aggregating Spearman correlation results across all ETFs...\")\n",
        "\n",
        "    # --- Step 1 & 2: Group by Method and Aggregate Statistics ---\n",
        "    # Define the aggregation operations in a dictionary.\n",
        "    # This is a clean and efficient way to compute multiple statistics at once.\n",
        "    agg_functions = {\n",
        "        'correlation': 'mean',\n",
        "        'p_value': 'mean'\n",
        "    }\n",
        "\n",
        "    # Add aggregation rules for the significance percentage columns.\n",
        "    # The mean of a boolean column gives the proportion of True values.\n",
        "    for alpha in config['evaluation_config']['significance_levels']:\n",
        "        col_name = f\"significant_at_{int(alpha*100)}pct\"\n",
        "        if col_name in spearman_results_df.columns:\n",
        "            agg_functions[col_name] = 'mean'\n",
        "\n",
        "    # Perform the groupby and aggregation.\n",
        "    summary_df = spearman_results_df.groupby('method').agg(agg_functions)\n",
        "\n",
        "    # --- Step 3: Format the Output Table ---\n",
        "    # Convert the significance proportions to percentages.\n",
        "    for alpha in config['evaluation_config']['significance_levels']:\n",
        "        col_name = f\"significant_at_{int(alpha*100)}pct\"\n",
        "        if col_name in summary_df.columns:\n",
        "            summary_df[col_name] = summary_df[col_name] * 100\n",
        "\n",
        "    # Rename columns to exactly match Table 4 from the paper.\n",
        "    rename_map = {\n",
        "        'correlation': 'Average Coefficient',\n",
        "        'p_value': 'Average P-value',\n",
        "        'significant_at_5pct': '% of significant samples (α=5%)',\n",
        "        'significant_at_10pct': '% of significant samples (α=10%)'\n",
        "    }\n",
        "    summary_df = summary_df.rename(columns=rename_map)\n",
        "\n",
        "    # Define the desired order of methods (rows) for the final table.\n",
        "    # Assuming the 'method' column in the input contains these exact names.\n",
        "    method_order = ['Jaccard', 'weighted Jaccard', 'BertScore', 'STRAPSim']\n",
        "    # Filter and reorder the summary DataFrame to match the paper's presentation.\n",
        "    # .reindex() is a robust way to enforce a specific order.\n",
        "    summary_df = summary_df.reindex(method_order)\n",
        "\n",
        "    # Apply final formatting for presentation.\n",
        "    # Define precision for each column.\n",
        "    formatting_map = {\n",
        "        'Average Coefficient': '{:.4f}',\n",
        "        'Average P-value': '{:.4f}',\n",
        "        '% of significant samples (α=5%)': '{:.0f}',\n",
        "        '% of significant samples (α=10%)': '{:.0f}'\n",
        "    }\n",
        "    formatted_summary_df = summary_df.style.format(formatting_map)\n",
        "\n",
        "    logging.info(\"Successfully aggregated results into summary table.\")\n",
        "    # Display the formatted table in the log for immediate review.\n",
        "    logging.info(f\"\\n--- Final Results Summary (Table 4 Replication) ---\\n{formatted_summary_df.to_string()}\")\n",
        "\n",
        "    # Return the raw (unformatted) DataFrame for potential downstream use.\n",
        "    return summary_df\n"
      ],
      "metadata": {
        "id": "CfkKKav0QJvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 25: Design End-to-End Pipeline Orchestrator\n",
        "\n",
        "def run_strapsim_pipeline(\n",
        "    etf_holdings_df: pd.DataFrame,\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    monthly_returns_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end STRAPSim research pipeline.\n",
        "\n",
        "    This master function executes the entire research workflow as described in the\n",
        "    paper, from initial data validation to the final results aggregation. It serves\n",
        "    as a single entry point to reproduce the study's findings, managing the\n",
        "    sequential execution of dozens of modular, single-responsibility functions.\n",
        "\n",
        "    The pipeline is organized into the following major phases:\n",
        "    1.  **Validation**: Rigorously validates the configuration and all raw input\n",
        "        DataFrames. The pipeline halts immediately if any validation fails.\n",
        "    2.  **Cleansing**: Standardizes and cleans the validated data to prepare it\n",
        "        for analysis.\n",
        "    3.  **Feature Engineering**: Constructs the feature and target matrices for the\n",
        "        Random Forest model, including one-hot encoding and normalization.\n",
        "    4.  **Model Training**: Optimizes hyperparameters via cross-validation, trains\n",
        "        the final model, and validates its performance against paper benchmarks.\n",
        "    5.  **Similarity Computation**: Generates the core proximity matrix and then\n",
        "        computes all similarity matrices (STRAPSim and baselines).\n",
        "    6.  **Evaluation**: Computes the ground-truth return correlation matrix and\n",
        "        evaluates the performance of each similarity metric by calculating the\n",
        "        Spearman rank correlation, producing a final summary table.\n",
        "\n",
        "    Args:\n",
        "        etf_holdings_df (pd.DataFrame): The raw DataFrame of ETF holdings.\n",
        "        bond_features_df (pd.DataFrame): The raw DataFrame of bond features.\n",
        "        monthly_returns_df (pd.DataFrame): The raw DataFrame of monthly returns.\n",
        "        config (Dict[str, Any]): The global configuration dictionary for the study.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the key artifacts of the study,\n",
        "                        including the final results summary table, all computed\n",
        "                        similarity matrices, and the trained model.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Starting STRAPSim End-to-End Research Pipeline ---\")\n",
        "\n",
        "    # A dictionary to store all major artifacts produced by the pipeline.\n",
        "    results_artifacts: Dict[str, Any] = {}\n",
        "\n",
        "    try:\n",
        "        # === PHASE 1: INPUT VALIDATION ===\n",
        "        logging.info(\"[PHASE 1/6] Validating all inputs...\")\n",
        "        is_valid, errors = validate_configuration(config)\n",
        "        if not is_valid: raise ValueError(f\"Configuration is invalid: {errors}\")\n",
        "\n",
        "        is_valid, errors = validate_etf_holdings_dataframe(etf_holdings_df, config)\n",
        "        if not is_valid: raise ValueError(f\"ETF holdings DataFrame is invalid: {errors}\")\n",
        "\n",
        "        # NOTE: Assuming bond_features_df has been prepared with an ordered categorical for ratings.\n",
        "        # In a real pipeline, this would be another pre-processing step.\n",
        "        is_valid, errors = validate_bond_features_dataframe(bond_features_df, config)\n",
        "        if not is_valid: raise ValueError(f\"Bond features DataFrame is invalid: {errors}\")\n",
        "\n",
        "        is_valid, errors = validate_monthly_returns_dataframe(monthly_returns_df, etf_holdings_df, config)\n",
        "        if not is_valid: raise ValueError(f\"Monthly returns DataFrame is invalid: {errors}\")\n",
        "\n",
        "        is_valid, errors = validate_cross_dataset_consistency(etf_holdings_df, bond_features_df, monthly_returns_df, config)\n",
        "        if not is_valid: raise ValueError(f\"Cross-dataset consistency check failed: {errors}\")\n",
        "        logging.info(\"All inputs validated successfully.\")\n",
        "\n",
        "        # === PHASE 2: DATA CLEANSING ===\n",
        "        logging.info(\"[PHASE 2/6] Cleansing and standardizing data...\")\n",
        "        holdings_clean = cleanse_etf_holdings_dataframe(etf_holdings_df, config)\n",
        "        features_clean = cleanse_bond_features_dataframe(bond_features_df, config)\n",
        "        returns_clean = cleanse_monthly_returns_dataframe(monthly_returns_df, config)\n",
        "        logging.info(\"All data cleansed successfully.\")\n",
        "\n",
        "        # === PHASE 3: FEATURE ENGINEERING ===\n",
        "        logging.info(\"[PHASE 3/6] Engineering features for Random Forest model...\")\n",
        "        categorical_encoded = prepare_categorical_features(features_clean.set_index('cusip'), config)\n",
        "        numerical_scaled, scaling_params = prepare_numerical_features(features_clean.set_index('cusip'), config)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = construct_and_split_data(\n",
        "            features_clean, categorical_encoded, numerical_scaled, config\n",
        "        )\n",
        "        # The complete feature matrix for proximity calculation\n",
        "        X_complete = pd.concat([X_train, X_test]).sort_index()\n",
        "        logging.info(\"Feature engineering and data splitting complete.\")\n",
        "\n",
        "        # === PHASE 4: MODEL TRAINING & PROXIMITY MATRIX GENERATION ===\n",
        "        logging.info(\"[PHASE 4/6] Training model and generating proximity matrix...\")\n",
        "        optimal_params = optimize_random_forest_hyperparameters(X_train, y_train, config)\n",
        "\n",
        "        final_model = train_final_model_and_evaluate(\n",
        "            X_train, y_train, X_test, y_test, optimal_params, config\n",
        "        )\n",
        "        results_artifacts['trained_model'] = final_model\n",
        "\n",
        "        proximity_matrix_df = generate_proximity_matrix(final_model, X_complete, config)\n",
        "        results_artifacts['proximity_matrix'] = proximity_matrix_df\n",
        "        logging.info(\"Model training and proximity matrix generation complete.\")\n",
        "\n",
        "        # === PHASE 5: SIMILARITY & GROUND-TRUTH COMPUTATION ===\n",
        "        logging.info(\"[PHASE 5/6] Computing all similarity matrices...\")\n",
        "        portfolio_data = extract_portfolio_level_data(holdings_clean, proximity_matrix_df)\n",
        "\n",
        "        # Store proximity matrix as a numpy array for performance\n",
        "        proximity_matrix_np = proximity_matrix_df.to_numpy()\n",
        "\n",
        "        # Compute STRAPSim\n",
        "        strapsim_df, _ = compute_all_strapsim_pairs(portfolio_data, proximity_matrix_np, config)\n",
        "\n",
        "        # Compute Baselines\n",
        "        jaccard_df, _ = compute_jaccard_matrix(portfolio_data, config)\n",
        "        wj_df, _ = compute_weighted_jaccard_matrix(portfolio_data, config)\n",
        "        _, _, f1_df, _, _, _ = compute_bertscore_matrix(portfolio_data, proximity_matrix_np, config)\n",
        "\n",
        "        # Compute Ground Truth\n",
        "        correlation_df = compute_return_correlation_matrix(returns_clean, config)\n",
        "\n",
        "        similarity_matrices = {\n",
        "            'STRAPSim': strapsim_df,\n",
        "            'Jaccard': jaccard_df,\n",
        "            'weighted Jaccard': wj_df,\n",
        "            'BertScore': f1_df # Using F1 as the primary BertScore metric\n",
        "        }\n",
        "        results_artifacts['similarity_matrices'] = similarity_matrices\n",
        "        results_artifacts['correlation_matrix'] = correlation_df\n",
        "        logging.info(\"All similarity matrices computed successfully.\")\n",
        "\n",
        "        # === PHASE 6: FINAL EVALUATION ===\n",
        "        logging.info(\"[PHASE 6/6] Performing final evaluation...\")\n",
        "        analysis_df = prepare_data_for_spearman_analysis(similarity_matrices, correlation_df, config)\n",
        "\n",
        "        spearman_results_df = compute_spearman_rank_correlation(analysis_df, config)\n",
        "\n",
        "        final_summary_table = aggregate_spearman_results(spearman_results_df, config)\n",
        "        results_artifacts['final_summary_table'] = final_summary_table\n",
        "        logging.info(\"Evaluation complete.\")\n",
        "\n",
        "    except (ValueError, TypeError, KeyError, RuntimeError) as e:\n",
        "        # Catch any errors during the pipeline, log them, and re-raise.\n",
        "        logging.error(f\"--- PIPELINE FAILED ---\")\n",
        "        logging.error(f\"An error occurred during the pipeline execution: {e}\")\n",
        "        # Re-raise the exception to halt execution and provide a traceback.\n",
        "        raise\n",
        "\n",
        "    logging.info(\"--- STRAPSim End-to-End Research Pipeline Completed Successfully ---\")\n",
        "\n",
        "    # Return all the key results and artifacts.\n",
        "    return results_artifacts\n"
      ],
      "metadata": {
        "id": "RJ0kQGkMTxV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extended Pipeline"
      ],
      "metadata": {
        "id": "O4-HJJAOWhNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 26: Random Forest Hyperparameter Sensitivity Analysis\n",
        "\n",
        "def _run_single_sensitivity_scenario(\n",
        "    scenario_params: Dict[str, Any],\n",
        "    base_config: Dict[str, Any],\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.DataFrame,\n",
        "    X_test: pd.DataFrame,\n",
        "    y_test: pd.DataFrame,\n",
        "    X_complete: pd.DataFrame,\n",
        "    portfolio_data: Dict, # Using generic Dict for brevity, should be PortfolioData\n",
        "    correlation_df: pd.DataFrame\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Executes a single run of the core analysis pipeline for a given scenario.\n",
        "\n",
        "    This helper function takes a specific set of hyperparameters, trains a model,\n",
        "    generates the proximity and STRAPSim matrices, and evaluates the final\n",
        "    Spearman correlation. It is a streamlined version of the main pipeline,\n",
        "    bypassing the hyperparameter search.\n",
        "\n",
        "    Args:\n",
        "        scenario_params (Dict[str, Any]): The hyperparameters for this specific run.\n",
        "        base_config (Dict[str, Any]): The base configuration dictionary.\n",
        "        (and all necessary data artifacts: X_train, y_train, etc.)\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]: A tuple containing the test RMSE of the model and\n",
        "                             the final average Spearman correlation for STRAPSim.\n",
        "    \"\"\"\n",
        "    # Create a deep copy of the config to avoid modifying the original.\n",
        "    scenario_config = copy.deepcopy(base_config)\n",
        "\n",
        "    # --- Train Model with Scenario Parameters ---\n",
        "    # Instantiate and train the model directly with the scenario's hyperparameters.\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=scenario_params['n_estimators'],\n",
        "        max_depth=scenario_params.get('max_depth'), # .get() handles None for max_depth\n",
        "        random_state=scenario_config['random_forest_config']['random_state'],\n",
        "        n_jobs=scenario_config['random_forest_config']['n_jobs']\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # --- Evaluate Model Performance (RMSE) ---\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    test_rmse, _ = _calculate_multi_output_metrics(y_test, y_test_pred, y_test.columns.tolist())\n",
        "\n",
        "    # --- Generate Proximity and STRAPSim Matrices ---\n",
        "    proximity_matrix_df = generate_proximity_matrix(model, X_complete, scenario_config)\n",
        "    proximity_matrix_np = proximity_matrix_df.to_numpy()\n",
        "\n",
        "    strapsim_df, _ = compute_all_strapsim_pairs(\n",
        "        portfolio_data, proximity_matrix_np, scenario_config\n",
        "    )\n",
        "\n",
        "    # --- Evaluate Final Spearman Correlation ---\n",
        "    # We only need to evaluate the STRAPSim method for this analysis.\n",
        "    analysis_df = prepare_data_for_spearman_analysis(\n",
        "        {'STRAPSim': strapsim_df}, correlation_df, scenario_config\n",
        "    )\n",
        "    spearman_results_df = compute_spearman_rank_correlation(analysis_df, scenario_config)\n",
        "\n",
        "    # The result is the average correlation for the STRAPSim method.\n",
        "    avg_spearman_corr = spearman_results_df['correlation'].mean()\n",
        "\n",
        "    return test_rmse, avg_spearman_corr\n",
        "\n",
        "\n",
        "def run_hyperparameter_sensitivity_analysis(\n",
        "    # This function requires all the clean data and artifacts as input.\n",
        "    # In a class-based implementation, these would be attributes.\n",
        "    # Here, we pass them as arguments for a functional approach.\n",
        "    base_config: Dict[str, Any],\n",
        "    optimal_params: Dict[str, Any],\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.DataFrame,\n",
        "    X_test: pd.DataFrame,\n",
        "    y_test: pd.DataFrame,\n",
        "    portfolio_data: Dict, # PortfolioData\n",
        "    correlation_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates a sensitivity analysis on Random Forest hyperparameters.\n",
        "\n",
        "    This function systematically evaluates the robustness of the study's main\n",
        "    conclusion (the performance of STRAPSim) to changes in the underlying\n",
        "    Random Forest model's hyperparameters (`n_estimators` and `max_depth`).\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  **Define Scenarios**: Creates a list of alternative hyperparameter\n",
        "        configurations, varying one parameter at a time while keeping the other\n",
        "        at its optimal value.\n",
        "    2.  **Iterate and Execute**: For each scenario, it runs a streamlined version\n",
        "        of the analysis pipeline to compute the key performance indicators:\n",
        "        the model's test RMSE and the final average Spearman correlation for STRAPSim.\n",
        "    3.  **Aggregate and Report**: Compiles the results from all scenarios into a\n",
        "        single summary DataFrame, allowing for a clear analysis of how the final\n",
        "        outcome changes with the model's configuration.\n",
        "\n",
        "    Args:\n",
        "        base_config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "        optimal_params (Dict[str, Any]): The optimal parameters found by GridSearchCV.\n",
        "        (and all necessary data artifacts from the main pipeline)\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A summary DataFrame showing the test RMSE and average\n",
        "                      Spearman correlation for each tested hyperparameter scenario.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Starting Hyperparameter Sensitivity Analysis ---\")\n",
        "\n",
        "    # --- Step 1: Define Scenarios ---\n",
        "    scenarios: List[Dict[str, Any]] = []\n",
        "    # Scenario Group 1: Vary n_estimators, keep max_depth optimal.\n",
        "    n_estimators_range = [50, 100, 200, 300, 500, 1000]\n",
        "    for n in n_estimators_range:\n",
        "        scenarios.append({'n_estimators': n, 'max_depth': optimal_params['max_depth']})\n",
        "\n",
        "    # Scenario Group 2: Vary max_depth, keep n_estimators optimal.\n",
        "    max_depth_range = [5, 10, 15, 20, 25, None]\n",
        "    for d in max_depth_range:\n",
        "        scenarios.append({'n_estimators': optimal_params['n_estimators'], 'max_depth': d})\n",
        "\n",
        "    # Ensure the optimal parameters are included for a baseline comparison.\n",
        "    if optimal_params not in scenarios:\n",
        "        scenarios.insert(0, optimal_params)\n",
        "\n",
        "    # Remove duplicate scenarios if any exist.\n",
        "    unique_scenarios = [dict(t) for t in {tuple(d.items()) for d in scenarios}]\n",
        "    logging.info(f\"Defined {len(unique_scenarios)} unique scenarios for sensitivity analysis.\")\n",
        "\n",
        "    # --- Step 2: Iterate and Execute ---\n",
        "    results: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Recreate the complete feature matrix needed for proximity calculation.\n",
        "    X_complete = pd.concat([X_train, X_test]).sort_index()\n",
        "\n",
        "    for i, params in enumerate(unique_scenarios):\n",
        "        logging.info(f\"--- Running Scenario {i+1}/{len(unique_scenarios)}: {params} ---\")\n",
        "        try:\n",
        "            test_rmse, avg_spearman_corr = _run_single_sensitivity_scenario(\n",
        "                scenario_params=params,\n",
        "                base_config=base_config,\n",
        "                X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                X_complete=X_complete,\n",
        "                portfolio_data=portfolio_data,\n",
        "                correlation_df=correlation_df\n",
        "            )\n",
        "\n",
        "            # Store the results for this scenario.\n",
        "            results.append({\n",
        "                'n_estimators': params['n_estimators'],\n",
        "                'max_depth': params.get('max_depth'),\n",
        "                'test_rmse': test_rmse,\n",
        "                'avg_spearman_corr': avg_spearman_corr\n",
        "            })\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Scenario {params} failed with error: {e}\")\n",
        "            results.append({\n",
        "                'n_estimators': params['n_estimators'],\n",
        "                'max_depth': params.get('max_depth'),\n",
        "                'test_rmse': np.nan,\n",
        "                'avg_spearman_corr': np.nan\n",
        "            })\n",
        "\n",
        "    # --- Step 3: Aggregate and Report ---\n",
        "    results_df = pd.DataFrame(results).sort_values(by=['n_estimators', 'max_depth']).reset_index(drop=True)\n",
        "\n",
        "    logging.info(\"--- Hyperparameter Sensitivity Analysis Complete ---\")\n",
        "    logging.info(f\"\\n--- Sensitivity Analysis Results ---\\n{results_df.to_string()}\")\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "cqWOHOL8XvBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 27: Data Split Sensitivity Analysis\n",
        "\n",
        "def _run_single_split_scenario(\n",
        "    random_seed: int,\n",
        "    base_config: Dict[str, Any],\n",
        "    optimal_params: Dict[str, Any],\n",
        "    X_complete: pd.DataFrame,\n",
        "    y_complete: pd.DataFrame,\n",
        "    portfolio_data: Dict, # PortfolioData\n",
        "    correlation_df: pd.DataFrame\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Executes a single run of the analysis pipeline for a given random seed.\n",
        "\n",
        "    This helper function takes a specific random seed, re-splits the data,\n",
        "    re-trains the model, and re-evaluates the final Spearman correlation.\n",
        "\n",
        "    Args:\n",
        "        random_seed (int): The random state to use for the train-test split.\n",
        "        base_config (Dict[str, Any]): The base configuration dictionary.\n",
        "        optimal_params (Dict[str, Any]): The fixed, optimal model hyperparameters.\n",
        "        (and all necessary data artifacts: X_complete, y_complete, etc.)\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]: A tuple containing the test RMSE of the model and\n",
        "                             the final average Spearman correlation for STRAPSim.\n",
        "    \"\"\"\n",
        "    # Create a deep copy of the config and update the random seed for the split.\n",
        "    scenario_config = copy.deepcopy(base_config)\n",
        "    scenario_config['feature_engineering']['random_state_for_shuffle'] = random_seed\n",
        "\n",
        "    # --- Step 1: Re-split the data with the new random seed ---\n",
        "    X_train, X_test, y_train, y_test = _perform_data_split(\n",
        "        X_complete, y_complete, scenario_config\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Re-train the model and evaluate its RMSE ---\n",
        "    # Note: We do not re-run hyperparameter optimization. We use the optimal params.\n",
        "    model = train_final_model_and_evaluate(\n",
        "        X_train, y_train, X_test, y_test, optimal_params, scenario_config\n",
        "    )\n",
        "\n",
        "    # Extract the test RMSE for this run's model.\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    test_rmse, _ = _calculate_multi_output_metrics(y_test, y_test_pred, y_test.columns.tolist())\n",
        "\n",
        "    # --- Step 3: Generate Proximity and STRAPSim Matrices ---\n",
        "    proximity_matrix_df = generate_proximity_matrix(model, X_complete, scenario_config)\n",
        "    proximity_matrix_np = proximity_matrix_df.to_numpy()\n",
        "\n",
        "    strapsim_df, _ = compute_all_strapsim_pairs(\n",
        "        portfolio_data, proximity_matrix_np, scenario_config\n",
        "    )\n",
        "\n",
        "    # --- Step 4: Evaluate Final Spearman Correlation ---\n",
        "    analysis_df = prepare_data_for_spearman_analysis(\n",
        "        {'STRAPSim': strapsim_df}, correlation_df, scenario_config\n",
        "    )\n",
        "    spearman_results_df = compute_spearman_rank_correlation(analysis_df, scenario_config)\n",
        "    avg_spearman_corr = spearman_results_df['correlation'].mean()\n",
        "\n",
        "    return test_rmse, avg_spearman_corr\n",
        "\n",
        "\n",
        "def run_data_split_sensitivity_analysis(\n",
        "    base_config: Dict[str, Any],\n",
        "    optimal_params: Dict[str, Any],\n",
        "    # This function requires the complete, unsplit datasets.\n",
        "    X_complete: pd.DataFrame,\n",
        "    y_complete: pd.DataFrame,\n",
        "    portfolio_data: Dict, # PortfolioData\n",
        "    correlation_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates a sensitivity analysis on the train-test split random seed.\n",
        "\n",
        "    This function assesses the robustness of the study's results to the specific\n",
        "    random partitioning of data into training and testing sets. It repeatedly\n",
        "    runs the core analysis pipeline, each time with a different random seed,\n",
        "    and analyzes the stability of the final performance metrics.\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  **Define Scenarios**: A predefined list of random seeds is used.\n",
        "    2.  **Iterate and Execute**: For each seed, it re-splits the data, re-trains\n",
        "        the model using the fixed optimal hyperparameters, and re-calculates\n",
        "        the model's test RMSE and the final STRAPSim Spearman correlation.\n",
        "    3.  **Aggregate and Report**: Compiles the results from all runs into a\n",
        "        summary DataFrame and calculates the mean and standard deviation of the\n",
        "        key metrics to quantify the stability of the results.\n",
        "\n",
        "    Args:\n",
        "        base_config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "        optimal_params (Dict[str, Any]): The optimal parameters found by GridSearchCV.\n",
        "        X_complete (pd.DataFrame): The complete, unsplit feature matrix.\n",
        "        y_complete (pd.DataFrame): The complete, unsplit target matrix.\n",
        "        (and other necessary data artifacts)\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A summary DataFrame showing the test RMSE and average\n",
        "                      Spearman correlation for each tested random seed.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Starting Data Split Sensitivity Analysis ---\")\n",
        "\n",
        "    # --- Step 1: Define Scenarios (Random Seeds) ---\n",
        "    # A list of seeds to test the stability of the results.\n",
        "    random_seeds_to_test: List[int] = [0, 1, 7, 21, 42, 99, 123, 456, 789, 999]\n",
        "    logging.info(f\"Testing {len(random_seeds_to_test)} different random seeds for train-test split.\")\n",
        "\n",
        "    # --- Step 2: Iterate and Execute ---\n",
        "    results: List[Dict[str, Any]] = []\n",
        "\n",
        "    for i, seed in enumerate(random_seeds_to_test):\n",
        "        logging.info(f\"--- Running Scenario {i+1}/{len(random_seeds_to_test)}: random_seed={seed} ---\")\n",
        "        try:\n",
        "            test_rmse, avg_spearman_corr = _run_single_split_scenario(\n",
        "                random_seed=seed,\n",
        "                base_config=base_config,\n",
        "                optimal_params=optimal_params,\n",
        "                X_complete=X_complete,\n",
        "                y_complete=y_complete,\n",
        "                portfolio_data=portfolio_data,\n",
        "                correlation_df=correlation_df\n",
        "            )\n",
        "\n",
        "            results.append({\n",
        "                'random_seed': seed,\n",
        "                'test_rmse': test_rmse,\n",
        "                'avg_spearman_corr': avg_spearman_corr\n",
        "            })\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Scenario with seed {seed} failed with error: {e}\")\n",
        "            results.append({\n",
        "                'random_seed': seed,\n",
        "                'test_rmse': np.nan,\n",
        "                'avg_spearman_corr': np.nan\n",
        "            })\n",
        "\n",
        "    # --- Step 3: Aggregate and Report ---\n",
        "    results_df = pd.DataFrame(results).set_index('random_seed')\n",
        "\n",
        "    logging.info(\"--- Data Split Sensitivity Analysis Complete ---\")\n",
        "    logging.info(f\"\\n--- Sensitivity Analysis Results ---\\n{results_df.to_string()}\")\n",
        "\n",
        "    # Calculate and log summary statistics for the key outcome variable.\n",
        "    mean_corr = results_df['avg_spearman_corr'].mean()\n",
        "    std_corr = results_df['avg_spearman_corr'].std()\n",
        "    logging.info(f\"\\n--- Stability Summary ---\")\n",
        "    logging.info(f\"Average Spearman Correlation across all seeds: {mean_corr:.4f}\")\n",
        "    logging.info(f\"Standard Deviation of Spearman Correlation:   {std_corr:.4f}\")\n",
        "\n",
        "    if std_corr > 0.05:\n",
        "        logging.warning(\"High standard deviation detected, suggesting results may be sensitive to the data split.\")\n",
        "    else:\n",
        "        logging.info(\"Low standard deviation detected, suggesting results are robust to the data split.\")\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "0R__91-wXsqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 28: Similarity Metric Component Sensitivity\n",
        "\n",
        "# Define type hints from previous tasks for clarity.\n",
        "Portfolio = Dict[str, Union[List[str], np.ndarray]]\n",
        "PortfolioData = Dict[str, Portfolio]\n",
        "BERTScoreFullResult = Tuple[float, float, float, float, float, float]\n",
        "\n",
        "\n",
        "def _run_threshold_sensitivity_analysis(\n",
        "    base_config: Dict[str, Any],\n",
        "    portfolio_data: PortfolioData,\n",
        "    proximity_matrix_np: np.ndarray\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analyzes the sensitivity of STRAPSim results to the convergence threshold.\n",
        "\n",
        "    Args:\n",
        "        base_config (Dict[str, Any]): The base configuration dictionary.\n",
        "        portfolio_data (PortfolioData): The structured portfolio data.\n",
        "        proximity_matrix_np (np.ndarray): The NumPy proximity matrix.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A summary of results for each tested threshold.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Starting STRAPSim Convergence Threshold Sensitivity Analysis ---\")\n",
        "\n",
        "    # Define the scenarios: a range of convergence thresholds to test.\n",
        "    thresholds_to_test = [1e-4, 1e-5, 1e-6, 1e-7, 1e-8]\n",
        "    results = []\n",
        "\n",
        "    for threshold in thresholds_to_test:\n",
        "        logging.info(f\"Running analysis with convergence_threshold = {threshold}...\")\n",
        "        # Create a deep copy of the config and modify the threshold.\n",
        "        scenario_config = copy.deepcopy(base_config)\n",
        "        scenario_config['strapsim_algorithm']['weight_precision_tolerance'] = threshold\n",
        "\n",
        "        start_time = time.time()\n",
        "        # Re-compute the STRAPSim matrix with the modified config.\n",
        "        strapsim_df, _ = compute_all_strapsim_pairs(\n",
        "            portfolio_data, proximity_matrix_np, scenario_config\n",
        "        )\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        # Calculate the average similarity score (excluding the diagonal).\n",
        "        np.fill_diagonal(strapsim_df.values, np.nan)\n",
        "        avg_score = strapsim_df.stack().mean()\n",
        "\n",
        "        results.append({\n",
        "            'convergence_threshold': threshold,\n",
        "            'avg_strapsim_score': avg_score,\n",
        "            'computation_time_sec': duration\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results).set_index('convergence_threshold')\n",
        "\n",
        "\n",
        "def _run_low_weight_holding_sensitivity_analysis(\n",
        "    base_config: Dict[str, Any],\n",
        "    holdings_clean_df: pd.DataFrame,\n",
        "    proximity_matrix_df: pd.DataFrame,\n",
        "    correlation_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analyzes the sensitivity of results to the exclusion of low-weight holdings.\n",
        "\n",
        "    Args:\n",
        "        base_config (Dict[str, Any]): The base configuration dictionary.\n",
        "        holdings_clean_df (pd.DataFrame): The cleansed holdings data.\n",
        "        proximity_matrix_df (pd.DataFrame): The proximity matrix as a DataFrame.\n",
        "        correlation_df (pd.DataFrame): The ground-truth return correlation matrix.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A summary of evaluation results for each exclusion threshold.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Starting Low-Weight Holding Exclusion Sensitivity Analysis ---\")\n",
        "\n",
        "    # Define scenarios: weight thresholds below which holdings are excluded. 0.0 is the baseline.\n",
        "    exclusion_thresholds = [0.0, 0.0001, 0.001, 0.005, 0.01] # 0.0%, 0.01%, 0.1%, 0.5%, 1.0%\n",
        "    results = []\n",
        "    proximity_matrix_np = proximity_matrix_df.to_numpy()\n",
        "\n",
        "    for threshold in exclusion_thresholds:\n",
        "        logging.info(f\"Running analysis with exclusion_threshold = {threshold:.4f}...\")\n",
        "\n",
        "        # Filter the holdings DataFrame.\n",
        "        filtered_holdings = holdings_clean_df[holdings_clean_df['weight'] >= threshold].copy()\n",
        "\n",
        "        # CRITICAL: Re-normalize the weights of the remaining holdings.\n",
        "        weight_sums = filtered_holdings.groupby('etf_id')['weight'].transform('sum')\n",
        "        filtered_holdings['weight'] = filtered_holdings['weight'] / weight_sums\n",
        "\n",
        "        # Re-run the core analysis pipeline with the filtered & re-normalized data.\n",
        "        portfolio_data_filtered = extract_portfolio_level_data(filtered_holdings, proximity_matrix_df)\n",
        "\n",
        "        strapsim_df, _ = compute_all_strapsim_pairs(\n",
        "            portfolio_data_filtered, proximity_matrix_np, base_config\n",
        "        )\n",
        "\n",
        "        analysis_df = prepare_data_for_spearman_analysis(\n",
        "            {'STRAPSim': strapsim_df}, correlation_df, base_config\n",
        "        )\n",
        "        spearman_results_df = compute_spearman_rank_correlation(analysis_df, base_config)\n",
        "        avg_spearman_corr = spearman_results_df['correlation'].mean()\n",
        "\n",
        "        results.append({\n",
        "            'exclusion_threshold': threshold,\n",
        "            'avg_spearman_corr': avg_spearman_corr,\n",
        "            'avg_holdings_per_etf': filtered_holdings.groupby('etf_id').size().mean()\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results).set_index('exclusion_threshold')\n",
        "\n",
        "\n",
        "def _compute_bertscore_for_pair(\n",
        "    portfolio_x: Portfolio,\n",
        "    portfolio_y: Portfolio,\n",
        "    proximity_matrix: np.ndarray,\n",
        "    weight_scheme: str = 'min_weight'\n",
        ") -> BERTScoreFullResult:\n",
        "    \"\"\"\n",
        "    Computes the adapted BERTScore and its residuals with dynamic weight schemes.\n",
        "\n",
        "    This function provides a complete implementation of the adapted BERTScore\n",
        "    metric as described in Section 2.4 of the paper, including both the primary\n",
        "    scores (Equations 6a, 6b, 6c) and the residual scores (Equations 7a, 7b, 7c).\n",
        "    This version is updated to accept a `weight_scheme` parameter, which\n",
        "    dynamically alters the logic used in the residual calculation, making it\n",
        "    suitable for sensitivity analysis.\n",
        "\n",
        "    Args:\n",
        "        portfolio_x (Portfolio): The structured data for the reference portfolio (X).\n",
        "        portfolio_y (Portfolio): The structured data for the candidate portfolio (Y).\n",
        "        proximity_matrix (np.ndarray): The global (N x N) matrix of similarities.\n",
        "        weight_scheme (str): The weighting scheme to use for residual calculation.\n",
        "                             Options: 'min_weight', 'max_weight', 'arithmetic_mean'.\n",
        "                             Defaults to 'min_weight'.\n",
        "\n",
        "    Returns:\n",
        "        BERTScoreFullResult: A tuple containing six scores:\n",
        "        (Recall, Precision, F1, Recall_Residual, Precision_Residual, F1_Residual).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an unknown `weight_scheme` is provided.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    # Extract integer indices for efficient slicing of the proximity matrix.\n",
        "    indices_x: np.ndarray = portfolio_x['indices']\n",
        "    indices_y: np.ndarray = portfolio_y['indices']\n",
        "\n",
        "    # Extract weight vectors as NumPy arrays.\n",
        "    weights_x: np.ndarray = portfolio_x['weights']\n",
        "    weights_y: np.ndarray = portfolio_y['weights']\n",
        "\n",
        "    # --- Edge Case Handling ---\n",
        "    # Handle cases where one or both portfolios are empty.\n",
        "    if len(indices_x) == 0 or len(indices_y) == 0:\n",
        "        # If both are empty, they are identical (similarity=1, residual=0).\n",
        "        is_identical = len(indices_x) == len(indices_y)\n",
        "        score = 1.0 if is_identical else 0.0\n",
        "        residual = 0.0 if is_identical else 1.0\n",
        "        return (score, score, score, residual, residual, residual)\n",
        "\n",
        "    # --- Primary Score Calculation (Vectorized) ---\n",
        "    # Slice the proximity matrix to get the relevant sub-matrix of similarities.\n",
        "    sub_matrix = proximity_matrix[np.ix_(indices_x, indices_y)]\n",
        "\n",
        "    # Equation 6a (Recall): Weighted average of max similarities for each item in X.\n",
        "    # For each row (item in X), find the maximum similarity score against all items in Y.\n",
        "    max_sim_for_x = sub_matrix.max(axis=1)\n",
        "    # The recall is the dot product of X's weights and these best-match scores.\n",
        "    recall = np.dot(weights_x, max_sim_for_x)\n",
        "\n",
        "    # Equation 6b (Precision): Weighted average of max similarities for each item in Y.\n",
        "    # For each column (item in Y), find the maximum similarity score against all items in X.\n",
        "    max_sim_for_y = sub_matrix.max(axis=0)\n",
        "    # The precision is the dot product of Y's weights and these best-match scores.\n",
        "    precision = np.dot(weights_y, max_sim_for_y)\n",
        "\n",
        "    # Equation 6c (F1 Score): Harmonic mean of Precision and Recall.\n",
        "    # Check for division by zero.\n",
        "    if (precision + recall) == 0:\n",
        "        f1_score = 0.0\n",
        "    else:\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    # --- DYNAMIC Residual Score Calculation (Iterative) ---\n",
        "    # Helper to apply the selected weighting scheme.\n",
        "    def get_transfer_weight(w1: float, w2: float) -> float:\n",
        "        if weight_scheme == 'min_weight':\n",
        "            return min(w1, w2)\n",
        "        if weight_scheme == 'max_weight':\n",
        "            return max(w1, w2)\n",
        "        if weight_scheme == 'arithmetic_mean':\n",
        "            return (w1 + w2) / 2.0\n",
        "        # Raise an error for unsupported schemes.\n",
        "        raise ValueError(f\"Unknown weight_scheme: '{weight_scheme}'\")\n",
        "\n",
        "    # Equation 7a (Recall Residual): Calculate remaining weight in X.\n",
        "    # Create a copy of the weights to track consumption.\n",
        "    residual_weights_x = weights_x.copy()\n",
        "    # Find the index of the best match in Y for each item in X.\n",
        "    best_match_indices_for_x = np.argmax(sub_matrix, axis=1)\n",
        "    # Iterate through each item in X to calculate its weight consumption.\n",
        "    for i in range(len(indices_x)):\n",
        "        # Get the index of the best-matching item in Y.\n",
        "        best_match_j = best_match_indices_for_x[i]\n",
        "        # Calculate the weight to be transferred based on the chosen scheme.\n",
        "        transfer_weight = get_transfer_weight(weights_x[i], weights_y[best_match_j])\n",
        "        # Decrement the residual weight.\n",
        "        residual_weights_x[i] -= transfer_weight\n",
        "    # The residual score is 1 minus the sum of remaining non-negative weights.\n",
        "    recall_residual = 1.0 - np.sum(np.maximum(0, residual_weights_x))\n",
        "\n",
        "    # Equation 7b (Precision Residual): Calculate remaining weight in Y.\n",
        "    # Create a copy of the weights to track consumption.\n",
        "    residual_weights_y = weights_y.copy()\n",
        "    # Find the index of the best match in X for each item in Y.\n",
        "    best_match_indices_for_y = np.argmax(sub_matrix, axis=0)\n",
        "    # Iterate through each item in Y to calculate its weight consumption.\n",
        "    for j in range(len(indices_y)):\n",
        "        # Get the index of the best-matching item in X.\n",
        "        best_match_i = best_match_indices_for_y[j]\n",
        "        # Calculate the weight to be transferred.\n",
        "        transfer_weight = get_transfer_weight(weights_y[j], weights_x[best_match_i])\n",
        "        # Decrement the residual weight.\n",
        "        residual_weights_y[j] -= transfer_weight\n",
        "    # The residual score is 1 minus the sum of remaining non-negative weights.\n",
        "    precision_residual = 1.0 - np.sum(np.maximum(0, residual_weights_y))\n",
        "\n",
        "    # Equation 7c (F1 Residual): Harmonic mean of the two residual scores.\n",
        "    # Check for division by zero.\n",
        "    if (precision_residual + recall_residual) == 0:\n",
        "        f1_residual = 0.0\n",
        "    else:\n",
        "        f1_residual = 2 * (precision_residual * recall_residual) / (precision_residual + recall_residual)\n",
        "\n",
        "    # Return all six computed metrics.\n",
        "    return recall, precision, f1_score, recall_residual, precision_residual, f1_residual\n",
        "\n",
        "\n",
        "def compute_bertscore_matrix(\n",
        "    portfolios: PortfolioData,\n",
        "    proximity_matrix: np.ndarray,\n",
        "    config: Dict[str, Any],\n",
        "    weight_scheme: str = 'min_weight'\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of adapted BERTScore and residuals for all pairs.\n",
        "\n",
        "    This function systematically calculates the full suite of six BERTScore-related\n",
        "    metrics for every unique pair of ETFs. It is refactored to accept a\n",
        "    `weight_scheme` parameter, which is passed down to the core calculation\n",
        "    helper function, enabling sensitivity analysis on this component.\n",
        "\n",
        "    Args:\n",
        "        portfolios (PortfolioData): The nested dictionary of structured portfolio data.\n",
        "        proximity_matrix (np.ndarray): The global (N x N) matrix of similarities.\n",
        "        config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "        weight_scheme (str): The weighting scheme to use for residual calculation.\n",
        "                             Defaults to 'min_weight'.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, ...]: A tuple containing the six square DataFrames:\n",
        "        (Recall_Scores, Precision_Scores, F1_Scores, Recall_Residuals,\n",
        "         Precision_Residuals, F1_Residuals).\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(portfolios, dict):\n",
        "        raise TypeError(\"Input 'portfolios' must be a dictionary.\")\n",
        "    if not isinstance(proximity_matrix, np.ndarray):\n",
        "        raise TypeError(\"Input 'proximity_matrix' must be a numpy array.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Get a sorted list of ETF IDs for deterministic matrix order.\n",
        "    etf_ids: List[str] = sorted(portfolios.keys())\n",
        "    n_etfs = len(etf_ids)\n",
        "    # Create a mapping from ETF ID to its integer index in the matrices.\n",
        "    etf_to_matrix_idx: Dict[str, int] = {etf_id: i for i, etf_id in enumerate(etf_ids)}\n",
        "\n",
        "    # Initialize six empty matrices for all scores and residuals.\n",
        "    matrices = {\n",
        "        \"recall\": np.full((n_etfs, n_etfs), np.nan, dtype=np.float64),\n",
        "        \"precision\": np.full((n_etfs, n_etfs), np.nan, dtype=np.float64),\n",
        "        \"f1\": np.full((n_etfs, n_etfs), np.nan, dtype=np.float64),\n",
        "        \"recall_residual\": np.full((n_etfs, n_etfs), np.nan, dtype=np.float64),\n",
        "        \"precision_residual\": np.full((n_etfs, n_etfs), np.nan, dtype=np.float64),\n",
        "        \"f1_residual\": np.full((n_etfs, n_etfs), np.nan, dtype=np.float64),\n",
        "    }\n",
        "\n",
        "    # Set diagonals: 1.0 for scores, 0.0 for residuals.\n",
        "    for name, matrix in matrices.items():\n",
        "        fill_value = 1.0 if \"residual\" not in name else 0.0\n",
        "        np.fill_diagonal(matrix, fill_value)\n",
        "\n",
        "    # --- Pairwise Computation ---\n",
        "    # Generate all unique pairs of ETF IDs to compute.\n",
        "    etf_pairs = list(itertools.combinations(etf_ids, 2))\n",
        "\n",
        "    # Iterate through each unique pair.\n",
        "    for etf_id_x, etf_id_y in tqdm(etf_pairs, desc=f\"Computing BERTScore Suite (scheme: {weight_scheme})\"):\n",
        "        # Retrieve the structured data for the two portfolios.\n",
        "        portfolio_x = portfolios[etf_id_x]\n",
        "        portfolio_y = portfolios[etf_id_y]\n",
        "\n",
        "        # Compute scores for the (X, Y) direction, passing the weight_scheme.\n",
        "        r_xy, p_xy, f1_xy, rr_xy, pr_xy, fr_xy = _compute_bertscore_for_pair(\n",
        "            portfolio_x, portfolio_y, proximity_matrix, weight_scheme\n",
        "        )\n",
        "\n",
        "        # Compute scores for the (Y, X) direction.\n",
        "        r_yx, p_yx, f1_yx, rr_yx, pr_yx, fr_yx = _compute_bertscore_for_pair(\n",
        "            portfolio_y, portfolio_x, proximity_matrix, weight_scheme\n",
        "        )\n",
        "\n",
        "        # Get matrix indices for storage.\n",
        "        idx_x = etf_to_matrix_idx[etf_id_x]\n",
        "        idx_y = etf_to_matrix_idx[etf_id_y]\n",
        "\n",
        "        # Store directional scores (Recall and Precision + their residuals).\n",
        "        matrices[\"recall\"][idx_x, idx_y], matrices[\"recall\"][idx_y, idx_x] = r_xy, r_yx\n",
        "        matrices[\"precision\"][idx_x, idx_y], matrices[\"precision\"][idx_y, idx_x] = p_xy, p_yx\n",
        "        matrices[\"recall_residual\"][idx_x, idx_y], matrices[\"recall_residual\"][idx_y, idx_x] = rr_xy, rr_yx\n",
        "        matrices[\"precision_residual\"][idx_x, idx_y], matrices[\"precision_residual\"][idx_y, idx_x] = pr_xy, pr_yx\n",
        "\n",
        "        # Store symmetric scores (F1 and its residual) by averaging for robustness.\n",
        "        f1_avg = (f1_xy + f1_yx) / 2.0\n",
        "        matrices[\"f1\"][idx_x, idx_y] = matrices[\"f1\"][idx_y, idx_x] = f1_avg\n",
        "        fr_avg = (fr_xy + fr_yx) / 2.0\n",
        "        matrices[\"f1_residual\"][idx_x, idx_y] = matrices[\"f1_residual\"][idx_y, idx_x] = fr_avg\n",
        "\n",
        "    # --- Finalization ---\n",
        "    # Convert all matrices to labeled DataFrames.\n",
        "    dataframes = {\n",
        "        name: pd.DataFrame(matrix, index=etf_ids, columns=etf_ids)\n",
        "        for name, matrix in matrices.items()\n",
        "    }\n",
        "\n",
        "    # Return the tuple of six completed DataFrames.\n",
        "    return (\n",
        "        dataframes[\"recall\"], dataframes[\"precision\"], dataframes[\"f1\"],\n",
        "        dataframes[\"recall_residual\"], dataframes[\"precision_residual\"], dataframes[\"f1_residual\"],\n",
        "    )\n",
        "\n",
        "\n",
        "def _run_bertscore_weight_scheme_analysis(\n",
        "    base_config: Dict[str, Any],\n",
        "    portfolio_data: PortfolioData,\n",
        "    proximity_matrix_np: np.ndarray,\n",
        "    correlation_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analyzes the sensitivity of BERTScore results to the weighting scheme.\n",
        "\n",
        "    This helper function iterates through different weighting schemes defined for\n",
        "    the BERTScore residual calculation. For each scheme, it re-computes the\n",
        "    BERTScore F1 matrix and evaluates its alignment with the ground-truth return\n",
        "    correlations via the Spearman rank correlation test.\n",
        "\n",
        "    Args:\n",
        "        base_config (Dict[str, Any]): The base configuration dictionary.\n",
        "        portfolio_data (PortfolioData): The structured portfolio data.\n",
        "        proximity_matrix_np (np.ndarray): The NumPy proximity matrix.\n",
        "        correlation_df (pd.DataFrame): The ground-truth return correlation matrix.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A summary of evaluation results for each weighting scheme.\n",
        "    \"\"\"\n",
        "    # Announce the start of this specific analysis.\n",
        "    logging.info(\"--- Starting BERTScore Weighting Scheme Sensitivity Analysis ---\")\n",
        "\n",
        "    # Define scenarios: different weighting schemes for the residual calculation.\n",
        "    schemes_to_test = ['min_weight', 'max_weight', 'arithmetic_mean']\n",
        "    # Initialize a list to store the results of each scenario.\n",
        "    results = []\n",
        "\n",
        "    # Iterate through each defined weighting scheme.\n",
        "    for scheme in schemes_to_test:\n",
        "        # Log the current scenario being executed.\n",
        "        logging.info(f\"Running analysis with weight_scheme = '{scheme}'...\")\n",
        "\n",
        "        # Re-compute the full suite of BERTScore matrices using the specific scheme.\n",
        "        # We only need the F1 score matrix for this analysis.\n",
        "        _, _, f1_df, _, _, _ = compute_bertscore_matrix(\n",
        "            portfolio_data, proximity_matrix_np, base_config, weight_scheme=scheme\n",
        "        )\n",
        "\n",
        "        # Evaluate the final Spearman correlation for the F1 score from this scheme.\n",
        "        # Prepare the data for analysis, focusing only on the 'BertScore' method.\n",
        "        analysis_df = prepare_data_for_spearman_analysis(\n",
        "            {'BertScore': f1_df}, correlation_df, base_config\n",
        "        )\n",
        "        # Compute the Spearman correlation results.\n",
        "        spearman_results_df = compute_spearman_rank_correlation(analysis_df, base_config)\n",
        "        # Calculate the average Spearman correlation for the BertScore method.\n",
        "        avg_spearman_corr = spearman_results_df[spearman_results_df['method'] == 'BertScore']['correlation'].mean()\n",
        "\n",
        "        # Append the results for this scheme to our list.\n",
        "        results.append({'weight_scheme': scheme, 'avg_spearman_corr': avg_spearman_corr})\n",
        "\n",
        "    # Convert the list of results into a pandas DataFrame, indexed by the scheme.\n",
        "    return pd.DataFrame(results).set_index('weight_scheme')\n",
        "\n",
        "\n",
        "def run_metric_component_sensitivity_analysis(\n",
        "    base_config: Dict[str, Any],\n",
        "    holdings_clean_df: pd.DataFrame,\n",
        "    portfolio_data: PortfolioData,\n",
        "    proximity_matrix_df: pd.DataFrame,\n",
        "    correlation_df: pd.DataFrame\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates a complete suite of sensitivity analyses on metric components.\n",
        "\n",
        "    This complete, remediated version performs all three specified analyses:\n",
        "    1.  **STRAPSim Convergence Threshold**: Tests sensitivity to numerical precision.\n",
        "    2.  **Low-Weight Holding Exclusion**: Examines the impact of trimming portfolios.\n",
        "    3.  **BERTScore Weighting Scheme**: Analyzes how different weighting schemes in\n",
        "        the residual calculation affect the final outcome.\n",
        "\n",
        "    Args:\n",
        "        base_config (Dict[str, Any]): The validated global configuration dictionary.\n",
        "        holdings_clean_df (pd.DataFrame): The cleansed, full holdings DataFrame.\n",
        "        portfolio_data (PortfolioData): The structured portfolio data from the full dataset.\n",
        "        proximity_matrix_df (pd.DataFrame): The proximity matrix as a DataFrame.\n",
        "        correlation_df (pd.DataFrame): The ground-truth return correlation matrix.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary containing the summary DataFrames\n",
        "                                 for each sensitivity analysis performed.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(base_config, dict):\n",
        "        raise TypeError(\"Input 'base_config' must be a dictionary.\")\n",
        "\n",
        "    # Initialize a dictionary to store the results of each analysis.\n",
        "    results: Dict[str, pd.DataFrame] = {}\n",
        "    # Convert proximity matrix to NumPy array for performance.\n",
        "    proximity_matrix_np = proximity_matrix_df.to_numpy()\n",
        "\n",
        "    # --- Analysis 1: STRAPSim Convergence Threshold ---\n",
        "    # Execute the sensitivity analysis for the STRAPSim convergence threshold.\n",
        "    convergence_results_df = _run_threshold_sensitivity_analysis(\n",
        "        base_config, portfolio_data, proximity_matrix_np\n",
        "    )\n",
        "    # Store the results.\n",
        "    results['convergence_sensitivity'] = convergence_results_df\n",
        "    # Log the summary table for immediate review.\n",
        "    logging.info(f\"\\n--- Convergence Sensitivity Results ---\\n{convergence_results_df.to_string()}\")\n",
        "\n",
        "    # --- Analysis 2: Low-Weight Holding Exclusion ---\n",
        "    # Execute the sensitivity analysis for excluding low-weight holdings.\n",
        "    low_weight_results_df = _run_low_weight_holding_sensitivity_analysis(\n",
        "        base_config, holdings_clean_df, proximity_matrix_df, correlation_df\n",
        "    )\n",
        "    # Store the results.\n",
        "    results['low_weight_sensitivity'] = low_weight_results_df\n",
        "    # Log the summary table.\n",
        "    logging.info(f\"\\n--- Low-Weight Holding Sensitivity Results ---\\n{low_weight_results_df.to_string()}\")\n",
        "\n",
        "    # --- Analysis 3: BERTScore Weighting Scheme ---\n",
        "    # Execute the newly added sensitivity analysis for BERTScore weighting schemes.\n",
        "    bertscore_weight_results_df = _run_bertscore_weight_scheme_analysis(\n",
        "        base_config, portfolio_data, proximity_matrix_np, correlation_df\n",
        "    )\n",
        "    # Store the results.\n",
        "    results['bertscore_weight_sensitivity'] = bertscore_weight_results_df\n",
        "    # Log the summary table.\n",
        "    logging.info(f\"\\n--- BERTScore Weighting Sensitivity Results ---\\n{bertscore_weight_results_df.to_string()}\")\n",
        "\n",
        "    # Return the dictionary containing all analysis results.\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "MwzkxJ6vZZiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrators\n",
        "\n",
        "def run_strapsim_pipeline_for_robustness(\n",
        "    etf_holdings_df: pd.DataFrame,\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    monthly_returns_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the core STRAPSim pipeline and returns all intermediate artifacts.\n",
        "\n",
        "    This function is a variant of the main pipeline orchestrator, designed\n",
        "    specifically to produce not only the final results but also all the\n",
        "\n",
        "    intermediate data structures (cleansed data, feature matrices, trained model,\n",
        "    etc.) that are required as inputs for the subsequent robustness analyses.\n",
        "    This \"calculate-once, reuse-many\" approach is highly efficient and robust for\n",
        "    conducting sensitivity studies.\n",
        "\n",
        "    The pipeline proceeds through the main analytical phases:\n",
        "    1.  **Data Cleansing**: Standardizes and cleans the validated raw data.\n",
        "    2.  **Feature Engineering**: Constructs the feature and target matrices for the\n",
        "        Random Forest model, including one-hot encoding and normalization, and\n",
        "        then splits the data into training and testing sets.\n",
        "    3.  **Model Training**: Optimizes hyperparameters via cross-validation and\n",
        "        trains the final model.\n",
        "    4.  **Proximity Matrix Generation**: Uses the trained model to generate the\n",
        "        constituent-level similarity matrix.\n",
        "    5.  **Artifact Collection**: Gathers all key intermediate and final data\n",
        "        structures into a single dictionary for downstream use.\n",
        "\n",
        "    Note: This function intentionally omits the final similarity computation and\n",
        "    evaluation steps of the main pipeline. Its sole purpose is to generate the\n",
        "    foundational artifacts (like the proximity matrix and portfolio data) that\n",
        "    are common inputs to all robustness analysis scenarios. The final evaluation\n",
        "    is performed within each specific sensitivity test to measure the impact of\n",
        "    the scenario's parameters.\n",
        "\n",
        "    Args:\n",
        "        etf_holdings_df (pd.DataFrame): The raw DataFrame of ETF holdings.\n",
        "        bond_features_df (pd.DataFrame): The raw DataFrame of bond features.\n",
        "        monthly_returns_df (pd.DataFrame): The raw DataFrame of monthly returns.\n",
        "        config (Dict[str, Any]): The global configuration dictionary for the study.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing all key artifacts\n",
        "                        produced during the pipeline execution, ready for use in\n",
        "                        robustness analysis functions.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any of the initial data validation checks fail.\n",
        "        TypeError: If inputs are not of the expected type.\n",
        "        Exception: Propagates any exception that occurs during the pipeline execution.\n",
        "    \"\"\"\n",
        "    # Announce the start of the main pipeline execution.\n",
        "    logging.info(\"--- Running Core STRAPSim Pipeline to Generate Artifacts ---\")\n",
        "\n",
        "    # Initialize a dictionary to store all major artifacts produced by the pipeline.\n",
        "    # The configuration itself is the first artifact.\n",
        "    pipeline_artifacts: Dict[str, Any] = {'config': config}\n",
        "\n",
        "    try:\n",
        "        # === PHASE 1 & 2: VALIDATION and CLEANSING ===\n",
        "        # The validation functions are assumed to be called before this orchestrator.\n",
        "        # This function proceeds directly to cleansing the already-validated data.\n",
        "        logging.info(\"Cleansing and standardizing all input data...\")\n",
        "\n",
        "        # Cleanse the ETF holdings data.\n",
        "        holdings_clean = cleanse_etf_holdings_dataframe(etf_holdings_df, config)\n",
        "\n",
        "        # Cleanse the bond features master data.\n",
        "        features_clean = cleanse_bond_features_dataframe(bond_features_df, config)\n",
        "\n",
        "        # Cleanse the monthly returns time-series data.\n",
        "        returns_clean = cleanse_monthly_returns_dataframe(monthly_returns_df, config)\n",
        "\n",
        "        # Store the cleansed data artifacts.\n",
        "        pipeline_artifacts.update({\n",
        "            'holdings_clean': holdings_clean,\n",
        "            'features_clean': features_clean,\n",
        "            'returns_clean': returns_clean\n",
        "        })\n",
        "\n",
        "        # === PHASE 3: FEATURE ENGINEERING ===\n",
        "        logging.info(\"Engineering features and splitting data...\")\n",
        "\n",
        "        # Prepare categorical features by one-hot encoding them.\n",
        "        categorical_encoded = prepare_categorical_features(features_clean.set_index('cusip'), config)\n",
        "\n",
        "        # Prepare numerical features by scaling them. The scaling parameters are also returned.\n",
        "        numerical_scaled, scaling_params = prepare_numerical_features(features_clean.set_index('cusip'), config)\n",
        "\n",
        "        # Assemble the final feature/target matrices and split into train/test sets.\n",
        "        X_train, X_test, y_train, y_test = construct_and_split_data(\n",
        "            features_clean, categorical_encoded, numerical_scaled, config\n",
        "        )\n",
        "\n",
        "        # Reconstruct the complete, unsplit feature and target matrices for later use.\n",
        "        X_complete = pd.concat([X_train, X_test]).sort_index()\n",
        "        y_complete = pd.concat([y_train, y_test]).sort_index()\n",
        "\n",
        "        # Store all feature engineering artifacts.\n",
        "        pipeline_artifacts.update({\n",
        "            'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test,\n",
        "            'X_complete': X_complete, 'y_complete': y_complete, 'scaling_params': scaling_params\n",
        "        })\n",
        "\n",
        "        # === PHASE 4: MODEL TRAINING & PROXIMITY MATRIX ===\n",
        "        logging.info(\"Training model and generating proximity matrix...\")\n",
        "\n",
        "        # Find the optimal hyperparameters using grid search on the training data.\n",
        "        optimal_params = optimize_random_forest_hyperparameters(X_train, y_train, config)\n",
        "\n",
        "        # Train the final model using the optimal parameters and evaluate its performance.\n",
        "        final_model = train_final_model_and_evaluate(\n",
        "            X_train, y_train, X_test, y_test, optimal_params, config\n",
        "        )\n",
        "\n",
        "        # Generate the constituent-level proximity matrix using the final model.\n",
        "        proximity_matrix_df = generate_proximity_matrix(final_model, X_complete, config)\n",
        "\n",
        "        # Store the modeling artifacts.\n",
        "        pipeline_artifacts.update({\n",
        "            'optimal_params': optimal_params,\n",
        "            'trained_model': final_model,\n",
        "            'proximity_matrix_df': proximity_matrix_df\n",
        "        })\n",
        "\n",
        "        # === PHASE 5: FINAL ARTIFACT PREPARATION ===\n",
        "        logging.info(\"Preparing final data structures for analysis...\")\n",
        "\n",
        "        # Create the structured portfolio data dictionary needed by similarity algorithms.\n",
        "        portfolio_data = extract_portfolio_level_data(holdings_clean, proximity_matrix_df)\n",
        "\n",
        "        # Compute the ground-truth return correlation matrix.\n",
        "        correlation_df = compute_return_correlation_matrix(returns_clean, config)\n",
        "\n",
        "        # Store the final data structure artifacts.\n",
        "        pipeline_artifacts.update({\n",
        "            'portfolio_data': portfolio_data,\n",
        "            'correlation_df': correlation_df\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any exception during the pipeline, log it with full traceback, and re-raise.\n",
        "        logging.error(\"--- Artifact Generation Pipeline FAILED ---\", exc_info=True)\n",
        "        raise\n",
        "\n",
        "    # Announce the successful completion of the artifact generation process.\n",
        "    logging.info(\"--- Core Pipeline Finished: All Artifacts Generated Successfully ---\")\n",
        "\n",
        "    # Return the comprehensive dictionary containing all generated data and model objects.\n",
        "    return pipeline_artifacts\n",
        "\n",
        "\n",
        "def run_robustness_analysis_suite(\n",
        "    pipeline_artifacts: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of the complete suite of robustness analyses.\n",
        "\n",
        "    This function takes a dictionary of pre-computed artifacts from the main\n",
        "    research pipeline and uses them to run a series of sensitivity tests. This\n",
        "    design is highly efficient as it avoids re-computing foundational data\n",
        "    structures for each analysis.\n",
        "\n",
        "    The suite includes:\n",
        "    1.  **Hyperparameter Sensitivity (Task 26)**\n",
        "    2.  **Data Split Sensitivity (Task 27)**\n",
        "    3.  **Metric Component Sensitivity (Task 28)**\n",
        "\n",
        "    Args:\n",
        "        pipeline_artifacts (Dict[str, Any]): A dictionary containing all the\n",
        "            necessary data and model artifacts from the main pipeline run.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary where keys are the names of the\n",
        "                                 analyses and values are the corresponding\n",
        "                                 summary DataFrames.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Check for the presence of a few key artifacts to ensure the input is valid.\n",
        "    required_artifacts = [\n",
        "        'base_config', 'optimal_params', 'X_complete', 'y_complete',\n",
        "        'portfolio_data', 'correlation_df', 'holdings_clean_df'\n",
        "    ]\n",
        "    if not all(key in pipeline_artifacts for key in required_artifacts):\n",
        "        raise ValueError(\"Input 'pipeline_artifacts' is missing required data.\")\n",
        "\n",
        "    # Announce the start of the robustness analysis phase.\n",
        "    logging.info(\"--- Starting Full Robustness Analysis Suite ---\")\n",
        "\n",
        "    # Initialize a dictionary to store the results of all analyses.\n",
        "    robustness_results: Dict[str, pd.DataFrame] = {}\n",
        "\n",
        "    try:\n",
        "        # --- Unpack all necessary artifacts from the input dictionary ---\n",
        "        # This makes the subsequent function calls clean and readable.\n",
        "        base_config = pipeline_artifacts['base_config']\n",
        "        optimal_params = pipeline_artifacts['optimal_params']\n",
        "        X_complete = pipeline_artifacts['X_complete']\n",
        "        y_complete = pipeline_artifacts['y_complete']\n",
        "        X_train, y_train = pipeline_artifacts['X_train'], pipeline_artifacts['y_train']\n",
        "        X_test, y_test = pipeline_artifacts['X_test'], pipeline_artifacts['y_test']\n",
        "        portfolio_data = pipeline_artifacts['portfolio_data']\n",
        "        correlation_df = pipeline_artifacts['correlation_df']\n",
        "        holdings_clean_df = pipeline_artifacts['holdings_clean_df']\n",
        "        proximity_matrix_df = pipeline_artifacts['proximity_matrix_df']\n",
        "\n",
        "        # --- Execute Hyperparameter Sensitivity Analysis (Task 26) ---\n",
        "        logging.info(\"\\n\" + \"=\"*80)\n",
        "        robustness_results['hyperparameter_sensitivity'] = run_hyperparameter_sensitivity_analysis(\n",
        "            base_config, optimal_params, X_train, y_train, X_test, y_test,\n",
        "            portfolio_data, correlation_df\n",
        "        )\n",
        "\n",
        "        # --- Execute Data Split Sensitivity Analysis (Task 27) ---\n",
        "        logging.info(\"\\n\" + \"=\"*80)\n",
        "        robustness_results['data_split_sensitivity'] = run_data_split_sensitivity_analysis(\n",
        "            base_config, optimal_params, X_complete, y_complete,\n",
        "            portfolio_data, correlation_df\n",
        "        )\n",
        "\n",
        "        # --- Execute Metric Component Sensitivity Analysis (Task 28) ---\n",
        "        logging.info(\"\\n\" + \"=\"*80)\n",
        "        robustness_results['metric_component_sensitivity'] = run_metric_component_sensitivity_analysis(\n",
        "            base_config, holdings_clean_df, portfolio_data,\n",
        "            proximity_matrix_df, correlation_df\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch and log any errors during the analyses.\n",
        "        logging.error(\"--- ROBUSTNESS ANALYSIS SUITE FAILED ---\", exc_info=True)\n",
        "        raise\n",
        "\n",
        "    # Announce the successful completion of the entire suite.\n",
        "    logging.info(\"\\n\" + \"=\"*80)\n",
        "    logging.info(\"--- Full Robustness Analysis Suite Completed Successfully ---\")\n",
        "\n",
        "    # Return the aggregated results.\n",
        "    return robustness_results\n",
        "\n",
        "\n",
        "def run_full_study(\n",
        "    etf_holdings_df: pd.DataFrame,\n",
        "    bond_features_df: pd.DataFrame,\n",
        "    monthly_returns_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Top-level orchestrator to run the main analysis and all robustness checks.\n",
        "\n",
        "    This function provides a single entry point to execute the entire study. It\n",
        "    first runs the core pipeline to generate the main results and all necessary\n",
        "    intermediate data artifacts. It then passes these artifacts to the robustness\n",
        "    analysis suite to perform a full validation of the study's conclusions.\n",
        "\n",
        "    Args:\n",
        "        etf_holdings_df (pd.DataFrame): The raw DataFrame of ETF holdings.\n",
        "        bond_features_df (pd.DataFrame): The raw DataFrame of bond features.\n",
        "        monthly_returns_df (pd.DataFrame): The raw DataFrame of monthly returns.\n",
        "        config (Dict[str, Any]): The global configuration dictionary for the study.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing the results from\n",
        "                        both the main analysis and the robustness suite.\n",
        "    \"\"\"\n",
        "    # --- Stage 1: Run the core pipeline to generate primary results and artifacts ---\n",
        "    # The config is added to the artifacts dictionary for use in the next stage.\n",
        "    pipeline_artifacts = run_strapsim_pipeline_for_robustness(\n",
        "        etf_holdings_df, bond_features_df, monthly_returns_df, config\n",
        "    )\n",
        "    pipeline_artifacts['base_config'] = config\n",
        "\n",
        "    # --- Stage 2: Run the full suite of robustness analyses using the artifacts ---\n",
        "    robustness_results = run_robustness_analysis_suite(pipeline_artifacts)\n",
        "\n",
        "    # --- Final Aggregation ---\n",
        "    # Combine the main results (if any were stored) and the robustness results.\n",
        "    final_output = {\n",
        "        \"main_analysis_artifacts\": pipeline_artifacts,\n",
        "        \"robustness_analysis_results\": robustness_results\n",
        "    }\n",
        "\n",
        "    return final_output\n",
        "\n"
      ],
      "metadata": {
        "id": "KS5WBnZ6kR8k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}